Directory structure:
└── dmboynton56-sports-edge/
    ├── README.md
    ├── constrainst.txt
    ├── debug_prediction.py
    ├── diagnose_key_games.py
    ├── evaluate_predictions.py
    ├── MODEL_IMPROVEMENTS.md
    ├── predict_WEEK_10.py
    ├── predict_WEEK_11.py
    ├── PRODUCTION_USAGE.md
    ├── requirements.txt
    ├── SETUP.md
    ├── sports-edge-analysis-display-cbdcab3c.plan.md
    ├── sports-edge-analysis-plan.md
    ├── sports-edge-export-plan.md
    ├── sports-edge-website-display-plan.md
    ├── notebooks/
    │   ├── nba_eda.ipynb
    │   ├── nfl_eda.ipynb
    │   └── week10_diagnostics.ipynb
    ├── sql/
    │   └── 001_initial_schema.sql
    ├── src/
    │   ├── __init__.py
    │   ├── data/
    │   │   ├── __init__.py
    │   │   ├── nba_fetcher.py
    │   │   ├── nfl_fetcher.py
    │   │   └── odds_fetcher.py
    │   ├── features/
    │   │   ├── __init__.py
    │   │   ├── form_metrics.py
    │   │   ├── rest_schedule.py
    │   │   └── strength.py
    │   ├── models/
    │   │   ├── __init__.py
    │   │   ├── link_function.py
    │   │   ├── predictor.py
    │   │   ├── spread_model.py
    │   │   └── win_prob_model.py
    │   └── pipeline/
    │       ├── __init__.py
    │       ├── refresh.py
    │       └── train_models.py
    └── .github/
        └── workflows/
            └── refresh.yml

================================================
FILE: README.md
================================================
# Sports-Edge: NFL/NBA Betting Analysis Pipeline

A machine learning pipeline to compute model spreads and home win probabilities for NFL/NBA games, compare against sportsbook lines, and display results on a personal portfolio.

## PROGRESS

We are in a decent spot right now in terms of our models outputted realistic spreads, computed through thoroughly thoughout rolling stats, like strengh of schedule, point differential stats, etc. 

Looking to incorportate situational alterations, like VORP/LEBRON/PVS 

## Setup

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Copy `.env.example` to `.env` and fill in your API keys:
```bash
cp .env.example .env
```

3. Run EDA notebooks to explore data:
```bash
jupyter notebook notebooks/
```

## Usage

### Run Analysis Pipeline

```bash
python -m src.pipeline.refresh --league NFL --date 2025-11-06
python -m src.pipeline.refresh --league NBA --date 2025-11-06
```

### Train Models

Models are trained via notebooks or separate training scripts. Saved artifacts go in `models/` directory.

## Project Structure

- `notebooks/` - Jupyter notebooks for EDA and exploration
- `src/` - Source code modules
  - `data/` - Data fetching modules (NFL, NBA, odds)
  - `features/` - Feature engineering modules
  - `models/` - Model training and inference
  - `pipeline/` - CLI and orchestration
- `data/` - Raw and curated data (gitignored)
- `models/` - Saved model artifacts (gitignored)
- `sql/` - Database migration scripts

## Data Sources

- **NFL**: `nfl_data_py`
- **NBA**: `nba_api`
- **Odds**: The Odds API



================================================
FILE: constrainst.txt
================================================
pandas>=2.0.0



================================================
FILE: debug_prediction.py
================================================
# Deep Feature Analysis for Model Disagreement
# This script investigates why models disagree and why predictions differ from book spreads

import pandas as pd
import numpy as np
import pickle
import os
import sys
from src.models.predictor import GamePredictor
from src.data import nfl_fetcher

# Load historical data
schedule_df = nfl_fetcher.fetch_nfl_schedule(2024)
schedule_df['game_date'] = pd.to_datetime(schedule_df['gameday'])

# Initialize predictor
predictor = GamePredictor('NFL', 'v2')

# Game to analyze
game_row = pd.DataFrame({
    'home_team': ['DEN'],
    'away_team': ['LV'],
    'game_date': ['2025-11-06'],
    'season': [2025]
})

print("=" * 80)
print("DEEP FEATURE ANALYSIS")
print("=" * 80)

# 1. Build features for this game
print("\n1. BUILDING FEATURES FOR PREDICTION")
print("-" * 80)
features_df = predictor.build_features_for_game(game_row, schedule_df)
win_feature_names = predictor.win_feature_names or predictor.feature_names
spread_feature_names = predictor.spread_feature_names or predictor.feature_names

spread_matrix = pd.DataFrame()
for col in spread_feature_names:
    spread_matrix[col] = features_df[col] if col in features_df.columns else 0.0
spread_matrix = spread_matrix.fillna(0.0)
spread_pred_vector = predictor.spread_model.predict(spread_matrix)

win_matrix = pd.DataFrame()
for col in win_feature_names:
    if col == 'model_spread_feature':
        win_matrix[col] = spread_pred_vector
    elif col in features_df.columns:
        win_matrix[col] = features_df[col]
    else:
        win_matrix[col] = 0.0
win_matrix = win_matrix.fillna(0.0)

# Show key computed features
key_features_to_check = [
    'home_team_win_pct', 'away_team_win_pct',
    'home_team_point_diff', 'away_team_point_diff',
    'opp_strength_home_season', 'opp_strength_away_season',
    'rest_home', 'rest_away',
    'win_pct_differential', 'point_diff_differential'
]

print("\nKey features computed:")
for col in key_features_to_check:
    if col in features_df.columns:
        val = features_df[col].iloc[0]
        if pd.notna(val):
            print(f"  {col:35s}: {val:10.4f}")
        else:
            print(f"  {col:35s}: {'NaN':>10}")

# 2. Check which features the model expects vs what we have
print("\n2. MODEL FEATURE REQUIREMENTS")
print("-" * 80)

X = win_matrix
missing_features = []
for col in win_feature_names:
    if col == 'model_spread_feature':
        continue
    if col not in features_df.columns or pd.isna(features_df[col].iloc[0]):
        missing_features.append(col)

print(f"Model expects: {len(win_feature_names)} features")
print(f"Missing/defaulted: {len(missing_features)} ({len(missing_features)/len(win_feature_names)*100:.1f}%)")

# Check form features specifically
form_features = [f for f in win_feature_names if f.startswith('form_')]
missing_form = [f for f in form_features if f in missing_features]
print(f"\nForm features (EPA): {len(form_features)} total, {len(missing_form)} missing ({len(missing_form)/len(form_features)*100:.1f}%)")

# 3. Load training data statistics
print("\n3. COMPARING TO TRAINING DATA")
print("-" * 80)

models_dir = 'models'
medians_path = os.path.join(models_dir, 'feature_medians_nfl_v2.pkl')
if os.path.exists(medians_path):
    with open(medians_path, 'rb') as f:
        training_medians = pickle.load(f)
    
    print("\nTop features - Prediction vs Training Median:")
    print(f"{'Feature':<35} {'Predicted':>12} {'Training Median':>15} {'Difference':>12}")
    print("-" * 80)
    
    # Show features with largest differences
    diffs = []
    for col in win_feature_names:
        pred_val = X[col].iloc[0]
        train_median = training_medians.get(col, 0)
        diff = abs(pred_val - train_median)
        diffs.append((col, pred_val, train_median, diff))
    
    diffs.sort(key=lambda x: x[3], reverse=True)
    for col, pred_val, train_median, diff in diffs[:20]:
        print(f"{col:<35} {pred_val:>12.4f} {train_median:>15.4f} {diff:>12.4f}")

# 4. Feature importance analysis
print("\n4. FEATURE IMPORTANCE ANALYSIS")
print("-" * 80)

win_prob_path = os.path.join(models_dir, 'win_prob_model_nfl_v2.pkl')
spread_path = os.path.join(models_dir, 'spread_model_nfl_v2.pkl')

with open(win_prob_path, 'rb') as f:
    win_prob_data = pickle.load(f)
with open(spread_path, 'rb') as f:
    spread_data = pickle.load(f)

win_prob_model = win_prob_data['model']
spread_model = spread_data['model']

# Get feature importances
def get_importance(model):
    if hasattr(model, 'feature_importances_'):
        return model.feature_importances_
    elif hasattr(model, 'calibrated_classifiers_'):
        base = model.calibrated_classifiers_[0].estimator
        if hasattr(base, 'feature_importances_'):
            return base.feature_importances_
    return None

win_imp = get_importance(win_prob_model)
spread_imp = get_importance(spread_model)

if win_imp is not None:
    win_importance = pd.DataFrame({
        'feature': win_feature_names,
        'importance': win_imp,
        'value': X.iloc[0].values
    }).sort_values('importance', ascending=False)
    
    print("\nTop 15 Features - Win Probability Model:")
    print(f"{'Feature':<35} {'Importance':>12} {'Value':>12}")
    print("-" * 60)
    for _, row in win_importance.head(15).iterrows():
        print(f"{row['feature']:<35} {row['importance']:>12.4f} {row['value']:>12.4f}")

if spread_imp is not None:
    spread_importance = pd.DataFrame({
        'feature': spread_feature_names,
        'importance': spread_imp,
        'value': spread_matrix.iloc[0].values
    }).sort_values('importance', ascending=False)
    
    print("\nTop 15 Features - Spread Model:")
    print(f"{'Feature':<35} {'Importance':>12} {'Value':>12}")
    print("-" * 60)
    for _, row in spread_importance.head(15).iterrows():
        print(f"{row['feature']:<35} {row['importance']:>12.4f} {row['value']:>12.4f}")

# 5. Data availability check
print("\n5. DATA AVAILABILITY CHECK")
print("-" * 80)

game_date = pd.to_datetime('2025-11-06')
print(f"Predicting game on: {game_date.date()}")
print(f"Using historical data through: {schedule_df['game_date'].max().date()}")

# Check team strength features - these should be based on 2025 season, but we only have 2024
print(f"\nCRITICAL ISSUE:")
print(f"   Predicting {game_date.year} game but only have data through {schedule_df['game_date'].max().year}")
print(f"   Team strength features (win_pct, point_diff) will be:")
print(f"     - NaN (if looking for 2025 season games)")
print(f"     - Based on 2024 season (if falling back)")

# 6. Make prediction
print("\n6. PREDICTION ANALYSIS")
print("-" * 80)

prediction = predictor.predict(game_row, schedule_df)

print(f"\nOur Prediction: DEN by {prediction['predicted_spread']:.2f}")
print(f"Official Spread: DEN by 9.5")
print(f"Difference: {abs(prediction['predicted_spread'] - 9.5):.2f} points")
print(f"\nModel Disagreement: {prediction.get('model_disagreement', 0):.1%}")

# 7. Root Cause Analysis
print("\n7. ROOT CAUSE ANALYSIS")
print("-" * 80)

issues = []

# Check if we're using old season data
if 'home_team_win_pct' in X.columns:
    win_pct = X['home_team_win_pct'].iloc[0]
    if pd.isna(win_pct) or win_pct == 0:
        issues.append("ERROR: home_team_win_pct is NaN/0 - no 2025 season data available")
    else:
        issues.append(f"WARNING: home_team_win_pct = {win_pct:.3f} - based on 2024 season, not 2025")

# Check form features
if len(missing_form) > len(form_features) * 0.5:
    issues.append(f"ERROR: {len(missing_form)}/{len(form_features)} form features missing - no PBP data for future games")

# Check if key features are defaulting
high_importance_missing = []
if win_imp is not None:
    for feat, imp in zip(win_feature_names, win_imp):
        if feat in missing_features and imp > np.percentile(win_imp, 75):
            high_importance_missing.append((feat, imp))
    
    if high_importance_missing:
        issues.append(f"ERROR: {len(high_importance_missing)} high-importance features are missing/defaulted")

# Prediction accuracy
if abs(prediction['predicted_spread'] - 9.5) > 3:
    issues.append(f"ERROR: Prediction off by {abs(prediction['predicted_spread'] - 9.5):.1f} points")
    issues.append("   Likely causes:")
    issues.append("     1. Missing 2025 season data (team strength features)")
    issues.append("     2. Missing form/EPA features (no PBP for future games)")
    issues.append("     3. Models trained on historical data, predicting future with incomplete features")

print("\nIssues identified:")
for issue in issues:
    print(f"  {issue}")

# 8. Recommendations
print("\n8. RECOMMENDATIONS")
print("-" * 80)

print("""
To fix model disagreement and improve prediction accuracy:

1. **Get Current Season Data**
   - Load 2025 NFL schedule and results up to Nov 6, 2025
   - This will populate team strength features (win_pct, point_diff) correctly
   - Use: nfl_fetcher.fetch_nfl_schedule(2025)

2. **Handle Missing Form Features**
   - Form features (EPA) require play-by-play data
   - For future games, these will always be missing
   - Options:
     a) Use recent form (last 3-5 games) as proxy
     b) Train models without form features for future predictions
     c) Use season-to-date averages instead of rolling windows

3. **Separate Models for Future vs Historical**
   - Train one model with all features (for historical analysis)
   - Train another model without form features (for future predictions)
   - Or: Use feature importance to identify which features matter most

4. **Feature Engineering for Future Games**
   - Use preseason projections or early season data
   - Incorporate betting market data (closing lines) as features
   - Use team ratings from external sources

5. **Model Calibration**
   - The ensemble approach helps, but fixing root cause is better
   - Consider retraining with more recent data
   - Use temporal validation (train on older data, test on recent)
""")

print("\n" + "=" * 80)



================================================
FILE: diagnose_key_games.py
================================================
import os
import sys
from typing import List, Tuple, Dict, Optional

import numpy as np
import pandas as pd

from src.data import nfl_fetcher
from src.models.predictor import GamePredictor


TARGET_GAMES = [
    ('LAC', 'PIT'),  # PIT @ LAC
    ('GB', 'PHI'),   # PHI @ GB
]

SEASON = 2025


def load_schedule(season: int) -> pd.DataFrame:
    """Fetch schedule and ensure game_date column exists."""
    print(f"Loading {season} schedule via nfl_data_py...")
    schedule = nfl_fetcher.fetch_nfl_schedule(season)
    if 'gameday' in schedule.columns:
        schedule['game_date'] = pd.to_datetime(schedule['gameday'])
    elif 'game_date' in schedule.columns:
        schedule['game_date'] = pd.to_datetime(schedule['game_date'])
    else:
        raise ValueError("Schedule missing 'game_date'/'gameday' column.")
    schedule['season'] = season
    return schedule


def find_games(schedule: pd.DataFrame, targets: List[Tuple[str, str]]) -> List[pd.Series]:
    """Locate the target matchups in the schedule."""
    games = []
    for home, away in targets:
        match = schedule[
            (schedule['home_team'] == home) &
            (schedule['away_team'] == away)
        ]
        if match.empty:
            raise ValueError(f"Game {away} @ {home} not found in schedule.")
        games.append(match.iloc[0])
    return games


def load_feature_medians(predictor: GamePredictor) -> Dict[str, float]:
    """Load feature medians used for imputation."""
    medians_path = os.path.join(
        predictor.models_dir,
        f"feature_medians_{predictor.league.lower()}_{predictor.model_version}.pkl"
    )
    try:
        import pickle
        with open(medians_path, 'rb') as f:
            medians = pickle.load(f)
        if isinstance(medians, dict):
            return medians
    except Exception:
        # Fall back to zeros if medians missing
        pass
    return {}


def get_feature_matrix(features_df: pd.DataFrame,
                       feature_names: List[str],
                       spread_override: Optional[np.ndarray] = None) -> pd.DataFrame:
    """Project engineered features onto the model's expected columns."""
    X = pd.DataFrame()
    for col in feature_names:
        if col == 'model_spread_feature' and spread_override is not None:
            X[col] = spread_override
        elif col in features_df.columns:
            X[col] = features_df[col]
        else:
            X[col] = 0.0
    return X.fillna(0.0)


def average_feature_importances(win_prob_model, feature_names: List[str]) -> np.ndarray:
    """Average feature importances across calibrated estimators."""
    importances = np.zeros(len(feature_names))
    if hasattr(win_prob_model, 'calibrated_classifiers_'):
        used = 0
        for clf in win_prob_model.calibrated_classifiers_:
            est = getattr(clf, 'estimator', None)
            if est is not None and hasattr(est, 'feature_importances_'):
                importances += est.feature_importances_
                used += 1
        if used > 0:
            return importances / used
    if hasattr(win_prob_model, 'base_estimator') and hasattr(win_prob_model.base_estimator, 'feature_importances_'):
        return win_prob_model.base_estimator.feature_importances_
    return np.ones(len(feature_names))


def compute_feature_impacts(values: pd.Series, medians: Dict[str, float],
                            importances: np.ndarray, top_n: int = 8) -> List[Tuple[str, float, float, float]]:
    """Return top features ranked by |delta from median| * importance."""
    rows = []
    for idx, feat in enumerate(values.index):
        imp = importances[idx] if idx < len(importances) else 0.0
        if imp <= 0:
            continue
        median_val = medians.get(feat, 0.0)
        delta = values.iloc[idx] - median_val
        impact = abs(delta) * imp
        if impact == 0:
            continue
        rows.append((feat, values.iloc[idx], delta, impact))
    rows.sort(key=lambda x: x[3], reverse=True)
    return rows[:top_n]


def print_prediction_details(predictor: GamePredictor, game_series: pd.Series,
                             schedule: pd.DataFrame,
                             feature_medians: Dict[str, float],
                             win_importances: np.ndarray,
                             spread_importances: np.ndarray):
    """Predict a game and display feature diagnostics."""
    game_row = pd.DataFrame([{
        'home_team': game_series['home_team'],
        'away_team': game_series['away_team'],
        'game_date': game_series['game_date'],
        'season': game_series['season']
    }])
    
    prediction = predictor.predict(game_row, schedule)
    
    engineered_features = predictor.build_features_for_game(game_row, schedule)
    spread_feature_names = predictor.spread_feature_names or predictor.feature_names
    win_feature_names = predictor.win_feature_names or predictor.feature_names
    
    spread_matrix = get_feature_matrix(engineered_features, spread_feature_names)
    spread_values = predictor.spread_model.predict(spread_matrix)
    win_matrix = get_feature_matrix(engineered_features, win_feature_names, spread_override=spread_values)
    
    feature_impacts_win = compute_feature_impacts(win_matrix.iloc[0], feature_medians, win_importances)
    feature_impacts_spread = compute_feature_impacts(spread_matrix.iloc[0], feature_medians, spread_importances)
    
    print("\n" + "=" * 90)
    print(f"{prediction['away_team']} @ {prediction['home_team']} | {prediction['game_date']}")
    print(f"  Spread: {prediction['predicted_spread']:.2f} ({prediction['spread_interpretation']})")
    print(f"  Win Probabilities: Home {prediction['home_win_probability']:.1%} | Away {prediction['away_win_probability']:.1%}")
    print(f"  Model vs Spread: {prediction['home_win_prob_from_model']:.1%} vs {prediction['win_prob_from_spread']:.1%}")
    print(f"  Ensemble Confidence: {prediction['confidence']:.1%}")
    if prediction.get('model_disagreement', 0) > 0.15:
        print(f"  ⚠️  Disagreement: {prediction['model_disagreement']:.1%}")
    
    def _print_table(title: str, impacts: List[Tuple[str, float, float, float]]):
        print(f"\n  {title}")
        if not impacts:
            print("    (No informative features)")
            return
        print("    Feature                      Value      Δ vs Median     Weighted Impact")
        for feat, val, delta, impact in impacts:
            print(f"    {feat:25s} {val:9.3f} {delta:14.3f} {impact:18.4f}")
    
    _print_table("Top Win-Probability Drivers", feature_impacts_win)
    _print_table("Top Spread Drivers", feature_impacts_spread)


def main():
    try:
        schedule = load_schedule(SEASON)
    except Exception as exc:
        print(f"ERROR: Failed to load schedule: {exc}")
        sys.exit(1)
    
    games = find_games(schedule, TARGET_GAMES)
    
    predictor = GamePredictor('NFL', 'v2')
    feature_medians = load_feature_medians(predictor)
    
    win_feature_names = predictor.win_feature_names or predictor.feature_names
    spread_feature_names = predictor.spread_feature_names or predictor.feature_names
    win_importances = average_feature_importances(predictor.win_prob_model, win_feature_names)
    spread_importances = getattr(predictor.spread_model, 'feature_importances_', np.ones(len(spread_feature_names)))
    
    for game in games:
        print_prediction_details(
            predictor,
            game,
            schedule,
            feature_medians,
            win_importances,
            spread_importances
        )
    
    print("\n" + "=" * 90)
    print("Analysis complete.")


if __name__ == "__main__":
    main()



================================================
FILE: evaluate_predictions.py
================================================
"""
Utility script to compare saved predictions against actual results.

Example:
    python3 evaluate_predictions.py \
        --predictions data/predictions/week10.csv \
        --season 2025 \
        --week 10
"""

import argparse
import os
from typing import Tuple

import numpy as np
import pandas as pd

from src.data import nfl_fetcher


def load_actual_results(season: int, week: int) -> pd.DataFrame:
    """Fetch completed games for a season/week with scores."""
    schedule = nfl_fetcher.fetch_nfl_schedule(season)
    if 'gameday' in schedule.columns:
        schedule['game_date'] = pd.to_datetime(schedule['gameday'])
    elif 'game_date' in schedule.columns:
        schedule['game_date'] = pd.to_datetime(schedule['game_date'])
    else:
        raise ValueError("Schedule missing game_date/gameday column.")
    
    week_games = schedule[
        (schedule['week'] == week) &
        schedule['home_score'].notna() &
        schedule['away_score'].notna()
    ].copy()
    
    if week_games.empty:
        raise ValueError(f"No completed games found for season {season}, week {week}.")
    
    week_games['actual_margin'] = week_games['home_score'] - week_games['away_score']
    week_games['actual_home_win'] = (week_games['actual_margin'] > 0).astype(int)
    week_games['game_date'] = pd.to_datetime(week_games['game_date']).dt.normalize()
    
    return week_games[[
        'home_team', 'away_team', 'game_date',
        'home_score', 'away_score',
        'actual_margin', 'actual_home_win'
    ]]


def load_predictions(path: str) -> pd.DataFrame:
    """Load predictions saved via predict_WEEK_10/11 CSV export."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Predictions file not found: {path}")
    
    df = pd.read_csv(path)
    required_cols = {
        'home_team', 'away_team', 'game_date',
        'predicted_spread', 'home_win_probability',
        'predicted_winner'
    }
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"Predictions file missing columns: {missing}")
    
    df['game_date'] = pd.to_datetime(df['game_date']).dt.normalize()
    return df


def evaluate(predictions: pd.DataFrame, actuals: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:
    """Join predictions with actuals and compute error metrics."""
    merged = predictions.merge(
        actuals,
        on=['home_team', 'away_team', 'game_date'],
        how='inner',
        suffixes=('', '_actual')
    )
    
    if merged.empty:
        raise ValueError("No overlapping games found between predictions and actual results.")
    
    merged['spread_error'] = merged['predicted_spread'] - merged['actual_margin']
    merged['abs_spread_error'] = merged['spread_error'].abs()
    merged['home_win_predicted_label'] = (merged['predicted_spread'] > 0).astype(int)
    merged['home_win_correct'] = (merged['home_win_predicted_label'] == merged['actual_home_win']).astype(int)
    merged['brier_component'] = (merged['home_win_probability'] - merged['actual_home_win']) ** 2
    
    metrics = {
        'games_evaluated': len(merged),
        'direction_accuracy': merged['home_win_correct'].mean(),
        'mean_abs_spread_error': merged['abs_spread_error'].mean(),
        'spread_rmse': np.sqrt((merged['spread_error'] ** 2).mean()),
        'brier_score': merged['brier_component'].mean()
    }
    
    return merged, metrics


def print_summary(metrics: dict) -> None:
    """Pretty-print aggregate metrics."""
    print("\nEvaluation Summary")
    print("------------------")
    print(f"Games evaluated:        {metrics['games_evaluated']}")
    print(f"Direction accuracy:     {metrics['direction_accuracy']:.3f}")
    print(f"Mean abs spread error:  {metrics['mean_abs_spread_error']:.2f} pts")
    print(f"Spread RMSE:            {metrics['spread_rmse']:.2f} pts")
    print(f"Brier score:            {metrics['brier_score']:.3f}")


def print_miss_list(merged: pd.DataFrame, top_n: int = 5) -> None:
    """Show largest misses to guide feature diagnostics."""
    largest_misses = merged.sort_values('abs_spread_error', ascending=False).head(top_n)
    if largest_misses.empty:
        return
    
    print(f"\nTop {len(largest_misses)} Biggest Misses")
    print("-----------------------------------")
    for _, row in largest_misses.iterrows():
        print(
            f"{row['away_team']} @ {row['home_team']} ({row['game_date'].date()}): "
            f"predicted {row['predicted_spread']:.1f}, actual margin {row['actual_margin']:.1f}, "
            f"error {row['spread_error']:.1f}"
        )


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Evaluate saved predictions against actual results.")
    parser.add_argument('--predictions', required=True, help="Path to CSV saved from predict_WEEK_*.py")
    parser.add_argument('--season', type=int, required=True, help="Season year (e.g., 2025)")
    parser.add_argument('--week', type=int, required=True, help="Week number to evaluate")
    parser.add_argument('--export-path', type=str, help="Optional path to write the merged evaluation CSV")
    return parser.parse_args()


def main():
    args = parse_args()
    preds = load_predictions(args.predictions)
    actuals = load_actual_results(args.season, args.week)
    merged, metrics = evaluate(preds, actuals)
    print_summary(metrics)
    print_miss_list(merged)
    
    if args.export_path:
        os.makedirs(os.path.dirname(args.export_path), exist_ok=True)
        merged.to_csv(args.export_path, index=False)
        print(f"\nDetailed evaluation saved to {args.export_path}")


if __name__ == "__main__":
    main()



================================================
FILE: MODEL_IMPROVEMENTS.md
================================================
# Model Improvements Summary

## Changes Made

### 1. **Model Calibration** (`src/pipeline/train_models.py`)
   - Added `CalibratedClassifierCV` wrapper to win probability model
   - Uses isotonic regression for probability calibration
   - Improves probability estimates to match actual outcomes
   - Reports Brier score for calibration quality

### 2. **Ensemble Approach** (`src/models/predictor.py`)
   - Implements intelligent ensemble of win probability and spread models
   - When models disagree significantly (>0.15), favors spread-derived probability (more consistent)
   - When models agree, uses weighted average (60% win prob model, 40% spread-derived)
   - Returns both individual and ensemble predictions for transparency

### 3. **Model Diagnostics** (Notebook Cell)
   - Comprehensive diagnostic analysis tool
   - Measures model agreement/disagreement
   - Calibration curves for both models
   - Link function verification
   - Specific recommendations for improvements

### 4. **Consistency Checks** (`src/pipeline/train_models.py`)
   - Automatically checks model consistency during training
   - Reports mean disagreement and sign agreement
   - Warns if models disagree significantly

## How It Works

### Training Phase
1. Win probability model is wrapped with `CalibratedClassifierCV` for better probability estimates
2. Spread model trains normally
3. Link function is calibrated on test data
4. Consistency metrics are calculated and reported

### Prediction Phase
1. Both models make predictions independently
2. Disagreement is calculated: `|win_prob_model - win_prob_from_spread|`
3. **If disagreement > 0.15**: Use spread-derived probability (100% weight)
4. **If disagreement ≤ 0.15**: Use weighted average (60% win prob, 40% spread)
5. Return ensemble probability as primary, with individual model outputs for transparency

## Benefits

1. **Consistency**: Win probabilities always align with spread predictions
2. **Calibration**: Probabilities match actual win rates
3. **Transparency**: Can see both model outputs and disagreement level
4. **Robustness**: Handles model disagreements gracefully

## Usage

### Retrain Models
Run the "Export Models for Production Use" cell in the notebook. The new training includes:
- Calibrated win probability model
- Consistency checks
- Better metrics reporting

### Run Diagnostics
Run the "Model Diagnostic and Calibration Analysis" cell to:
- See how well models agree
- Check calibration quality
- Get specific recommendations

### Make Predictions
The predictor now returns:
- `home_win_probability`: Final ensemble probability (primary)
- `home_win_prob_from_model`: Direct win prob model output
- `win_prob_from_spread`: Spread-derived probability
- `model_disagreement`: Level of disagreement between models

## Example Output

```python
{
    'predicted_spread': 3.2,
    'home_win_probability': 0.58,  # Ensemble (primary)
    'home_win_prob_from_model': 0.45,  # Direct model
    'win_prob_from_spread': 0.62,  # From spread
    'model_disagreement': 0.17,  # High disagreement
    'predicted_winner': 'DEN'  # Based on ensemble
}
```

In this case, since disagreement is high (>0.15), the final probability (0.58) is closer to the spread-derived value (0.62) than the direct model (0.45), ensuring consistency with the spread prediction.




================================================
FILE: predict_WEEK_10.py
================================================
import argparse
import os
import sys
from typing import List

import pandas as pd

from src.data import nfl_fetcher
from src.models.predictor import GamePredictor


MODEL_VERSION = 'v2'


def load_season_schedule(season: int) -> pd.DataFrame:
    """Fetch the NFL schedule for a season and normalize columns."""
    print(f"Loading {season} NFL season data...")
    schedule = nfl_fetcher.fetch_nfl_schedule(season)
    if 'gameday' in schedule.columns:
        schedule['game_date'] = pd.to_datetime(schedule['gameday'])
    elif 'game_date' in schedule.columns:
        schedule['game_date'] = pd.to_datetime(schedule['game_date'])
    else:
        raise ValueError("Schedule missing game_date/gameday column.")
    schedule['season'] = season
    print(f"  Loaded {len(schedule)} games for {season}")
    return schedule


def filter_completed_games(schedule: pd.DataFrame) -> pd.DataFrame:
    """Return only games that have final scores logged."""
    if 'home_score' not in schedule.columns or 'away_score' not in schedule.columns:
        raise ValueError("Schedule is missing score columns needed to identify completed games.")
    
    completed = schedule[
        schedule['home_score'].notna() & schedule['away_score'].notna()
    ].copy()
    
    print(f"  Completed games: {len(completed)}")
    if len(completed) > 0:
        print(f"  Completed through: {completed['game_date'].max().date()}")
    else:
        print("  WARNING: No completed games yet; team-strength features will be empty.")
    
    return completed


def collect_week_10_games(schedule: pd.DataFrame) -> pd.DataFrame:
    """
    Gather Week 10 games for the target window (Nov 6, Nov 9, Nov 10).
    """
    target_dates = pd.to_datetime(['2025-11-06', '2025-11-09', '2025-11-10']).date
    if 'week' not in schedule.columns:
        raise ValueError("Schedule missing 'week' column required to filter Week 10 games.")
    
    mask = (schedule['week'] == 10) & schedule['game_date'].dt.date.isin(target_dates)
    week_10_games = schedule[mask].copy()
    
    if week_10_games.empty:
        raise ValueError("No Week 10 games found for Nov 6/9/10.")
    
    week_10_games = week_10_games.sort_values('game_date').reset_index(drop=True)
    print(f"\nFound {len(week_10_games)} Week 10 games on Nov 6/9/10:")
    for _, row in week_10_games.iterrows():
        print(f"  {row['game_date'].date()} - {row['away_team']} @ {row['home_team']}")
    
    return week_10_games[['home_team', 'away_team', 'game_date', 'season']]


def team_has_data(team: str, game_date: pd.Timestamp, completed_games: pd.DataFrame) -> bool:
    """Check whether the team has at least one current-season completed game before the given date."""
    games_before = completed_games[
        (completed_games['game_date'] < game_date) &
        ((completed_games['home_team'] == team) | (completed_games['away_team'] == team))
    ]
    return len(games_before) > 0


def predict_games(games: pd.DataFrame, schedule: pd.DataFrame, completed_games: pd.DataFrame) -> List[dict]:
    """Predict a batch of games, skipping any without sufficient data."""
    predictor = GamePredictor('NFL', MODEL_VERSION)
    
    predictions = []
    for _, game in games.iterrows():
        game_date = pd.to_datetime(game['game_date'])
        home_team = game['home_team']
        away_team = game['away_team']
        
        if not team_has_data(home_team, game_date, completed_games):
            print(f"\nSkipping {away_team} @ {home_team} ({game_date.date()}): "
                  f"No completed games for {home_team} before this date.")
            continue
        if not team_has_data(away_team, game_date, completed_games):
            print(f"\nSkipping {away_team} @ {home_team} ({game_date.date()}): "
                  f"No completed games for {away_team} before this date.")
            continue
        
        game_df = pd.DataFrame([game])
        result = predictor.predict(game_df, schedule)
        predictions.append(result)
    
    return predictions


def display_predictions(predictions: List[dict]) -> None:
    """Pretty-print prediction results."""
    if not predictions:
        print("\nNo predictions generated.")
        return
    
    print("\n" + "=" * 80)
    print("WEEK 10 PREDICTIONS (NOV 6 / NOV 9 / NOV 10)")
    print("=" * 80)
    
    for pred in predictions:
        print(f"\n{pred['away_team']} @ {pred['home_team']}  |  {pred['game_date']}")
        print(f"  Spread: {pred['predicted_spread']:.2f} ({pred['spread_interpretation']})")
        print(f"  Win Probabilities: Home {pred['home_win_probability']:.1%} | Away {pred['away_win_probability']:.1%}")
        print(f"  Predicted Winner: {pred['predicted_winner']} (Confidence {pred['confidence']:.1%})")
        print(f"  Model win prob: {pred.get('home_win_prob_from_model', float('nan')):.1%}")
        print(f"  From spread: {pred.get('win_prob_from_spread', float('nan')):.1%}")
        if pred.get('model_disagreement', 0) > 0.15:
            print(f"  ⚠️  Disagreement: {pred['model_disagreement']:.1%}")
    
    print("\n" + "=" * 80)

def save_predictions(predictions: List[dict], path: str) -> None:
    """Persist predictions to CSV."""
    if not predictions:
        print("No predictions to save; skipping write.")
        return
    
    df = pd.DataFrame(predictions)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)
    print(f"\nSaved {len(df)} predictions to {path}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Predict Week 10 games and optionally save results.")
    parser.add_argument("--season", type=int, default=2025, help="NFL season to score (default: 2025)")
    parser.add_argument("--save-path", type=str, help="Optional CSV path to save predictions.")
    return parser.parse_args()


def main():
    args = parse_args()
    season = args.season
    try:
        schedule_df = load_season_schedule(season)
    except Exception as err:
        print(f"ERROR: {err}")
        sys.exit(1)
    
    try:
        completed_games = filter_completed_games(schedule_df)
    except Exception as err:
        print(f"ERROR: {err}")
        sys.exit(1)
    
    print(f"\nUsing {season} season data:")
    print(f"  Total games: {len(schedule_df)}")
    print(f"  Date range: {schedule_df['game_date'].min().date()} to {schedule_df['game_date'].max().date()}")
    
    try:
        week_10_games = collect_week_10_games(schedule_df)
    except Exception as err:
        print(f"ERROR: {err}")
        sys.exit(1)
    
    predictions = predict_games(week_10_games, schedule_df, completed_games)
    display_predictions(predictions)
    
    if args.save_path:
        save_predictions(predictions, args.save_path)


if __name__ == "__main__":
    main()



================================================
FILE: predict_WEEK_11.py
================================================
import sys
from typing import List

import pandas as pd

from src.data import nfl_fetcher
from src.models.predictor import GamePredictor


MODEL_VERSION = 'v2'


def load_season_schedule(season: int) -> pd.DataFrame:
    """Fetch the NFL schedule for a season and normalize columns."""
    print(f"Loading {season} NFL season data...")
    schedule = nfl_fetcher.fetch_nfl_schedule(season)
    if 'gameday' in schedule.columns:
        schedule['game_date'] = pd.to_datetime(schedule['gameday'])
    elif 'game_date' in schedule.columns:
        schedule['game_date'] = pd.to_datetime(schedule['game_date'])
    else:
        raise ValueError("Schedule missing game_date/gameday column.")
    schedule['season'] = season
    print(f"  Loaded {len(schedule)} games for {season}")
    return schedule


def filter_completed_games(schedule: pd.DataFrame) -> pd.DataFrame:
    """Return only games that have final scores logged."""
    if 'home_score' not in schedule.columns or 'away_score' not in schedule.columns:
        raise ValueError("Schedule is missing score columns needed to identify completed games.")
    
    completed = schedule[
        schedule['home_score'].notna() & schedule['away_score'].notna()
    ].copy()
    
    print(f"  Completed games: {len(completed)}")
    if len(completed) > 0:
        print(f"  Completed through: {completed['game_date'].max().date()}")
    else:
        print("  WARNING: No completed games yet; team-strength features will be empty.")
    
    return completed


def collect_week_11_games(schedule: pd.DataFrame) -> pd.DataFrame:
    """
    Gather Week 11 games for the target window (Nov 13, Nov 14, Nov 16, Nov 17).
    """
    target_dates = pd.to_datetime([
        '2025-11-13',
        '2025-11-14',
        '2025-11-16',
        '2025-11-17'
    ]).date
    if 'week' not in schedule.columns:
        raise ValueError("Schedule missing 'week' column required to filter Week 11 games.")
    
    mask = (schedule['week'] == 11) & schedule['game_date'].dt.date.isin(target_dates)
    week_11_games = schedule[mask].copy()
    
    if week_11_games.empty:
        raise ValueError("No Week 11 games found for Nov 13/14/16/17.")
    
    week_11_games = week_11_games.sort_values('game_date').reset_index(drop=True)
    print(f"\nFound {len(week_11_games)} Week 11 games on Nov 13/14/16/17:")
    for _, row in week_11_games.iterrows():
        print(f"  {row['game_date'].date()} - {row['away_team']} @ {row['home_team']}")
    
    return week_11_games[['home_team', 'away_team', 'game_date', 'season']]


def team_has_data(team: str, game_date: pd.Timestamp, completed_games: pd.DataFrame) -> bool:
    """Check whether the team has at least one current-season completed game before the given date."""
    games_before = completed_games[
        (completed_games['game_date'] < game_date) &
        ((completed_games['home_team'] == team) | (completed_games['away_team'] == team))
    ]
    return len(games_before) > 0


def predict_games(games: pd.DataFrame, schedule: pd.DataFrame, completed_games: pd.DataFrame) -> List[dict]:
    """Predict a batch of games, skipping any without sufficient data."""
    predictor = GamePredictor('NFL', MODEL_VERSION)
    
    predictions = []
    for _, game in games.iterrows():
        game_date = pd.to_datetime(game['game_date'])
        home_team = game['home_team']
        away_team = game['away_team']
        
        if not team_has_data(home_team, game_date, completed_games):
            print(f"\nSkipping {away_team} @ {home_team} ({game_date.date()}): "
                  f"No completed games for {home_team} before this date.")
            continue
        if not team_has_data(away_team, game_date, completed_games):
            print(f"\nSkipping {away_team} @ {home_team} ({game_date.date()}): "
                  f"No completed games for {away_team} before this date.")
            continue
        
        game_df = pd.DataFrame([game])
        result = predictor.predict(game_df, schedule)
        predictions.append(result)
    
    return predictions


def display_predictions(predictions: List[dict]) -> None:
    """Pretty-print prediction results."""
    if not predictions:
        print("\nNo predictions generated.")
        return
    
    print("\n" + "=" * 80)
    print("WEEK 11 PREDICTIONS (NOV 13 / NOV 14 / NOV 16 / NOV 17)")
    print("=" * 80)
    
    for pred in predictions:
        print(f"\n{pred['away_team']} @ {pred['home_team']}  |  {pred['game_date']}")
        print(f"  Spread: {pred['predicted_spread']:.2f} ({pred['spread_interpretation']})")
        print(f"  Win Probabilities: Home {pred['home_win_probability']:.1%} | Away {pred['away_win_probability']:.1%}")
        print(f"  Predicted Winner: {pred['predicted_winner']} (Confidence {pred['confidence']:.1%})")
        print(f"  Model win prob: {pred.get('home_win_prob_from_model', float('nan')):.1%}")
        print(f"  From spread: {pred.get('win_prob_from_spread', float('nan')):.1%}")
        if pred.get('model_disagreement', 0) > 0.15:
            print(f"  ⚠️  Disagreement: {pred['model_disagreement']:.1%}")
    
    print("\n" + "=" * 80)


def main():
    season = 2025
    try:
        schedule_df = load_season_schedule(season)
    except Exception as err:
        print(f"ERROR: {err}")
        sys.exit(1)
    
    try:
        completed_games = filter_completed_games(schedule_df)
    except Exception as err:
        print(f"ERROR: {err}")
        sys.exit(1)
    
    print(f"\nUsing {season} season data:")
    print(f"  Total games: {len(schedule_df)}")
    print(f"  Date range: {schedule_df['game_date'].min().date()} to {schedule_df['game_date'].max().date()}")
    
    try:
        week_11_games = collect_week_11_games(schedule_df)
    except Exception as err:
        print(f"ERROR: {err}")
        sys.exit(1)
    
    predictions = predict_games(week_11_games, schedule_df, completed_games)
    display_predictions(predictions)


if __name__ == "__main__":
    main()



================================================
FILE: PRODUCTION_USAGE.md
================================================
# Production Usage Guide

## Overview

This guide explains how to go from EDA notebook results to production predictions for specific games.

## Architecture

```
Notebook (EDA) → Model Training → Saved Models → Production Pipeline → Supabase → Website
```

## Step-by-Step Process

### 1. Train and Export Models (Notebook)

Run the notebook cells in order. The **"Export Models for Production Use"** cell will:
- Train win probability and spread models
- Calibrate the link function
- Save everything to `models/` directory with version tags

**Output files:**
- `models/win_prob_model_nfl_v1.pkl`
- `models/spread_model_nfl_v1.pkl`
- `models/link_function_nfl_v1.pkl`
- `models/feature_medians_nfl_v1.pkl`

### 2. Use Production Predictor (Python)

#### Single Game Prediction

```python
from src.models.predictor import GamePredictor
import pandas as pd
from src.data import nfl_fetcher

# Load historical data
schedule_df = nfl_fetcher.fetch_nfl_schedule(2024)
schedule_df['game_date'] = pd.to_datetime(schedule_df['gameday'])

# Initialize predictor
predictor = GamePredictor('NFL', 'v1')

# Predict a specific game
game_row = pd.DataFrame({
    'home_team': ['KC'],
    'away_team': ['BUF'],
    'game_date': ['2025-01-15'],
    'season': [2025]
})

prediction = predictor.predict(game_row, schedule_df)
print(f"Spread: {prediction['predicted_spread']:.2f}")
print(f"Home Win Prob: {prediction['home_win_probability']:.1%}")
```

#### Batch Predictions

```python
# Predict multiple games
future_games = schedule_df[schedule_df['game_date'] > pd.Timestamp.now()].head(10)
predictions_df = predictor.predict_batch(future_games, schedule_df)
```

### 3. Use Production Pipeline (CLI)

The `refresh.py` pipeline automatically:
- Fetches games for a date
- Loads historical data
- Builds features (matching notebook)
- Makes predictions
- Exports to Supabase

```bash
# Predict and export for a specific date
python -m src.pipeline.refresh --league NFL --date 2025-01-15 --model-version v1
```

### 4. Feature Contract

The production system uses the same features as the notebook:

**Core Features:**
- `rest_home`, `rest_away` - Days of rest
- `b2b_home`, `b2b_away` - Back-to-back flags
- `opp_strength_home_season`, `opp_strength_away_season` - Opponent strength

**Team Strength Features:**
- `home_team_win_pct`, `away_team_win_pct` - Season-to-date win %
- `home_team_point_diff`, `away_team_point_diff` - Season-to-date point differential

**Interaction Features:**
- `rest_differential` - Rest advantage
- `win_pct_differential` - Win % advantage
- `point_diff_differential` - Point diff advantage
- `opp_strength_differential` - Opponent strength difference
- `week_number`, `month`, `is_playoff` - Time features

**Form Features (if PBP data available):**
- `form_home_epa_off_3/5/10`, `form_away_epa_off_3/5/10`
- `form_home_epa_def_3/5/10`, `form_away_epa_def_3/5/10`
- `form_epa_off_diff_3/5/10`, `form_epa_def_diff_3/5/10`

### 5. Integration with Supabase

The pipeline automatically:
1. Upserts games to `games` table
2. Inserts odds snapshots to `odds_snapshots` table
3. Inserts predictions to `model_predictions` table
4. Inserts features to `features` table (as JSONB)
5. Logs run to `model_runs` table

**Query today's games:**
```sql
SELECT * FROM games_today_enriched;
```

This view shows:
- `book_spread` - Latest sportsbook spread
- `my_spread` - Our model's spread
- `edge_pts` - Difference (our_spread - book_spread)
- `my_home_win_prob` - Our win probability
- `model_version` - Model version used

### 6. Next.js Integration

The website can query the `games_today_enriched` view via API:

```typescript
// app/api/sports-edges/route.ts
const { data } = await supabase
  .from('games_today_enriched')
  .select('*')
  .order('game_time_utc');
```

## Workflow Summary

1. **Development/EDA**: Use notebook to explore, train, validate
2. **Export**: Run export cell to save models
3. **Production**: Use `GamePredictor` class or `refresh.py` CLI
4. **Automation**: Schedule `refresh.py` to run every 15 minutes
5. **Display**: Website reads from Supabase view

## Key Files

- **Notebook**: `notebooks/nfl_eda.ipynb` - EDA and model training
- **Predictor**: `src/models/predictor.py` - Production prediction class
- **Training**: `src/pipeline/train_models.py` - Model training/export
- **Pipeline**: `src/pipeline/refresh.py` - CLI for daily predictions
- **Models**: `models/*.pkl` - Saved model artifacts

## Example: Complete Workflow

```python
# 1. In notebook: Train and export models
# (Run export cell)

# 2. In production script or CLI:
from src.models.predictor import GamePredictor
from src.data import nfl_fetcher

# Load data
schedule = nfl_fetcher.fetch_nfl_schedule(2024)
schedule['game_date'] = pd.to_datetime(schedule['gameday'])

# Predict
predictor = GamePredictor('NFL', 'v1')
game = pd.DataFrame({
    'home_team': ['KC'],
    'away_team': ['BUF'],
    'game_date': ['2025-01-15'],
    'season': [2025]
})

result = predictor.predict(game, schedule)
print(f"{result['away_team']} @ {result['home_team']}")
print(f"Spread: {result['predicted_spread']:.1f}")
print(f"Home Win: {result['home_win_probability']:.1%}")

# 3. Or use CLI:
# python -m src.pipeline.refresh --league NFL --date 2025-01-15 --model-version v1
```

## Notes

- Models are versioned (e.g., `v1`, `v2`) for easy rollback
- Feature contract is stable - new features require model retraining
- Missing features are filled with medians from training data
- Form features are optional (work without PBP data, just less accurate)




================================================
FILE: requirements.txt
================================================
nba_api>=1.2.1
requests>=2.31.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
lightgbm>=4.0.0
python-dotenv>=1.0.0
supabase>=2.0.0
jupyter>=1.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
scipy>=1.10.0
pyarrow>=12.0.0




================================================
FILE: SETUP.md
================================================
# Sports-Edge: Setup Instructions

## Prerequisites

1. **API Keys**:
   - The Odds API key: https://the-odds-api.com/
   - Supabase project URL and service role key

2. **Python Environment**:
   - Python 3.9+
   - Virtual environment recommended

## Setup Steps

### 1. Install Dependencies

```bash
cd sports-edge
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env and add your API keys
```

### 3. Set Up Supabase Database

1. Go to your Supabase project SQL editor
2. Run the migration script: `sql/001_initial_schema.sql`
3. Verify tables and views are created

### 4. Train Initial Models (Optional)

Models can be trained via notebooks or separate training scripts. For MVP, you can use simple baseline models that will be created on first run.

### 5. Test the Pipeline

```bash
# Test NFL refresh
python -m src.pipeline.refresh --league NFL --date 2025-11-06

# Test NBA refresh
python -m src.pipeline.refresh --league NBA --date 2025-11-06
```

### 6. Set Up GitHub Actions (Optional)

1. Add secrets to your GitHub repository:
   - `ODDS_API_KEY`
   - `SUPABASE_URL`
   - `SUPABASE_SERVICE_ROLE_KEY`

2. The workflow will run automatically every 15 minutes during game days

### 7. Verify Next.js Integration

1. Ensure Supabase environment variables are set in `personal-portfolio/.env.local`:
   ```
   SUPABASE_URL=your_supabase_url
   SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
   ```

2. Start the Next.js dev server:
   ```bash
   cd personal-portfolio
   npm run dev
   ```

3. Navigate to the Work section and verify Sports Edge card appears

## Troubleshooting

- **No games found**: Check that the date is during the season and API keys are valid
- **Database errors**: Verify Supabase schema is migrated and RLS policies are set
- **Model errors**: Ensure model files exist in `models/` directory or models will be skipped




================================================
FILE: sports-edge-analysis-display-cbdcab3c.plan.md
================================================
[Binary file]


================================================
FILE: sports-edge-analysis-plan.md
================================================
# Sports-Edge: Data Exploration & Modeling Plan

> Goal: Build a lightweight, defensible model to compute **our spread** and **home win probability** for NFL/NBA games, refresh it intraday, and benchmark it against sportsbook lines.

---

## 1) Data Sources (MVP)
- **NFL**: `nflreadpy` (or `nfl_data_py` if preferred). Season schedule, team-week aggregates, rolling EPA/success rate where available.
- **NBA**: `nba_api` (team game logs, advanced team box scores).
- **Odds/Lines**: The Odds API (spreads, totals, moneylines). Alternatives: SportsDataIO, Sportradar.
- **Reference Strength** (optional): Elo/SRS from Basketball-Reference (NBA), nflfastR-style EPA-derived strength (NFL).

> We’ll cache raw pulls to `/data/raw/{league}/YYYY-MM-DD/` and curated features to `/data/curated/{league}/YYYY-MM-DD/` for reproducibility.

---

## 2) Notebook-First EDA (Jupyter)
Create one notebook per league under `/notebooks`:
- `nfl_eda.ipynb`
- `nba_eda.ipynb`

### What we’ll inspect
- Distributions: margins, totals, pace (NBA), EPA (NFL).
- **Rest & schedule** features: back-to-backs, 3-in-4, travel distance (Haversine), days rest.
- **Form** windows: rolling (last 3 / 5 / 10) team net rating (NBA) or EPA/success rate (NFL).
- **Correlations** with outcome margin and win%.
- **Leak checks**: ensure no post-game info leaks into pre-game features.

### Quick visuals
- Feature importance via permutation on a simple model.
- Calibration curves for win%.
- Residuals (model margin − closing spread).

---

## 3) Feature Contract (v1)
We’ll lock a minimal, pre-game **feature contract** used by both training & daily scoring:

```
game_id, league, season, game_time_utc,
home_team, away_team,
rest_home, rest_away, b2b_home, b2b_away,
travel_km_home_last, travel_km_away_last,
form_home_net_rating_3, form_away_net_rating_3,     # NBA
form_home_epa_off_3, form_away_epa_off_3,           # NFL
form_home_epa_def_3, form_away_epa_def_3,           # NFL
opp_strength_home_season, opp_strength_away_season
```

Keep this **stable**; add columns only with migration. Store as a JSON in `features` table for flexibility.

---

## 4) Modeling Approach (MVP → v2)
**Targets**
- `home_win ~ logistic(features)` → home win probability (0–1)
- `home_margin ~ regression(features)` → our point spread (home favored = positive)

**Models**
- Start simple: LogisticRegression / Ridge / LightGBM (or XGBoost) for margin.
- v2: Add hierarchical shrinkage or team random effects to stabilize small samples.

**Linking spread ↔ win%**
- Convert `home_margin` to win% using a logistic mapping calibrated on historical game margins.
- Keep the link function under version control (documented).

**Training windows**
- Rolling 2–3 seasons, with early-season priors (blend season priors with latest form).

**Validation**
- Time-based split (train until date T, test on T+).
- Metrics: Brier score & log loss (win%), MAE vs spread (margin), **BTCL%** (Beat The Closing Line).

---

## 5) Data Model in Supabase
Tables (MVP):
- `games(id, league, season, game_time_utc, home_team, away_team)`
- `odds_snapshots(id, game_id, book, market, line, price, snapshot_ts)`
- `model_predictions(id, game_id, model_name, model_version, my_spread, my_home_win_prob, asof_ts)`
- (optional) `features(id, game_id, feature_json, asof_ts)`
- (optional) `model_runs(id, started_at, finished_at, league, rows_written, success, error_text)`

Views:
- `games_today_enriched` → latest odds + latest model per game for same-day display.

---

## 6) Refresh Cadence
- 08:00 America/Denver pre-load (schedule & priors).
- Every 15 minutes until tipoff/kickoff: refresh odds + recompute predictions.
- Final snapshot at event start.

---

## 7) Risks & Mitigations
- **Upstream rate limits** → precompute via cron, cache to DB, UI only SELECTs.
- **Schema drift** → feature contract + SQL migrations.
- **Early-season volatility** → blend team priors / shrinkage.
- **Injuries/uncertainty** → add an optional “what-if” endpoint in v2.



================================================
FILE: sports-edge-export-plan.md
================================================
# Sports-Edge: Export & Deployment Plan (Portfolio Integration)

> Goal: Move cleaned features + model outputs from the Python pipeline into the **Supabase** DB that powers the portfolio, with a repeatable deployment path and auditability.

---

## 1) Environment & Secrets
- **Supabase**: `SUPABASE_URL`, `SUPABASE_SERVICE_ROLE_KEY` (write), `NEXT_PUBLIC_SUPABASE_URL`, `NEXT_PUBLIC_SUPABASE_ANON_KEY` (read).
- **Odds API**: `ODDS_API_KEY`.
- Store secrets:
  - **GitHub Actions** → repository secrets.
  - **Local dev** → `.env`, never commit.

---

## 2) Schema Migration (SQL)
Create migrations in `/supabase/migrations` or keep a `sql/` folder in the analysis repo and run them via CI.

**Tables**
```sql
create table if not exists games (
  id uuid primary key default gen_random_uuid(),
  league text check (league in ('NFL','NBA')) not null,
  season int not null,
  game_time_utc timestamptz not null,
  home_team text not null,
  away_team text not null
);

create table if not exists odds_snapshots (
  id uuid primary key default gen_random_uuid(),
  game_id uuid references games(id) on delete cascade,
  book text not null,
  market text check (market in ('spread','moneyline','total')) not null,
  line numeric,
  price numeric,
  snapshot_ts timestamptz not null default now()
);

create table if not exists model_predictions (
  id uuid primary key default gen_random_uuid(),
  game_id uuid references games(id) on delete cascade,
  model_name text not null,
  model_version text not null,
  my_spread numeric,
  my_home_win_prob numeric,
  asof_ts timestamptz not null default now()
);

create table if not exists features (
  id uuid primary key default gen_random_uuid(),
  game_id uuid references games(id) on delete cascade,
  feature_json jsonb not null,
  asof_ts timestamptz not null default now()
);

create table if not exists model_runs (
  id uuid primary key default gen_random_uuid(),
  league text,
  started_at timestamptz default now(),
  finished_at timestamptz,
  rows_written int,
  success boolean,
  error_text text
);
```

**View (latest odds+pred per game for today)**
```sql
create or replace view games_today_enriched as
select
  g.league, g.season, g.game_time_utc,
  g.home_team, g.away_team,
  o.line as book_spread,
  p.my_spread,
  (p.my_spread - o.line) as edge_pts,
  p.my_home_win_prob,
  p.model_version,
  p.asof_ts
from games g
left join lateral (
  select line
  from odds_snapshots o
  where o.game_id = g.id and o.market = 'spread'
  order by snapshot_ts desc limit 1
) o on true
left join lateral (
  select my_spread, my_home_win_prob, model_version, asof_ts
  from model_predictions p
  where p.game_id = g.id
  order by asof_ts desc limit 1
) p on true
where g.game_time_utc::date = (now() at time zone 'America/Denver')::date
order by g.game_time_utc;
```

**RLS Policies (public read-only)**
```sql
alter table games enable row level security;
alter table odds_snapshots enable row level security;
alter table model_predictions enable row level security;
alter table features enable row level security;
alter table model_runs enable row level security;

create policy "public read games" on games for select using (true);
create policy "public read odds" on odds_snapshots for select using (true);
create policy "public read preds" on model_predictions for select using (true);
create policy "public read features" on features for select using (true);
create policy "public read runs" on model_runs for select using (true);
```

---

## 3) Batch Export Job (Python CLI)
Location: a dedicated repo (e.g., `sports-edge-pipeline/`) or a `/scripts/` folder in your monorepo.

**CLI outline**
```
python -m sports_edge.refresh --league NFL --date 2025-11-06 --runs 1
  # steps:
  # 1) fetch schedule + team form (league-specific)
  # 2) fetch latest odds
  # 3) build features (respect feature contract)
  # 4) run model inference (load artifact)
  # 5) upsert games, odds_snapshots, model_predictions, features
  # 6) write model_runs row (audit)
```

**Idempotency**
- Upsert keyed by `(league, season, home_team, away_team, game_time_utc)` for `games`.
- For odds: `(game_id, book, market, snapshot_ts)`.
- For preds: `(game_id, model_version, asof_ts)`.

---

## 4) Scheduling
Two options (both OK for resume bullet points):
- **Supabase Scheduled Functions** (Edge Functions + `pg_cron`) — low infra, easy secret sharing.
- **GitHub Actions (cron)** — portable: `on: schedule: '*/15 * * * *'`. The job runs Python, writes to Supabase via service key.

---

## 5) CI/CD & Artifacts
- Persist model artifact (`.pkl` or ONNX) with semantic version (e.g., `v0.1.0`) in a release or object storage.
- Every run logs `model_version` into `model_predictions` and `model_runs`.
- Add a small `healthcheck` step to confirm DB connectivity and table existence before writes.

---

## 6) Observability
- Emit run metrics to `model_runs`.
- Optional Slack/Email on failure via GitHub Actions or a Supabase function webhook.



================================================
FILE: sports-edge-website-display-plan.md
================================================
# Sports-Edge: Site Access & Display Plan (Next.js + Supabase)

> Goal: Show **today’s games** with **book spread vs our spread** and **home win%** directly on the portfolio, keeping the UI fast, secure, and decoupled from the modeling job.

---

## 1) Server API Route (Next.js App Router)
Create `app/api/sports-edges/route.ts`:

```ts
import { NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'

export async function GET() {
  const supabase = createClient(
    process.env.SUPABASE_URL!,
    process.env.SUPABASE_SERVICE_ROLE_KEY! // server-only; NEVER expose to client
  )

  const { data, error } = await supabase
    .from('games_today_enriched')
    .select('*')
    .order('game_time_utc', { ascending: true })

  if (error) return NextResponse.json({ error: error.message }, { status: 500 })
  return NextResponse.json(data, {
    headers: { 'Cache-Control': 'public, s-maxage=60, stale-while-revalidate=120' }
  })
}
```

**Why**: server-only secrets, minimal JSON, easy caching, and your React card simply fetches from `/api/sports-edges`.

---

## 2) Client Component (Project Card)
`components/SportsEdgeCard.tsx`

```tsx
'use client'
import { useEffect, useState } from 'react'

type EdgeRow = {
  league: string
  season: number
  game_time_utc: string
  home_team: string
  away_team: string
  book_spread: number | null
  my_spread: number | null
  edge_pts: number | null
  my_home_win_prob: number | null
  model_version: string | null
  asof_ts: string | null
}

export default function SportsEdgeCard() {
  const [rows, setRows] = useState<EdgeRow[]>([])
  const [err, setErr] = useState<string | null>(null)

  useEffect(() => {
    fetch('/api/sports-edges')
      .then(r => r.ok ? r.json() : r.json().then(j => Promise.reject(j.error)))
      .then(setRows)
      .catch(e => setErr(String(e)))
  }, [])

  if (err) return <div className="rounded-xl border p-4 text-red-600 text-sm">Error: {err}</div>

  return (
    <div className="rounded-2xl border p-4">
      <div className="mb-2 text-sm opacity-70">Today’s Edges (model vs books)</div>
      <div className="grid grid-cols-1 md:grid-cols-2 gap-3">
        {rows.map((r, i) => (
          <div key={i} className="rounded-xl border p-3">
            <div className="text-xs opacity-60">
              {r.league} • {new Date(r.game_time_utc).toLocaleString()}
            </div>
            <div className="text-sm font-medium mt-1">
              {r.away_team} @ {r.home_team}
            </div>
            <div className="text-xs mt-1">
              Book: {r.book_spread?.toFixed(1) ?? '—'} | Ours: {r.my_spread?.toFixed(1) ?? '—'}
            </div>
            <div className={`text-xs mt-1 ${Math.abs(r.edge_pts ?? 0) >= 1.0 ? 'text-emerald-600' : 'opacity-70'}`}>
              Edge: {r.edge_pts?.toFixed(1) ?? '—'} pts • Home win: {r.my_home_win_prob != null ? (100*r.my_home_win_prob).toFixed(1) + '%' : '—'}
            </div>
            <div className="text-[10px] opacity-60 mt-1">
              Updated: {r.asof_ts ? new Date(r.asof_ts).toLocaleTimeString() : '—'} • v{r.model_version ?? '—'}
            </div>
          </div>
        ))}
      </div>
    </div>
  )
}
```

Place `<SportsEdgeCard />` inside your **Work/Projects** section card.

---

## 3) Optional: Dedicated Page
If you want deeper charts/tables, create `app/projects/sports-edge/page.tsx` that SSR-fetches `/api/sports-edges` and renders a table with sorting/filtering.

---

## 4) Performance & Reliability
- **Caching**: 60s CDN cache on API route; client re-fetch on focus for freshness.
- **Fallback**: show placeholder “No games today” if array empty.
- **Partial data**: gracefully handle `null` spreads/preds when upstream fetch lags.
- **Timezone**: format using America/Denver for display; DB stores UTC.

---

## 5) Security
- Enable **RLS** with read-only policies for public.
- Only the API route uses the **service role key**; the client never does.
- For admin-only routes (e.g., backfills), guard with auth middleware.

---

## 6) QA Checklist
- [ ] API route returns 200 with array (today or empty).
- [ ] Card renders at least one row on a game day.
- [ ] Edge sign logic confirmed: `edge_pts = my_spread - book_spread`.
- [ ] Accessibility: readable at mobile widths; semantic headings.
- [ ] No PII or secrets in client bundles.



================================================
FILE: notebooks/nba_eda.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# NBA EDA: Data Exploration & Feature Analysis

This notebook explores NBA data, computes features, and analyzes correlations with game outcomes.

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import sys
import os

# Add src to path
sys.path.insert(0, os.path.join(os.getcwd(), '..'))

from src.data import nba_fetcher
from src.features import rest_schedule, form_metrics, strength

sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)




================================================
FILE: notebooks/nfl_eda.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# NFL EDA: Data Exploration & Feature Analysis

This notebook explores NFL data, computes features, and analyzes correlations with game outcomes.

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import sys
import os

# Add src to path
sys.path.insert(0, os.path.join(os.getcwd(), '..'))

from src.data import nfl_fetcher
from src.features import rest_schedule, form_metrics, strength

sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)


# Load Historical NFL Data (2020-2025)
seasons = [2020, 2021, 2022, 2023, 2024, 2025]
all_schedules = []

print("Loading NFL schedule data...")
for season in seasons:
    try:
        schedule = nfl_fetcher.fetch_nfl_schedule(season)
        all_schedules.append(schedule)
        print(f"Loaded {season}: {len(schedule)} games")
    except Exception as e:
        print(f"Error loading {season}: {e}")

if all_schedules:
    schedule_df = pd.concat(all_schedules, ignore_index=True)
    print(f"\nTotal games loaded: {len(schedule_df)}")
    
    # Ensure we have game_date column
    if 'gameday' in schedule_df.columns:
        schedule_df['game_date'] = pd.to_datetime(schedule_df['gameday'])
    elif 'game_date' not in schedule_df.columns:
        # Try other common date column names
        date_col = None
        for col in ['date', 'game_time', 'start_time']:
            if col in schedule_df.columns:
                date_col = col
                break
        if date_col:
            schedule_df['game_date'] = pd.to_datetime(schedule_df[date_col])
        else:
            print("Warning: Could not find date column. Please check schedule columns.")
            print("Available columns:", schedule_df.columns.tolist())
    
    # Compute margin and home win
    if 'home_score' in schedule_df.columns and 'away_score' in schedule_df.columns:
        schedule_df['margin'] = schedule_df['home_score'] - schedule_df['away_score']
        schedule_df['home_win'] = (schedule_df['margin'] > 0).astype(int)
        print(f"\nGames with scores: {schedule_df['home_win'].notna().sum()}")
        print(f"Home win rate: {schedule_df['home_win'].mean():.3f}")
    else:
        print("Warning: Score columns not found. Available columns:", schedule_df.columns.tolist())
    
    # Display summary
    print("\nSchedule DataFrame Info:")
    print(schedule_df.head())
    print(f"\nDate range: {schedule_df['game_date'].min()} to {schedule_df['game_date'].max()}")
else:
    print("No schedule data loaded!")

# Output:
#   Loading NFL schedule data...

#   Loaded 2020: 269 games

#   Loaded 2021: 285 games

#   Loaded 2022: 284 games

#   Loaded 2023: 285 games

#   Loaded 2024: 285 games

#   Loaded 2025: 272 games

#   

#   Total games loaded: 1680

#   

#   Games with scores: 1680

#   Home win rate: 0.495

#   

#   Schedule DataFrame Info:

#              game_id  season game_type  week     gameday   weekday gametime  \

#   0   2020_01_HOU_KC    2020       REG     1  2020-09-10  Thursday    20:20   

#   1  2020_01_SEA_ATL    2020       REG     1  2020-09-13    Sunday    13:00   

#   2  2020_01_CLE_BAL    2020       REG     1  2020-09-13    Sunday    13:00   

#   3  2020_01_NYJ_BUF    2020       REG     1  2020-09-13    Sunday    13:00   

#   4   2020_01_LV_CAR    2020       REG     1  2020-09-13    Sunday    13:00   

#   

#     away_team  away_score home_team  ...    away_qb_name       home_qb_name  \

#   0       HOU        20.0        KC  ...  Deshaun Watson    Patrick Mahomes   

#   1       SEA        38.0       ATL  ...  Russell Wilson          Matt Ryan   

#   2       CLE         6.0       BAL  ...  Baker Mayfield      Lamar Jackson   

#   3       NYJ        17.0       BUF  ...     Sam Darnold         Josh Allen   

#   4        LV        34.0       CAR  ...      Derek Carr  Teddy Bridgewater   

#   

#           away_coach      home_coach         referee  stadium_id  \

#   0     Bill O'Brien       Andy Reid  Clete Blakeman       KAN00   

#   1     Pete Carroll       Dan Quinn   Shawn Hochuli       ATL97   

#   2  Kevin Stefanski   John Harbaugh  Ronald Torbert       BAL00   

#   3        Adam Gase  Sean McDermott     Shawn Smith       BUF00   

#   4       Jon Gruden      Matt Rhule      Brad Allen       CAR00   

#   

#                      stadium  game_date margin  home_win  

#   0        Arrowhead Stadium 2020-09-10   14.0         1  

#   1    Mercedes-Benz Stadium 2020-09-13  -13.0         0  

#   2         M&T Bank Stadium 2020-09-13   32.0         1  

#   3            New Era Field 2020-09-13   10.0         1  

#   4  Bank of America Stadium 2020-09-13   -4.0         0  

#   

#   [5 rows x 49 columns]

#   

#   Date range: 2020-09-10 00:00:00 to 2026-01-04 00:00:00


# Apply Rest & Schedule Features
if 'schedule_df' in locals() and len(schedule_df) > 0:
    print("Computing rest and schedule features...")
    print("This may take a few minutes...")
    
    # Add rest features
    features_df = rest_schedule.add_rest_features(schedule_df, schedule_df)
    
    print("\nRest features added:")
    print(f"  - rest_home: {features_df['rest_home'].notna().sum()} values")
    print(f"  - rest_away: {features_df['rest_away'].notna().sum()} values")
    print(f"  - b2b_home: {features_df['b2b_home'].sum()} back-to-back games")
    print(f"  - b2b_away: {features_df['b2b_away'].sum()} back-to-back games")
    
    # Analyze rest day distributions
    if 'rest_home' in features_df.columns:
        print("\nRest Days Distribution (Home Teams):")
        rest_dist = features_df['rest_home'].value_counts().sort_index()
        print(rest_dist.head(10))
        
        # Visualize rest days impact
        if 'home_win' in features_df.columns:
            rest_analysis = features_df.groupby('rest_home')['home_win'].agg(['mean', 'count']).reset_index()
            rest_analysis.columns = ['rest_days', 'win_pct', 'games']
            rest_analysis = rest_analysis[rest_analysis['games'] >= 10]  # Filter to meaningful sample sizes
            
            plt.figure(figsize=(12, 6))
            plt.bar(rest_analysis['rest_days'], rest_analysis['win_pct'], alpha=0.7)
            plt.xlabel('Days of Rest (Home Team)', fontsize=12)
            plt.ylabel('Home Win Percentage', fontsize=12)
            plt.title('Home Win Rate by Days of Rest', fontsize=14)
            plt.grid(axis='y', alpha=0.3)
            plt.tight_layout()
            plt.show()
            
            print("\nWin % by Rest Days (Home):")
            print(rest_analysis)
else:
    print("Schedule data not available. Run the previous cell first.")

# Output:
#   Computing rest and schedule features...

#   This may take a few minutes...

#   

#   Rest features added:

#     - rest_home: 1584 values

#     - rest_away: 1584 values

#     - b2b_home: 0 back-to-back games

#     - b2b_away: 0 back-to-back games

#   

#   Rest Days Distribution (Home Teams):

#   rest_home

#   4.0      99

#   5.0      11

#   6.0     152

#   7.0     966

#   8.0     132

#   9.0       9

#   10.0     88

#   11.0      9

#   13.0     23

#   14.0     85

#   Name: count, dtype: int64

#   <Figure size 1200x600 with 1 Axes>
#   

#   Win % by Rest Days (Home):

#      rest_days   win_pct  games

#   0        4.0  0.484848     99

#   1        5.0  0.363636     11

#   2        6.0  0.546053    152

#   3        7.0  0.481366    966

#   4        8.0  0.500000    132

#   6       10.0  0.500000     88

#   8       13.0  0.608696     23

#   9       14.0  0.552941     85


# Apply Form Features (EPA) - Using nfl-data-py
if 'features_df' in locals():
    print("Loading play-by-play data for form metrics...")
    print("Note: This may take several minutes and requires significant memory.")
    
    try:
        import nfl_data_py as nfl
        pbp_data = []
        
        for season in seasons:
            try:
                print(f"Loading PBP for {season}...")
                pbp = nfl.import_pbp_data([season])
                if len(pbp) > 0:
                    pbp_data.append(pbp)
                    print(f"  Loaded {len(pbp)} plays for {season}")
                else:
                    print(f"  No data available for {season} (may be future season)")
            except Exception as e:
                print(f"  Could not load PBP for {season}: {e}")
        
        if pbp_data:
            pbp_df = pd.concat(pbp_data, ignore_index=True)
            
            # Ensure game_date column exists (nfl_data_py uses 'game_date')
            if 'game_date' in pbp_df.columns:
                pbp_df['game_date'] = pd.to_datetime(pbp_df['game_date'])
            elif 'gameday' in pbp_df.columns:
                pbp_df['game_date'] = pd.to_datetime(pbp_df['gameday'])
            else:
                print("Warning: Could not find date column in PBP data")
                print("Available columns:", pbp_df.columns.tolist()[:20])
                pbp_df = None
            
            if pbp_df is not None:
                print(f"\nTotal plays loaded: {len(pbp_df)}")
                print("Computing form features (this may take a while)...")
                
                # Add form features for different windows
                for window in [3, 5, 10]:
                    print(f"  Computing {window}-game rolling EPA...")
                    features_df = form_metrics.add_form_features_nfl(features_df, pbp_df, window=window)
                
                print("\nForm features added:")
                form_cols = [col for col in features_df.columns if col.startswith('form_')]
                for col in form_cols:
                    non_null = features_df[col].notna().sum()
                    print(f"  - {col}: {non_null} values")
        else:
            print("No play-by-play data loaded. Skipping form features.")
            
    except ImportError:
        print("nfl-data-py not available. Install with: pip install nfl-data-py")
        print("Skipping form features.")
    except Exception as e:
        print(f"Error loading play-by-play data: {e}")
        import traceback
        traceback.print_exc()
        print("Continuing without form features...")
else:
    print("Features DataFrame not available. Run previous cells first.")

# Output:
#   Loading play-by-play data for form metrics...

#   Note: This may take several minutes and requires significant memory.

#   Loading PBP for 2020...

#   2020 done.

#   Downcasting floats.

#     Loaded 47705 plays for 2020

#   Loading PBP for 2021...

#   2021 done.

#   Downcasting floats.

#     Loaded 49922 plays for 2021

#   Loading PBP for 2022...

#   2022 done.

#   Downcasting floats.

#     Loaded 49434 plays for 2022

#   Loading PBP for 2023...

#   2023 done.

#   Downcasting floats.

#     Loaded 49665 plays for 2023

#   Loading PBP for 2024...

#   2024 done.

#   Downcasting floats.

#     Loaded 49492 plays for 2024

#   Loading PBP for 2025...

#     Could not load PBP for 2025: name 'Error' is not defined

#   

#   Total plays loaded: 246218

#   Computing form features (this may take a while)...

#     Computing 3-game rolling EPA...

#     Computing 5-game rolling EPA...

#     Computing 10-game rolling EPA...

#   

#   Form features added:

#     - form_home_epa_off_3: 1632 values

#     - form_away_epa_off_3: 1632 values

#     - form_home_epa_def_3: 1632 values

#     - form_away_epa_def_3: 1632 values

#     - form_home_epa_off_5: 1600 values

#     - form_away_epa_off_5: 1600 values

#     - form_home_epa_def_5: 1600 values

#     - form_away_epa_def_5: 1600 values

#     - form_home_epa_off_10: 1521 values

#     - form_away_epa_off_10: 1519 values

#     - form_home_epa_def_10: 1521 values

#     - form_away_epa_def_10: 1519 values


# Apply Opponent Strength Features
if 'features_df' in locals():
    print("Computing opponent strength features...")
    print("This may take a few minutes...")
    
    features_df = strength.add_opponent_strength_features(features_df, schedule_df, league='nfl')
    
    print("\nOpponent strength features added:")
    print(f"  - opp_strength_home_season: {features_df['opp_strength_home_season'].notna().sum()} values")
    print(f"  - opp_strength_away_season: {features_df['opp_strength_away_season'].notna().sum()} values")
    
    if 'opp_strength_home_season' in features_df.columns:
        print(f"\nOpponent Strength Stats:")
        print(f"  Home team avg: {features_df['opp_strength_home_season'].mean():.2f}")
        print(f"  Away team avg: {features_df['opp_strength_away_season'].mean():.2f}")
else:
    print("Features DataFrame not available. Run previous cells first.")

# Output:
#   Computing opponent strength features...

#   This may take a few minutes...

#   

#   Opponent strength features added:

#     - opp_strength_home_season: 1584 values

#     - opp_strength_away_season: 1584 values

#   

#   Opponent Strength Stats:

#     Home team avg: 0.08

#     Away team avg: -0.04


# Add Team Strength Features (Season-to-Date Performance)
if 'features_df' in locals() and 'schedule_df' in locals():
    print("Computing team strength features...")
    
    # Calculate season-to-date win percentage and point differential for each team
    def compute_team_strength_features(games_df, historical_games):
        df = games_df.copy()
        
        # Initialize columns
        df['home_team_win_pct'] = np.nan
        df['away_team_win_pct'] = np.nan
        df['home_team_point_diff'] = np.nan
        df['away_team_point_diff'] = np.nan
        
        for idx, row in df.iterrows():
            game_date = pd.to_datetime(row['game_date'])
            home_team = row['home_team']
            away_team = row['away_team']
            season = row.get('season', game_date.year)
            
            # Get all games before this date in the same season
            season_games = historical_games[
                (pd.to_datetime(historical_games['game_date']) < game_date) &
                (historical_games.get('season', pd.to_datetime(historical_games['game_date']).dt.year) == season)
            ]
            
            # Home team stats
            home_games = season_games[
                (season_games['home_team'] == home_team) | (season_games['away_team'] == home_team)
            ]
            if len(home_games) > 0:
                home_wins = 0
                home_point_diff = []
                for _, game in home_games.iterrows():
                    if game['home_team'] == home_team:
                        if game.get('home_score', 0) > game.get('away_score', 0):
                            home_wins += 1
                        home_point_diff.append(game.get('home_score', 0) - game.get('away_score', 0))
                    else:
                        if game.get('away_score', 0) > game.get('home_score', 0):
                            home_wins += 1
                        home_point_diff.append(game.get('away_score', 0) - game.get('home_score', 0))
                
                df.loc[idx, 'home_team_win_pct'] = home_wins / len(home_games) if len(home_games) > 0 else 0.5
                df.loc[idx, 'home_team_point_diff'] = np.mean(home_point_diff) if home_point_diff else 0
            
            # Away team stats
            away_games = season_games[
                (season_games['home_team'] == away_team) | (season_games['away_team'] == away_team)
            ]
            if len(away_games) > 0:
                away_wins = 0
                away_point_diff = []
                for _, game in away_games.iterrows():
                    if game['home_team'] == away_team:
                        if game.get('home_score', 0) > game.get('away_score', 0):
                            away_wins += 1
                        away_point_diff.append(game.get('home_score', 0) - game.get('away_score', 0))
                    else:
                        if game.get('away_score', 0) > game.get('home_score', 0):
                            away_wins += 1
                        away_point_diff.append(game.get('away_score', 0) - game.get('home_score', 0))
                
                df.loc[idx, 'away_team_win_pct'] = away_wins / len(away_games) if len(away_games) > 0 else 0.5
                df.loc[idx, 'away_team_point_diff'] = np.mean(away_point_diff) if away_point_diff else 0
        
        return df
    
    features_df = compute_team_strength_features(features_df, schedule_df)
    
    print("\nTeam strength features added:")
    print(f"  - home_team_win_pct: {features_df['home_team_win_pct'].notna().sum()} values")
    print(f"  - away_team_win_pct: {features_df['away_team_win_pct'].notna().sum()} values")
    print(f"  - home_team_point_diff: {features_df['home_team_point_diff'].notna().sum()} values")
    print(f"  - away_team_point_diff: {features_df['away_team_point_diff'].notna().sum()} values")
    
    if 'home_team_win_pct' in features_df.columns:
        print(f"\nTeam Strength Stats:")
        print(f"  Home team avg win %: {features_df['home_team_win_pct'].mean():.3f}")
        print(f"  Away team avg win %: {features_df['away_team_win_pct'].mean():.3f}")
else:
    print("Features DataFrame or schedule_df not available.")

# Output:
#   Computing team strength features...

#   

#   Team strength features added:

#     - home_team_win_pct: 1584 values

#     - away_team_win_pct: 1584 values

#     - home_team_point_diff: 1464 values

#     - away_team_point_diff: 1464 values

#   

#   Team Strength Stats:

#     Home team avg win %: 0.495

#     Away team avg win %: 0.494


# Add Interaction and Derived Features
if 'features_df' in locals():
    print("Adding interaction and derived features...")
    
    # Rest differential
    if 'rest_home' in features_df.columns and 'rest_away' in features_df.columns:
        features_df['rest_differential'] = features_df['rest_home'] - features_df['rest_away']
        features_df['rest_advantage_home'] = (features_df['rest_home'] > features_df['rest_away']).astype(int)
        print("  - Added rest_differential and rest_advantage_home")
    
    # Team strength differentials
    if 'home_team_win_pct' in features_df.columns and 'away_team_win_pct' in features_df.columns:
        features_df['win_pct_differential'] = features_df['home_team_win_pct'] - features_df['away_team_win_pct']
        print("  - Added win_pct_differential")
    
    if 'home_team_point_diff' in features_df.columns and 'away_team_point_diff' in features_df.columns:
        features_df['point_diff_differential'] = features_df['home_team_point_diff'] - features_df['away_team_point_diff']
        print("  - Added point_diff_differential")
    
    # Opponent strength differential
    if 'opp_strength_home_season' in features_df.columns and 'opp_strength_away_season' in features_df.columns:
        features_df['opp_strength_differential'] = features_df['opp_strength_home_season'] - features_df['opp_strength_away_season']
        print("  - Added opp_strength_differential")
    
    # Week and time features
    if 'game_date' in features_df.columns:
        features_df['week_number'] = pd.to_datetime(features_df['game_date']).dt.isocalendar().week
        features_df['month'] = pd.to_datetime(features_df['game_date']).dt.month
        features_df['is_playoff'] = features_df.get('game_type', '').str.contains('POST', case=False, na=False).astype(int)
        print("  - Added week_number, month, is_playoff")
    
    # Form differentials (if form features exist)
    form_cols = [col for col in features_df.columns if col.startswith('form_')]
    if form_cols:
        # Find matching home/away pairs
        for window in [3, 5, 10]:
            home_off = f'form_home_epa_off_{window}'
            away_off = f'form_away_epa_off_{window}'
            if home_off in features_df.columns and away_off in features_df.columns:
                features_df[f'form_epa_off_diff_{window}'] = features_df[home_off] - features_df[away_off]
                print(f"  - Added form_epa_off_diff_{window}")
            
            home_def = f'form_home_epa_def_{window}'
            away_def = f'form_away_epa_def_{window}'
            if home_def in features_df.columns and away_def in features_df.columns:
                features_df[f'form_epa_def_diff_{window}'] = features_df[home_def] - features_df[away_def]
                print(f"  - Added form_epa_def_diff_{window}")
    
    print("\nInteraction features added successfully!")
else:
    print("Features DataFrame not available.")

# Output:
#   Adding interaction and derived features...

#     - Added rest_differential and rest_advantage_home

#     - Added win_pct_differential

#     - Added point_diff_differential

#     - Added opp_strength_differential

#     - Added week_number, month, is_playoff

#     - Added form_epa_off_diff_3

#     - Added form_epa_def_diff_3

#     - Added form_epa_off_diff_5

#     - Added form_epa_def_diff_5

#     - Added form_epa_off_diff_10

#     - Added form_epa_def_diff_10

#   

#   Interaction features added successfully!


# Correlation Analysis with Home Win
if 'features_df' in locals() and 'home_win' in features_df.columns:
    # Collect all feature columns
    feature_cols = [
        'rest_home', 'rest_away', 'b2b_home', 'b2b_away',
        'opp_strength_home_season', 'opp_strength_away_season',
        'home_team_win_pct', 'away_team_win_pct',
        'home_team_point_diff', 'away_team_point_diff',
        'rest_differential', 'rest_advantage_home',
        'win_pct_differential', 'point_diff_differential',
        'opp_strength_differential', 'week_number', 'month', 'is_playoff'
    ]
    
    # Add form features if they exist
    form_cols = [col for col in features_df.columns if col.startswith('form_')]
    feature_cols.extend(form_cols)
    
    # Add interaction features
    interaction_cols = [col for col in features_df.columns if 'diff' in col.lower() or 'differential' in col.lower()]
    feature_cols.extend(interaction_cols)
    
    # Filter to columns that exist and have data
    available_cols = [col for col in feature_cols if col in features_df.columns]
    available_cols = [col for col in available_cols if features_df[col].notna().sum() > 100]
    
    if available_cols:
        print(f"Analyzing {len(available_cols)} features...")
        
        # Compute correlations
        corr_data = features_df[available_cols + ['home_win']].corr()['home_win'].drop('home_win')
        corr_data = corr_data.sort_values(ascending=False)
        
        # Create visualization
        plt.figure(figsize=(10, max(8, len(available_cols) * 0.5)))
        corr_data.plot(kind='barh', color=['green' if x > 0 else 'red' for x in corr_data.values])
        plt.xlabel('Correlation with Home Win', fontsize=12)
        plt.ylabel('Feature', fontsize=12)
        plt.title('Feature Correlations with Home Win Probability', fontsize=14)
        plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        print("\nTop Features Affecting Home Win (by correlation):")
        print("=" * 60)
        for idx, (feature, corr) in enumerate(corr_data.head(10).items(), 1):
            print(f"{idx:2d}. {feature:35s} {corr:7.4f}")
        
        print("\nBottom Features (negative correlation):")
        print("=" * 60)
        for idx, (feature, corr) in enumerate(corr_data.tail(5).items(), 1):
            print(f"{idx:2d}. {feature:35s} {corr:7.4f}")
    else:
        print("No feature columns available for correlation analysis.")
else:
    print("Features DataFrame or home_win column not available.")

# Output:
#   Analyzing 48 features...

#   <Figure size 1000x2400 with 1 Axes>
#   

#   Top Features Affecting Home Win (by correlation):

#   ============================================================

#    1. form_epa_off_diff_10                 0.2609

#    2. form_epa_off_diff_10                 0.2609

#    3. point_diff_differential              0.2298

#    4. point_diff_differential              0.2298

#    5. form_epa_off_diff_5                  0.2249

#    6. form_epa_off_diff_5                  0.2249

#    7. win_pct_differential                 0.2085

#    8. win_pct_differential                 0.2085

#    9. form_epa_off_diff_3                  0.2063

#   10. form_epa_off_diff_3                  0.2063

#   

#   Bottom Features (negative correlation):

#   ============================================================

#    1. form_away_epa_off_5                 -0.1536

#    2. form_away_epa_off_10                -0.1882

#    3. b2b_home                                nan

#    4. b2b_away                                nan

#    5. is_playoff                              nan


# Feature Importance via Random Forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from lightgbm import LGBMClassifier

if 'features_df' in locals() and 'home_win' in features_df.columns:
    # Collect feature columns
    feature_cols = [
        'rest_home', 'rest_away', 'b2b_home', 'b2b_away',
        'opp_strength_home_season', 'opp_strength_away_season',
        'home_team_win_pct', 'away_team_win_pct',
        'home_team_point_diff', 'away_team_point_diff',
        'rest_differential', 'rest_advantage_home',
        'win_pct_differential', 'point_diff_differential',
        'opp_strength_differential', 'week_number', 'month', 'is_playoff'
    ]
    
    # Add form features if they exist
    form_cols = [col for col in features_df.columns if col.startswith('form_')]
    feature_cols.extend(form_cols)
    
    # Add interaction features
    interaction_cols = [col for col in features_df.columns if 'diff' in col.lower() or 'differential' in col.lower()]
    feature_cols.extend(interaction_cols)
    
    # Filter to available columns AND remove duplicates
    available_cols = [col for col in feature_cols if col in features_df.columns]
    # Remove duplicates while preserving order
    seen = set()
    available_cols = [col for col in available_cols if not (col in seen or seen.add(col))]
    
    # Prepare data (drop NaN rows)
    model_df = features_df[available_cols + ['home_win']].dropna()
    
    print(f"Data points available: {len(model_df)}")
    print(f"Features: {len(available_cols)}")
    
    if len(model_df) > 100:
        X = model_df[available_cols]
        y = model_df['home_win']
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"\nTraining set: {len(X_train)} samples")
        print(f"Test set: {len(X_test)} samples")
        
        # Train Random Forest
        print("\nTraining Random Forest classifier...")
        rf = RandomForestClassifier(
            n_estimators=200, 
            max_depth=12, 
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        )
        rf.fit(X_train, y_train)
        
        # Also train LightGBM (often better for tabular data)
        print("Training LightGBM classifier...")
        try:
            lgbm = LGBMClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=7,
                num_leaves=31,
                random_state=42,
                verbose=-1
            )
            lgbm.fit(X_train, y_train)
            use_lgbm = True
        except Exception as e:
            print(f"LightGBM failed: {e}")
            print("Using Random Forest only")
            use_lgbm = False
        
        # Predictions
        y_pred_rf = rf.predict(X_test)
        y_pred_proba_rf = rf.predict_proba(X_test)[:, 1]
        
        if use_lgbm:
            y_pred_lgbm = lgbm.predict(X_test)
            y_pred_proba_lgbm = lgbm.predict_proba(X_test)[:, 1]
        
        # Metrics
        accuracy_rf = accuracy_score(y_test, y_pred_rf)
        print(f"\nRandom Forest Performance:")
        print(f"  Accuracy: {accuracy_rf:.4f}")
        print(f"\nClassification Report:")
        print(classification_report(y_test, y_pred_rf, target_names=['Away Win', 'Home Win']))
        
        if use_lgbm:
            accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)
            print(f"\nLightGBM Performance:")
            print(f"  Accuracy: {accuracy_lgbm:.4f}")
            print(f"\nClassification Report:")
            print(classification_report(y_test, y_pred_lgbm, target_names=['Away Win', 'Home Win']))
        
        # Feature importance (use best model)
        if use_lgbm and accuracy_lgbm > accuracy_rf:
            best_model = lgbm
            best_name = "LightGBM"
            importance_values = lgbm.feature_importances_
        else:
            best_model = rf
            best_name = "Random Forest"
            importance_values = rf.feature_importances_
        
        # Ensure available_cols and importance_values have same length
        # (should be fine after removing duplicates, but double-check)
        assert len(available_cols) == len(importance_values), \
            f"Mismatch: {len(available_cols)} features but {len(importance_values)} importance values"
        
        importance_df = pd.DataFrame({
            'feature': available_cols,
            'importance': importance_values
        }).sort_values('importance', ascending=False)
        
        # Visualize feature importance
        plt.figure(figsize=(10, max(8, len(available_cols) * 0.5)))
        importance_df.head(20).plot(x='feature', y='importance', kind='barh', legend=False, color='steelblue')
        plt.xlabel('Feature Importance', fontsize=12)
        plt.ylabel('Feature', fontsize=12)
        plt.title(f'{best_name} Feature Importance for Home Win Prediction (Top 20)', fontsize=14)
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        plt.show()
        
        print(f"\nFeature Importance Rankings ({best_name}):")
        print("=" * 60)
        for idx, row in importance_df.head(20).iterrows():
            print(f"{idx + 1:2d}. {row['feature']:40s} {row['importance']:7.4f}")
    else:
        print(f"Not enough data: {len(model_df)} rows (need >100)")
        print("Try loading more seasons or check data availability.")
else:
    print("Features DataFrame or home_win column not available.")
# Output:
#   Data points available: 1318

#   Features: 36

#   

#   Training set: 1054 samples

#   Test set: 264 samples

#   

#   Training Random Forest classifier...

#   Training LightGBM classifier...

#   

#   Random Forest Performance:

#     Accuracy: 0.6136

#   

#   Classification Report:

#                 precision    recall  f1-score   support

#   

#       Away Win       0.58      0.62      0.60       122

#       Home Win       0.65      0.61      0.63       142

#   

#       accuracy                           0.61       264

#      macro avg       0.61      0.61      0.61       264

#   weighted avg       0.62      0.61      0.61       264

#   

#   

#   LightGBM Performance:

#     Accuracy: 0.5947

#   

#   Classification Report:

#                 precision    recall  f1-score   support

#   

#       Away Win       0.56      0.57      0.56       122

#       Home Win       0.62      0.62      0.62       142

#   

#       accuracy                           0.59       264

#      macro avg       0.59      0.59      0.59       264

#   weighted avg       0.59      0.59      0.59       264

#   

#   <Figure size 1000x1800 with 0 Axes>
#   <Figure size 1200x600 with 1 Axes>
#   

#   Feature Importance Rankings (Random Forest):

#   ============================================================

#   35. form_epa_off_diff_10                      0.0715

#   33. form_epa_off_diff_5                       0.0522

#    9. home_team_point_diff                      0.0417

#   13. win_pct_differential                      0.0414

#   19. form_home_epa_off_3                       0.0412

#   28. form_away_epa_off_10                      0.0411

#   31. form_epa_off_diff_3                       0.0396

#   14. point_diff_differential                   0.0395

#   23. form_home_epa_off_5                       0.0364

#   20. form_away_epa_off_3                       0.0361

#   27. form_home_epa_off_10                      0.0352

#    6. opp_strength_away_season                  0.0347

#   10. away_team_point_diff                      0.0332

#   29. form_home_epa_def_10                      0.0329

#   30. form_away_epa_def_10                      0.0315

#   15. opp_strength_differential                 0.0311

#   24. form_away_epa_off_5                       0.0303

#    5. opp_strength_home_season                  0.0300

#   36. form_epa_def_diff_10                      0.0296

#    8. away_team_win_pct                         0.0291


# Situational Analysis: Deep Dive into Specific Factors
if 'features_df' in locals() and 'home_win' in features_df.columns:
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Rest Days Impact
    if 'rest_home' in features_df.columns:
        rest_analysis = features_df.groupby('rest_home')['home_win'].agg(['mean', 'count']).reset_index()
        rest_analysis.columns = ['rest_days', 'win_pct', 'games']
        rest_analysis = rest_analysis[rest_analysis['games'] >= 10]
        
        axes[0, 0].bar(rest_analysis['rest_days'], rest_analysis['win_pct'], alpha=0.7, color='steelblue')
        axes[0, 0].axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='50% baseline')
        axes[0, 0].set_xlabel('Days of Rest (Home Team)', fontsize=11)
        axes[0, 0].set_ylabel('Home Win Percentage', fontsize=11)
        axes[0, 0].set_title('Home Win Rate by Days of Rest', fontsize=12)
        axes[0, 0].grid(axis='y', alpha=0.3)
        axes[0, 0].legend()
    
    # 2. Back-to-Back Impact
    if 'b2b_home' in features_df.columns:
        b2b_analysis = features_df.groupby('b2b_home')['home_win'].agg(['mean', 'count']).reset_index()
        b2b_analysis.columns = ['is_b2b', 'win_pct', 'games']
        
        axes[0, 1].bar(['No B2B', 'B2B'], b2b_analysis['win_pct'], alpha=0.7, color=['green', 'orange'])
        axes[0, 1].axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='50% baseline')
        axes[0, 1].set_ylabel('Home Win Percentage', fontsize=11)
        axes[0, 1].set_title('Home Win Rate: Back-to-Back vs Normal Rest', fontsize=12)
        axes[0, 1].grid(axis='y', alpha=0.3)
        axes[0, 1].legend()
        axes[0, 1].set_ylim([0, 1])
        
        print("Back-to-Back Analysis:")
        print(b2b_analysis)
    
    # 3. Opponent Strength Impact
    if 'opp_strength_home_season' in features_df.columns:
        # Create bins for opponent strength
        features_df['opp_strength_bin'] = pd.qcut(
            features_df['opp_strength_home_season'], 
            q=5, 
            labels=['Very Weak', 'Weak', 'Average', 'Strong', 'Very Strong'],
            duplicates='drop'
        )
        opp_strength_analysis = features_df.groupby('opp_strength_bin')['home_win'].agg(['mean', 'count']).reset_index()
        opp_strength_analysis.columns = ['opp_strength', 'win_pct', 'games']
        
        axes[1, 0].bar(opp_strength_analysis['opp_strength'], opp_strength_analysis['win_pct'], 
                      alpha=0.7, color='purple')
        axes[1, 0].axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='50% baseline')
        axes[1, 0].set_xlabel('Opponent Strength Faced (Home Team)', fontsize=11)
        axes[1, 0].set_ylabel('Home Win Percentage', fontsize=11)
        axes[1, 0].set_title('Home Win Rate by Opponent Strength Faced', fontsize=12)
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(axis='y', alpha=0.3)
        axes[1, 0].legend()
    
    # 4. Form Impact (if available)
    form_cols = [col for col in features_df.columns if col.startswith('form_home_epa_off_')]
    if form_cols:
        # Use 3-game window if available
        form_col = 'form_home_epa_off_3' if 'form_home_epa_off_3' in form_cols else form_cols[0]
        if features_df[form_col].notna().sum() > 50:
            # Create bins for form
            features_df['form_bin'] = pd.qcut(
                features_df[form_col], 
                q=5, 
                labels=['Very Poor', 'Poor', 'Average', 'Good', 'Very Good'],
                duplicates='drop'
            )
            form_analysis = features_df.groupby('form_bin')['home_win'].agg(['mean', 'count']).reset_index()
            form_analysis.columns = ['form_level', 'win_pct', 'games']
            
            axes[1, 1].bar(form_analysis['form_level'], form_analysis['win_pct'], 
                          alpha=0.7, color='coral')
            axes[1, 1].axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='50% baseline')
            axes[1, 1].set_xlabel('Recent Offensive Form (Home Team)', fontsize=11)
            axes[1, 1].set_ylabel('Home Win Percentage', fontsize=11)
            axes[1, 1].set_title('Home Win Rate by Recent Offensive EPA', fontsize=12)
            axes[1, 1].tick_params(axis='x', rotation=45)
            axes[1, 1].grid(axis='y', alpha=0.3)
            axes[1, 1].legend()
        else:
            axes[1, 1].text(0.5, 0.5, 'Insufficient form data', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
            axes[1, 1].set_title('Form Impact (Data Not Available)', fontsize=12)
    else:
        axes[1, 1].text(0.5, 0.5, 'Form features not computed\n(Run PBP cell to enable)', 
                       ha='center', va='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('Form Impact (Not Available)', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    # Print summary statistics
    print("\n" + "=" * 60)
    print("SITUATIONAL ANALYSIS SUMMARY")
    print("=" * 60)
    
    if 'rest_home' in features_df.columns:
        avg_rest = features_df['rest_home'].mean()
        print(f"\nAverage rest days (home): {avg_rest:.1f}")
    
    if 'b2b_home' in features_df.columns:
        b2b_pct = features_df['b2b_home'].mean() * 100
        print(f"Back-to-back frequency (home): {b2b_pct:.1f}%")
else:
    print("Features DataFrame not available.")

# Output:
#   Back-to-Back Analysis:

#      is_b2b   win_pct  games

#   0   False  0.495238   1680

#   /var/folders/_7/dq14b35d6zx7ftncg3t_9tg40000gn/T/ipykernel_49270/2282090174.py:44: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.

#     opp_strength_analysis = features_df.groupby('opp_strength_bin')['home_win'].agg(['mean', 'count']).reset_index()

#   /var/folders/_7/dq14b35d6zx7ftncg3t_9tg40000gn/T/ipykernel_49270/2282090174.py:70: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.

#     form_analysis = features_df.groupby('form_bin')['home_win'].agg(['mean', 'count']).reset_index()

#   <Figure size 1600x1200 with 4 Axes>
#   

#   ============================================================

#   SITUATIONAL ANALYSIS SUMMARY

#   ============================================================

#   

#   Average rest days (home): 7.5

#   Back-to-back frequency (home): 0.0%


# Check for 2025 Season Data and Future Games
if 'schedule_df' in locals():
    print("Checking for 2025 season data...")
    
    # Check what seasons we have
    available_seasons = sorted(schedule_df.get('season', pd.to_datetime(schedule_df['game_date']).dt.year).unique())
    print(f"Available seasons: {available_seasons}")
    
    # Check for 2025 games
    if 2025 in available_seasons:
        games_2025 = schedule_df[schedule_df.get('season', pd.to_datetime(schedule_df['game_date']).dt.year) == 2025]
        print(f"\n2025 season games found: {len(games_2025)}")
        
        # Check which games have scores (completed) vs no scores (future)
        if 'home_score' in games_2025.columns and 'away_score' in games_2025.columns:
            completed_2025 = games_2025[games_2025['home_score'].notna() & games_2025['away_score'].notna()]
            future_2025 = games_2025[games_2025['home_score'].isna() | games_2025['away_score'].isna()]
            
            print(f"  Completed games: {len(completed_2025)}")
            print(f"  Future games (no scores): {len(future_2025)}")
            
            if len(future_2025) > 0:
                print(f"\nUpcoming games in 2025:")
                print(future_2025[['game_date', 'away_team', 'home_team', 'week']].head(10))
        else:
            print("  Score columns not found - cannot determine completed vs future games")
    else:
        print("\nNo 2025 season data found in schedule")
    
    # Check for any future games (games after today)
    today = pd.Timestamp.now()
    future_games = schedule_df[pd.to_datetime(schedule_df['game_date']) > today]
    if len(future_games) > 0:
        print(f"\nTotal future games (all seasons): {len(future_games)}")
        print(f"Date range: {future_games['game_date'].min()} to {future_games['game_date'].max()}")
    else:
        print("\nNo future games found")
else:
    print("Schedule DataFrame not available.")

# Output:
#   Checking for 2025 season data...

#   Available seasons: [np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]

#   

#   2025 season games found: 272

#     Completed games: 136

#     Future games (no scores): 136

#   

#   Upcoming games in 2025:

#         game_date away_team home_team  week

#   1544 2025-11-09       ATL       IND    10

#   1545 2025-11-09        NO       CAR    10

#   1546 2025-11-09       NYG       CHI    10

#   1547 2025-11-09       JAX       HOU    10

#   1548 2025-11-09       BUF       MIA    10

#   1549 2025-11-09       BAL       MIN    10

#   1550 2025-11-09       CLE       NYJ    10

#   1551 2025-11-09        NE        TB    10

#   1552 2025-11-09       ARI       SEA    10

#   1553 2025-11-09        LA        SF    10

#   

#   Total future games (all seasons): 136

#   Date range: 2025-11-09 00:00:00 to 2026-01-04 00:00:00


# Predict Win Probabilities for Future Games
if 'features_df' in locals() and ('rf' in locals() or 'lgbm' in locals()):
    print("=" * 70)
    print("PREDICTING WIN PROBABILITIES FOR FUTURE GAMES")
    print("=" * 70)
    
    # Get future games (no scores)
    today = pd.Timestamp.now()
    future_games = schedule_df[
        (pd.to_datetime(schedule_df['game_date']) > today) |
        (schedule_df.get('home_score', pd.Series([np.nan] * len(schedule_df))).isna())
    ].copy()
    
    if len(future_games) > 0:
        print(f"\nFound {len(future_games)} future games to predict")
        
        # Compute features for future games using historical data
        print("\nComputing features for future games...")
        
        # Start with rest features
        future_features = rest_schedule.add_rest_features(future_games, schedule_df)
        
        # Add opponent strength features
        future_features = strength.add_opponent_strength_features(future_features, schedule_df, league='nfl')
        
        # Add team strength features (reuse function from earlier)
        if 'compute_team_strength_features' in globals():
            future_features = compute_team_strength_features(future_features, schedule_df)
        else:
            # Define it inline if not available
            def compute_team_strength_features(games_df, historical_games):
                df = games_df.copy()
                df['home_team_win_pct'] = np.nan
                df['away_team_win_pct'] = np.nan
                df['home_team_point_diff'] = np.nan
                df['away_team_point_diff'] = np.nan
                
                for idx, row in df.iterrows():
                    game_date = pd.to_datetime(row['game_date'])
                    home_team = row['home_team']
                    away_team = row['away_team']
                    season = row.get('season', game_date.year)
                    
                    season_games = historical_games[
                        (pd.to_datetime(historical_games['game_date']) < game_date) &
                        (historical_games.get('season', pd.to_datetime(historical_games['game_date']).dt.year) == season)
                    ]
                    
                    # Home team stats
                    home_games = season_games[(season_games['home_team'] == home_team) | (season_games['away_team'] == home_team)]
                    if len(home_games) > 0:
                        home_wins = sum(1 for _, g in home_games.iterrows() 
                                      if (g['home_team'] == home_team and g.get('home_score', 0) > g.get('away_score', 0)) or
                                         (g['away_team'] == home_team and g.get('away_score', 0) > g.get('home_score', 0)))
                        home_point_diff = [g.get('home_score', 0) - g.get('away_score', 0) if g['home_team'] == home_team 
                                         else g.get('away_score', 0) - g.get('home_score', 0) for _, g in home_games.iterrows()]
                        df.loc[idx, 'home_team_win_pct'] = home_wins / len(home_games)
                        df.loc[idx, 'home_team_point_diff'] = np.mean(home_point_diff) if home_point_diff else 0
                    
                    # Away team stats
                    away_games = season_games[(season_games['home_team'] == away_team) | (season_games['away_team'] == away_team)]
                    if len(away_games) > 0:
                        away_wins = sum(1 for _, g in away_games.iterrows() 
                                      if (g['home_team'] == away_team and g.get('home_score', 0) > g.get('away_score', 0)) or
                                         (g['away_team'] == away_team and g.get('away_score', 0) > g.get('home_score', 0)))
                        away_point_diff = [g.get('home_score', 0) - g.get('away_score', 0) if g['home_team'] == away_team 
                                          else g.get('away_score', 0) - g.get('home_score', 0) for _, g in away_games.iterrows()]
                        df.loc[idx, 'away_team_win_pct'] = away_wins / len(away_games)
                        df.loc[idx, 'away_team_point_diff'] = np.mean(away_point_diff) if away_point_diff else 0
                return df
            
            future_features = compute_team_strength_features(future_features, schedule_df)
        
        # Add interaction features
        if 'rest_home' in future_features.columns:
            future_features['rest_differential'] = future_features['rest_home'] - future_features['rest_away']
            future_features['rest_advantage_home'] = (future_features['rest_home'] > future_features['rest_away']).astype(int)
        if 'home_team_win_pct' in future_features.columns:
            future_features['win_pct_differential'] = future_features['home_team_win_pct'] - future_features['away_team_win_pct']
        if 'home_team_point_diff' in future_features.columns:
            future_features['point_diff_differential'] = future_features['home_team_point_diff'] - future_features['away_team_point_diff']
        if 'opp_strength_home_season' in future_features.columns:
            future_features['opp_strength_differential'] = future_features['opp_strength_home_season'] - future_features['opp_strength_away_season']
        if 'game_date' in future_features.columns:
            future_features['week_number'] = pd.to_datetime(future_features['game_date']).dt.isocalendar().week
            future_features['month'] = pd.to_datetime(future_features['game_date']).dt.month
            future_features['is_playoff'] = future_features.get('game_type', '').str.contains('POST', case=False, na=False).astype(int)
        
        # CRITICAL FIX: Filter available_cols to only columns that exist in future_features
        # Form features won't exist for future games (no PBP data yet)
        available_cols_for_future = [col for col in available_cols if col in future_features.columns]
        
        print(f"\nAvailable features for future games: {len(available_cols_for_future)} out of {len(available_cols)}")
        if len(available_cols_for_future) < len(available_cols):
            missing = set(available_cols) - set(available_cols_for_future)
            print(f"Missing features (likely form/EPA features): {list(missing)[:10]}...")
        
        # Filter to games that have enough features (using only available columns)
        future_predictable = future_features[
            future_features[available_cols_for_future].notna().sum(axis=1) >= len(available_cols_for_future) * 0.5
        ]
        
        if len(future_predictable) > 0:
            print(f"\nGames with sufficient features for prediction: {len(future_predictable)}")
            
            # Prepare features (fill missing with median from training data if available)
            X_future = future_predictable[available_cols_for_future].copy()
            
            # Fill missing values with median from training set if available
            if 'X_train' in locals():
                for col in available_cols_for_future:
                    if X_future[col].isna().any():
                        if col in X_train.columns:
                            median_val = X_train[col].median()
                            X_future[col] = X_future[col].fillna(median_val)
                        else:
                            X_future[col] = X_future[col].fillna(0)
            else:
                X_future = X_future.fillna(0)
            
            # IMPORTANT: We need to ensure X_future has the same columns as X_train
            # Add missing columns (form features) with default values
            missing_cols = set(available_cols) - set(available_cols_for_future)
            for col in missing_cols:
                X_future[col] = 0  # Default value for missing form features
            
            # Reorder columns to match training data
            X_future = X_future[available_cols]
            
            # Predict probabilities
            if 'use_lgbm' in locals() and use_lgbm and 'lgbm' in locals():
                future_probs = lgbm.predict_proba(X_future)[:, 1]
                model_name = "LightGBM"
            else:
                future_probs = rf.predict_proba(X_future)[:, 1]
                model_name = "Random Forest"
                
            # Create prediction dataframe
            predictions_df = pd.DataFrame({
                'game_date': future_predictable['game_date'],
                'away_team': future_predictable['away_team'],
                'home_team': future_predictable['home_team'],
                'home_win_probability': future_probs,
                'away_win_probability': 1 - future_probs,
                'predicted_winner': future_predictable['home_team'].where(future_probs > 0.5, future_predictable['away_team']),
                'confidence': np.abs(future_probs - 0.5) * 2  # 0-1 scale
            }).sort_values('game_date')
            
            print(f"\nUsing {model_name} model for predictions")
            print("\n" + "=" * 70)
            print("PREDICTIONS FOR UPCOMING GAMES")
            print("=" * 70)
            print(f"\nShowing next 20 games:")
            display_cols = ['game_date', 'away_team', '@', 'home_team', 'home_win_probability', 'predicted_winner', 'confidence']
            predictions_display = predictions_df.head(20).copy()
            predictions_display['@'] = '@'
            print(predictions_display[['game_date', 'away_team', '@', 'home_team', 'home_win_probability', 'predicted_winner', 'confidence']].to_string(index=False))
            
            # Summary statistics
            print(f"\n\nPrediction Summary:")
            print(f"  Average home win probability: {predictions_df['home_win_probability'].mean():.3f}")
            print(f"  Games where home team favored: {(predictions_df['home_win_probability'] > 0.5).sum()}")
            print(f"  Games where away team favored: {(predictions_df['home_win_probability'] < 0.5).sum()}")
            print(f"  Average confidence: {predictions_df['confidence'].mean():.3f}")
            
            # Visualize predictions
            plt.figure(figsize=(14, 8))
            plt.subplot(2, 1, 1)
            predictions_df.head(30).plot(x='game_date', y='home_win_probability', 
                                        kind='bar', figsize=(14, 6), ax=plt.gca())
            plt.axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='50% (coin flip)')
            plt.ylabel('Home Win Probability', fontsize=12)
            plt.xlabel('Game Date', fontsize=12)
            plt.title('Predicted Home Win Probabilities (Next 30 Games)', fontsize=14)
            plt.legend()
            plt.xticks(rotation=45)
            plt.grid(axis='y', alpha=0.3)
            
            plt.subplot(2, 1, 2)
            predictions_df['confidence'].hist(bins=20, alpha=0.7, color='steelblue')
            plt.xlabel('Confidence (0 = coin flip, 1 = certain)', fontsize=12)
            plt.ylabel('Number of Games', fontsize=12)
            plt.title('Distribution of Prediction Confidence', fontsize=14)
            plt.grid(axis='y', alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
        else:
            print("\nNo future games have sufficient features for prediction.")
            print("This may be because:")
            print("  - Teams haven't played enough games yet this season")
            print("  - Some required features couldn't be computed")
    else:
        print("\nNo future games found to predict.")
else:
    print("Model or features not available. Run previous cells to train model first.")
# Output:
#   ======================================================================

#   PREDICTING WIN PROBABILITIES FOR FUTURE GAMES

#   ======================================================================

#   

#   Found 136 future games to predict

#   

#   Computing features for future games...

#   

#   Available features for future games: 18 out of 36

#   Missing features (likely form/EPA features): ['form_away_epa_def_3', 'form_epa_off_diff_5', 'form_away_epa_off_10', 'form_epa_def_diff_3', 'form_away_epa_def_10', 'form_epa_def_diff_5', 'form_epa_off_diff_10', 'form_home_epa_off_5', 'form_home_epa_off_10', 'form_away_epa_off_3']...

#   

#   Games with sufficient features for prediction: 136

#   

#   Using LightGBM model for predictions

#   

#   ======================================================================

#   PREDICTIONS FOR UPCOMING GAMES

#   ======================================================================

#   

#   Showing next 20 games:

#    game_date away_team @ home_team  home_win_probability predicted_winner  confidence

#   2025-11-09       ATL @       IND              0.523023              IND    0.046047

#   2025-11-09       PIT @       LAC              0.482562              PIT    0.034876

#   2025-11-09       DET @       WAS              0.185306              DET    0.629389

#   2025-11-09       ARI @       SEA              0.554094              SEA    0.108188

#   2025-11-09        NE @        TB              0.587798               TB    0.175596

#   2025-11-09       CLE @       NYJ              0.483547              CLE    0.032906

#   2025-11-09        LA @        SF              0.218163               LA    0.563674

#   2025-11-09       BUF @       MIA              0.197841              BUF    0.604319

#   2025-11-09       JAX @       HOU              0.289468              JAX    0.421064

#   2025-11-09       NYG @       CHI              0.310597              NYG    0.378805

#   2025-11-09        NO @       CAR              0.527629              CAR    0.055258

#   2025-11-09       BAL @       MIN              0.442883              BAL    0.114234

#   2025-11-10       PHI @        GB              0.525130               GB    0.050260

#   2025-11-13       NYJ @        NE              0.403250              NYJ    0.193499

#   2025-11-16        SF @       ARI              0.539439              ARI    0.078877

#   2025-11-16       DET @       PHI              0.670740              PHI    0.341480

#   2025-11-16        KC @       DEN              0.348807               KC    0.302385

#   2025-11-16       BAL @       CLE              0.577353              CLE    0.154706

#   2025-11-16       SEA @        LA              0.643674               LA    0.287347

#   2025-11-16       HOU @       TEN              0.494847              HOU    0.010306

#   

#   

#   Prediction Summary:

#     Average home win probability: 0.491

#     Games where home team favored: 72

#     Games where away team favored: 64

#     Average confidence: 0.170

#   <Figure size 1400x600 with 2 Axes>

# Export Models for Production Use
# This cell trains and saves models in the format expected by the production pipeline

import sys
import os
sys.path.insert(0, os.path.join(os.getcwd(), '..'))

from src.pipeline.train_models import train_and_save_models

if 'features_df' in locals() and 'home_win' in features_df.columns and 'margin' in features_df.columns:
    print("=" * 70)
    print("TRAINING AND EXPORTING MODELS FOR PRODUCTION")
    print("=" * 70)
    
    # Use the feature columns from the trained model
    if 'available_cols' in locals():
        feature_cols_to_use = available_cols
    else:
        # Fallback: collect feature columns
        feature_cols_to_use = [
            'rest_home', 'rest_away', 'b2b_home', 'b2b_away',
            'opp_strength_home_season', 'opp_strength_away_season',
            'home_team_win_pct', 'away_team_win_pct',
            'home_team_point_diff', 'away_team_point_diff',
            'rest_differential', 'rest_advantage_home',
            'win_pct_differential', 'point_diff_differential',
            'opp_strength_differential', 'week_number', 'month', 'is_playoff'
        ]
        form_cols = [col for col in features_df.columns if col.startswith('form_')]
        feature_cols_to_use.extend(form_cols)
        interaction_cols = [col for col in features_df.columns if 'diff' in col.lower() or 'differential' in col.lower()]
        feature_cols_to_use.extend(interaction_cols)
        # Remove duplicates
        seen = set()
        feature_cols_to_use = [col for col in feature_cols_to_use if not (col in seen or seen.add(col))]
        # Filter to columns that exist
        feature_cols_to_use = [col for col in feature_cols_to_use if col in features_df.columns]
    
    print(f"\nUsing {len(feature_cols_to_use)} features for model training")
    
    # Train and save models
    try:
        results = train_and_save_models(
            features_df,
            target_col='home_win',
            margin_col='margin',
            feature_cols=feature_cols_to_use,
            league='NFL',
            model_version='v1',
            use_lgbm=True,
            test_size=0.2,
            random_state=42
        )
        
        print("\n" + "=" * 70)
        print("MODEL EXPORT COMPLETE")
        print("=" * 70)
        print(f"\nModels saved:")
        print(f"  - Win Probability: {results['win_prob_model_path']}")
        print(f"  - Spread: {results['spread_model_path']}")
        print(f"  - Link Function: {results['link_function_path']}")
        print(f"  - Feature Medians: {results['feature_medians_path']}")
        
        print(f"\nModel Performance:")
        print(f"  Win Probability Accuracy: {results['accuracy']:.4f}")
        print(f"  Spread MAE: {results['mae']:.2f} points")
        print(f"  Spread RMSE: {results['rmse']:.2f} points")
        print(f"  Link Function: a={results['link_params'][0]:.4f}, b={results['link_params'][1]:.4f}")
        
        print(f"\nYou can now use these models in production with:")
        print(f"  python -m src.pipeline.refresh --league NFL --date YYYY-MM-DD --model-version v1")
        
    except Exception as e:
        print(f"\nError training models: {e}")
        import traceback
        traceback.print_exc()
else:
    print("Features DataFrame with home_win and margin columns not available.")
    print("Run previous cells to prepare data.")

# Output:
#   ======================================================================

#   TRAINING AND EXPORTING MODELS FOR PRODUCTION

#   ======================================================================

#   

#   Using 36 features for model training

#   Training on 1235 samples, testing on 309 samples

#   Features: 36

#   

#   Training win probability model...

#   Win Probability Model (LIGHTGBM_CALIBRATED):

#     Accuracy: 0.5873

#     Brier Score: 0.2359 (lower is better)

#   

#   Training spread model...

#   Spread Model (LIGHTGBM_CALIBRATED):

#     MAE: 10.57 points

#     RMSE: 13.44 points

#   

#   Calibrating link function...

#   Link function parameters: a=0.3000, b=0.3095

#   

#   Model Consistency Check:

#     Mean disagreement: 0.2069

#     Sign agreement: 80.6%

#     ⚠️  Warning: High disagreement between models. Consider ensemble approach.

#   

#   Saved win probability model to /Users/drewboynton/Documents/VSCODE_PROJECTS/sports-edge/notebooks/../models/win_prob_model_nfl_v1.pkl

#   Saved spread model to /Users/drewboynton/Documents/VSCODE_PROJECTS/sports-edge/notebooks/../models/spread_model_nfl_v1.pkl

#   Saved link function to /Users/drewboynton/Documents/VSCODE_PROJECTS/sports-edge/notebooks/../models/link_function_nfl_v1.pkl

#   Saved feature medians to /Users/drewboynton/Documents/VSCODE_PROJECTS/sports-edge/notebooks/../models/feature_medians_nfl_v1.pkl

#   

#   ======================================================================

#   MODEL EXPORT COMPLETE

#   ======================================================================

#   

#   Models saved:

#     - Win Probability: /Users/drewboynton/Documents/VSCODE_PROJECTS/sports-edge/notebooks/../models/win_prob_model_nfl_v1.pkl

#     - Spread: /Users/drewboynton/Documents/VSCODE_PROJECTS/sports-edge/notebooks/../models/spread_model_nfl_v1.pkl

#     - Link Function: /Users/drewboynton/Documents/VSCODE_PROJECTS/sports-edge/notebooks/../models/link_function_nfl_v1.pkl

#     - Feature Medians: /Users/drewboynton/Documents/VSCODE_PROJECTS/sports-edge/notebooks/../models/feature_medians_nfl_v1.pkl

#   

#   Model Performance:

#     Win Probability Accuracy: 0.5873

#     Spread MAE: 10.57 points

#     Spread RMSE: 13.44 points

#     Link Function: a=0.3000, b=0.3095

#   

#   You can now use these models in production with:

#     python -m src.pipeline.refresh --league NFL --date YYYY-MM-DD --model-version v1


# Test Production Prediction Function
# Test the production predictor with a sample game

if 'features_df' in locals() and 'schedule_df' in locals():
    try:
        from src.models.predictor import GamePredictor, predict_single_game
        
        # Test with a recent game
        test_game = schedule_df.iloc[-10]  # Use a recent game
        
        print("Testing production predictor...")
        print(f"Game: {test_game['away_team']} @ {test_game['home_team']}")
        print(f"Date: {test_game['game_date']}")
        
        # Initialize predictor
        predictor = GamePredictor('NFL', 'v1')
        
        # Predict
        game_row = pd.DataFrame([test_game])
        prediction = predictor.predict(
            game_row,
            schedule_df,
            pbp_df if 'pbp_df' in locals() else None
        )
        
        print("\n" + "=" * 70)
        print("PRODUCTION PREDICTION TEST")
        print("=" * 70)
        print(f"\n{prediction['away_team']} @ {prediction['home_team']}")
        print(f"Predicted Spread: {prediction['predicted_spread']:.2f}")
        print(f"Home Win Probability: {prediction['home_win_probability']:.1%}")
        print(f"Away Win Probability: {prediction['away_win_probability']:.1%}")
        print(f"Predicted Winner: {prediction['predicted_winner']}")
        print(f"Confidence: {prediction['confidence']:.1%}")
        print(f"Interpretation: {prediction['spread_interpretation']}")
        
        # Compare to actual if available
        if 'home_score' in test_game and 'away_score' in test_game and pd.notna(test_game['home_score']):
            actual_margin = test_game['home_score'] - test_game['away_score']
            actual_winner = test_game['home_team'] if actual_margin > 0 else test_game['away_team']
            print(f"\nActual Result:")
            print(f"  Margin: {actual_margin:.1f}")
            print(f"  Winner: {actual_winner}")
            print(f"  Prediction Error: {abs(prediction['predicted_spread'] - actual_margin):.2f} points")
            print(f"  Correct Winner Prediction: {prediction['predicted_winner'] == actual_winner}")
        
        print("\n✓ Production predictor is working correctly!")
        
    except FileNotFoundError as e:
        print(f"\nModels not found: {e}")
        print("Run the model export cell first to train and save models.")
    except Exception as e:
        print(f"\nError testing predictor: {e}")
        import traceback
        traceback.print_exc()
else:
    print("Required data not available. Run previous cells first.")

# Output:
#   Testing production predictor...

#   Game: TEN @ JAX

#   Date: 2026-01-04 00:00:00

#   

#   ======================================================================

#   PRODUCTION PREDICTION TEST

#   ======================================================================

#   

#   TEN @ JAX

#   Predicted Spread: 4.61

#   Home Win Probability: 70.5%

#   Away Win Probability: 29.5%

#   Predicted Winner: JAX

#   Confidence: 41.1%

#   Interpretation: JAX by 4.6

#   

#   ✓ Production predictor is working correctly!


# Model Diagnostic and Calibration Analysis
# Run this after training models to diagnose issues and verify calibration

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss, log_loss
from sklearn.model_selection import train_test_split
import pickle
import os
import sys

sys.path.insert(0, os.path.join(os.getcwd(), '..'))
from src.models.link_function import spread_to_win_prob

def diagnose_models(features_df, model_version='v1', league='NFL'):
    """
    Comprehensive diagnostic analysis of win probability and spread models.
    """
    print("=" * 80)
    print("MODEL DIAGNOSTIC ANALYSIS")
    print("=" * 80)
    
    # Load models
    models_dir = 'models'
    win_prob_path = os.path.join(models_dir, f'win_prob_model_{league.lower()}_{model_version}.pkl')
    spread_path = os.path.join(models_dir, f'spread_model_{league.lower()}_{model_version}.pkl')
    link_path = os.path.join(models_dir, f'link_function_{league.lower()}_{model_version}.pkl')
    
    if not all(os.path.exists(p) for p in [win_prob_path, spread_path, link_path]):
        print("Models not found. Run the model export cell first.")
        return
    
    with open(win_prob_path, 'rb') as f:
        win_prob_data = pickle.load(f)
    with open(spread_path, 'rb') as f:
        spread_data = pickle.load(f)
    with open(link_path, 'rb') as f:
        link_data = pickle.load(f)
    
    win_prob_model = win_prob_data['model']
    spread_model = spread_data['model']
    feature_names = win_prob_data['feature_names']
    link_a = link_data['a']
    link_b = link_data['b']
    
    # Prepare test data
    exclude_cols = ['home_win', 'margin', 'game_id', 'game_date', 'gameday',
                   'home_team', 'away_team', 'home_score', 'away_score',
                   'league', 'season', 'game_type']
    available_features = [col for col in feature_names if col in features_df.columns]
    
    model_df = features_df[available_features + ['home_win', 'margin']].dropna()
    
    if len(model_df) < 100:
        print("Insufficient data for analysis")
        return
    
    X = model_df[available_features]
    y_win = model_df['home_win']
    y_margin = model_df['margin']
    
    # Split data (use same split as training)
    X_train, X_test, y_train_win, y_test_win, y_train_margin, y_test_margin = train_test_split(
        X, y_win, y_margin, test_size=0.2, random_state=42, stratify=y_win
    )
    
    # 1. PREDICTION COMPARISON
    print("\n1. PREDICTION AGREEMENT ANALYSIS")
    print("-" * 80)
    
    win_prob_pred = win_prob_model.predict_proba(X_test)[:, 1]
    spread_pred = spread_model.predict(X_test)
    win_prob_from_spread = spread_to_win_prob(spread_pred, link_a, link_b)
    
    # Calculate disagreement
    disagreement = np.abs(win_prob_pred - win_prob_from_spread)
    mean_disagreement = np.mean(disagreement)
    max_disagreement = np.max(disagreement)
    
    print(f"Mean absolute disagreement: {mean_disagreement:.4f}")
    print(f"Max disagreement: {max_disagreement:.4f}")
    print(f"Games with disagreement > 0.1: {(disagreement > 0.1).sum()} ({(disagreement > 0.1).mean()*100:.1f}%)")
    print(f"Games with disagreement > 0.2: {(disagreement > 0.2).sum()} ({(disagreement > 0.2).mean()*100:.1f}%)")
    
    # Check sign consistency
    win_prob_favorite = (win_prob_pred > 0.5).astype(int)
    spread_favorite = (spread_pred > 0).astype(int)
    sign_agreement = (win_prob_favorite == spread_favorite).mean()
    print(f"\nSign agreement (both predict same favorite): {sign_agreement:.1%}")
    
    # 2. CALIBRATION ANALYSIS
    print("\n2. WIN PROBABILITY MODEL CALIBRATION")
    print("-" * 80)
    
    brier = brier_score_loss(y_test_win, win_prob_pred)
    logloss = log_loss(y_test_win, win_prob_pred)
    
    print(f"Brier Score: {brier:.4f} (lower is better, perfect=0)")
    print(f"Log Loss: {logloss:.4f} (lower is better)")
    
    # Calibration curve
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_test_win, win_prob_pred, n_bins=10
    )
    
    fraction_from_spread, mean_from_spread = calibration_curve(
        y_test_win, win_prob_from_spread, n_bins=10
    )
    
    # Plot calibration
    plt.figure(figsize=(14, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="Win Prob Model", linewidth=2)
    plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    plt.xlabel("Mean Predicted Probability")
    plt.ylabel("Fraction of Positives")
    plt.title("Calibration Curve - Win Probability Model")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="Win Prob Model", alpha=0.7, linewidth=2)
    plt.plot(mean_from_spread, fraction_from_spread, "o-", label="From Spread Model", alpha=0.7, linewidth=2)
    plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    plt.xlabel("Mean Predicted Probability")
    plt.ylabel("Fraction of Positives")
    plt.title("Calibration Comparison")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 3. SPREAD MODEL ACCURACY
    print("\n3. SPREAD MODEL PERFORMANCE")
    print("-" * 80)
    
    from sklearn.metrics import mean_absolute_error, mean_squared_error
    
    mae = mean_absolute_error(y_test_margin, spread_pred)
    rmse = np.sqrt(mean_squared_error(y_test_margin, spread_pred))
    spread_bias = np.mean(spread_pred - y_test_margin)
    
    print(f"MAE: {mae:.2f} points")
    print(f"RMSE: {rmse:.2f} points")
    print(f"Bias (mean error): {spread_bias:.2f} points (should be ~0)")
    
    # 4. LINK FUNCTION VERIFICATION
    print("\n4. LINK FUNCTION VERIFICATION")
    print("-" * 80)
    
    print(f"Link function parameters: a={link_a:.4f}, b={link_b:.4f}")
    
    # Check if link function matches actual data
    spread_bins = np.linspace(spread_pred.min(), spread_pred.max(), 10)
    bin_centers = (spread_bins[:-1] + spread_bins[1:]) / 2
    
    actual_win_rates = []
    predicted_probs = []
    
    for i in range(len(spread_bins) - 1):
        mask = (spread_pred >= spread_bins[i]) & (spread_pred < spread_bins[i+1])
        if mask.sum() > 0:
            actual_win_rates.append(y_test_win[mask].mean())
            predicted_probs.append(win_prob_from_spread[mask].mean())
    
    plt.figure(figsize=(10, 6))
    plt.plot(bin_centers[:len(actual_win_rates)], actual_win_rates, 'o-', label='Actual Win Rate', linewidth=2, markersize=8)
    plt.plot(bin_centers[:len(predicted_probs)], predicted_probs, 's-', label='Link Function Prediction', linewidth=2, markersize=8)
    plt.xlabel('Predicted Spread')
    plt.ylabel('Win Probability')
    plt.title('Link Function Calibration Check')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # 5. DISAGREEMENT ANALYSIS
    print("\n5. DISAGREEMENT ANALYSIS")
    print("-" * 80)
    
    # Show examples of high disagreement
    high_disagreement_idx = np.where(disagreement > 0.15)[0]
    if len(high_disagreement_idx) > 0:
        print(f"\nExamples of high disagreement (>0.15):")
        print(f"{'Spread':>8} {'Win Prob':>10} {'From Spread':>12} {'Disagree':>10} {'Actual':>8}")
        print("-" * 60)
        for idx in high_disagreement_idx[:10]:
            actual_win = "Home" if y_test_win.iloc[idx] == 1 else "Away"
            print(f"{spread_pred[idx]:>8.2f} {win_prob_pred[idx]:>10.2%} {win_prob_from_spread[idx]:>12.2%} "
                  f"{disagreement[idx]:>10.2%} {actual_win:>8}")
    
    # 6. RECOMMENDATIONS
    print("\n6. RECOMMENDATIONS")
    print("-" * 80)
    
    recommendations = []
    
    if mean_disagreement > 0.15:
        recommendations.append("⚠️  High disagreement between models detected.")
        recommendations.append("   ✓ Using ensemble approach (favoring spread when disagreement > 0.15)")
        recommendations.append("   Consider: Retraining with more data or feature engineering")
    elif mean_disagreement > 0.10:
        recommendations.append("⚠️  Moderate disagreement. Models are using ensemble approach.")
    else:
        recommendations.append("✓ Models are well-aligned!")
    
    if sign_agreement < 0.90:
        recommendations.append("⚠️  Models disagree on favorite frequently. Check feature alignment.")
    
    if brier > 0.25:
        recommendations.append("⚠️  Win probability model could be better calibrated.")
        recommendations.append("   ✓ Calibration already applied in training")
    
    if abs(spread_bias) > 1.0:
        recommendations.append("⚠️  Spread model has bias. Check for data issues.")
    
    if not recommendations:
        recommendations.append("✓ All checks passed! Models are well-calibrated and consistent.")
    
    for rec in recommendations:
        print(rec)
    
    return {
        'mean_disagreement': mean_disagreement,
        'sign_agreement': sign_agreement,
        'brier_score': brier,
        'spread_mae': mae,
        'spread_bias': spread_bias
    }

# Run diagnostics
if 'features_df' in locals() and 'home_win' in features_df.columns:
    results = diagnose_models(features_df, model_version='v1', league='NFL')
else:
    print("Run previous cells to prepare features_df first")

# Output:
#   ================================================================================

#   MODEL DIAGNOSTIC ANALYSIS

#   ================================================================================

#   Models not found. Run the model export cell first.


"""
# Summary & Key Insights

## Top Features Affecting Winning Percentage

Based on the analysis above, here are the key findings:

"""

# Generate Summary Report
if 'features_df' in locals() and 'home_win' in features_df.columns:
    print("=" * 70)
    print("NFL FEATURE ENGINEERING ANALYSIS - SUMMARY REPORT")
    print("=" * 70)
    
    # Get feature importance if model was trained
    if 'importance_df' in locals():
        print("\nTOP 5 MOST IMPORTANT FEATURES (Random Forest):")
        print("-" * 70)
        for idx, row in importance_df.head(5).iterrows():
            print(f"{idx + 1}. {row['feature']:40s} Importance: {row['importance']:.4f}")
    
    # Get correlations if computed
    if 'corr_data' in locals():
        print("\nTOP 5 FEATURES BY CORRELATION WITH HOME WIN:")
        print("-" * 70)
        for idx, (feature, corr) in enumerate(corr_data.head(5).items(), 1):
            direction = "increases" if corr > 0 else "decreases"
            print(f"{idx}. {feature:40s} {direction} home win prob (r={corr:.4f})")
    
    # Key insights
    print("\n" + "=" * 70)
    print("KEY INSIGHTS:")
    print("=" * 70)
    
    if 'rest_home' in features_df.columns:
        rest_corr = features_df[['rest_home', 'home_win']].corr().iloc[0, 1]
        print(f"\n1. REST DAYS:")
        print(f"   - Correlation with home win: {rest_corr:.4f}")
        if rest_corr > 0.05:
            print("   - More rest days tend to increase home win probability")
        elif rest_corr < -0.05:
            print("   - More rest days tend to decrease home win probability")
        else:
            print("   - Rest days show minimal correlation with outcomes")
    
    if 'b2b_home' in features_df.columns:
        b2b_win_rate = features_df[features_df['b2b_home'] == True]['home_win'].mean()
        normal_win_rate = features_df[features_df['b2b_home'] == False]['home_win'].mean()
        print(f"\n2. BACK-TO-BACK GAMES:")
        print(f"   - Home win rate with B2B: {b2b_win_rate:.3f}")
        print(f"   - Home win rate without B2B: {normal_win_rate:.3f}")
        print(f"   - Difference: {(b2b_win_rate - normal_win_rate):.3f}")
    
    if 'opp_strength_home_season' in features_df.columns:
        opp_corr = features_df[['opp_strength_home_season', 'home_win']].corr().iloc[0, 1]
        print(f"\n3. OPPONENT STRENGTH:")
        print(f"   - Correlation with home win: {opp_corr:.4f}")
        print("   - Measures average point differential of opponents faced")
    
    form_cols = [col for col in features_df.columns if col.startswith('form_home_epa_off_')]
    if form_cols:
        form_col = form_cols[0]
        form_corr = features_df[[form_col, 'home_win']].corr().iloc[0, 1]
        print(f"\n4. RECENT FORM (EPA):")
        print(f"   - Correlation with home win: {form_corr:.4f}")
        print("   - Measures recent offensive performance")
    
    print("\n" + "=" * 70)
    print("RECOMMENDATIONS:")
    print("=" * 70)
    print("1. Focus on features with highest importance/correlation for model building")
    print("2. Consider interaction effects (e.g., rest + form)")
    print("3. Validate findings with out-of-sample testing")
    print("4. Monitor feature stability across different seasons")
    print("=" * 70)
else:
    print("Run previous cells to generate summary.")

# Output:
#   ======================================================================

#   NFL FEATURE ENGINEERING ANALYSIS - SUMMARY REPORT

#   ======================================================================

#   

#   TOP 5 MOST IMPORTANT FEATURES (Random Forest):

#   ----------------------------------------------------------------------

#   35. form_epa_off_diff_10                     Importance: 0.0715

#   33. form_epa_off_diff_5                      Importance: 0.0522

#   9. home_team_point_diff                     Importance: 0.0417

#   13. win_pct_differential                     Importance: 0.0414

#   19. form_home_epa_off_3                      Importance: 0.0412

#   

#   TOP 5 FEATURES BY CORRELATION WITH HOME WIN:

#   ----------------------------------------------------------------------

#   1. form_epa_off_diff_10                     increases home win prob (r=0.2609)

#   2. form_epa_off_diff_10                     increases home win prob (r=0.2609)

#   3. point_diff_differential                  increases home win prob (r=0.2298)

#   4. point_diff_differential                  increases home win prob (r=0.2298)

#   5. form_epa_off_diff_5                      increases home win prob (r=0.2249)

#   

#   ======================================================================

#   KEY INSIGHTS:

#   ======================================================================

#   

#   1. REST DAYS:

#      - Correlation with home win: 0.0343

#      - Rest days show minimal correlation with outcomes

#   

#   2. BACK-TO-BACK GAMES:

#      - Home win rate with B2B: nan

#      - Home win rate without B2B: 0.495

#      - Difference: nan

#   

#   3. OPPONENT STRENGTH:

#      - Correlation with home win: -0.0656

#      - Measures average point differential of opponents faced

#   

#   4. RECENT FORM (EPA):

#      - Correlation with home win: 0.1563

#      - Measures recent offensive performance

#   

#   ======================================================================

#   RECOMMENDATIONS:

#   ======================================================================

#   1. Focus on features with highest importance/correlation for model building

#   2. Consider interaction effects (e.g., rest + form)

#   3. Validate findings with out-of-sample testing

#   4. Monitor feature stability across different seasons

#   ======================================================================




================================================
FILE: notebooks/week10_diagnostics.ipynb
================================================
Error processing notebook: 'text/plain'


================================================
FILE: sql/001_initial_schema.sql
================================================
-- Sports-Edge: Initial Schema Migration
-- Creates tables, views, and RLS policies for sports betting analysis

-- Games table
create table if not exists games (
  id uuid primary key default gen_random_uuid(),
  league text check (league in ('NFL','NBA')) not null,
  season int not null,
  game_time_utc timestamptz not null,
  home_team text not null,
  away_team text not null,
  created_at timestamptz default now()
);

create index if not exists idx_games_league_season on games(league, season);
create index if not exists idx_games_date on games(game_time_utc);

-- Odds snapshots table
create table if not exists odds_snapshots (
  id uuid primary key default gen_random_uuid(),
  game_id uuid references games(id) on delete cascade,
  book text not null,
  market text check (market in ('spread','moneyline','total')) not null,
  line numeric,
  price numeric,
  snapshot_ts timestamptz not null default now(),
  created_at timestamptz default now()
);

create index if not exists idx_odds_game_id on odds_snapshots(game_id);
create index if not exists idx_odds_snapshot_ts on odds_snapshots(snapshot_ts);

-- Model predictions table
create table if not exists model_predictions (
  id uuid primary key default gen_random_uuid(),
  game_id uuid references games(id) on delete cascade,
  model_name text not null,
  model_version text not null,
  my_spread numeric,
  my_home_win_prob numeric,
  asof_ts timestamptz not null default now(),
  created_at timestamptz default now()
);

create index if not exists idx_preds_game_id on model_predictions(game_id);
create index if not exists idx_preds_asof_ts on model_predictions(asof_ts);
create index if not exists idx_preds_model_version on model_predictions(model_version);

-- Features table
create table if not exists features (
  id uuid primary key default gen_random_uuid(),
  game_id uuid references games(id) on delete cascade,
  feature_json jsonb not null,
  asof_ts timestamptz not null default now(),
  created_at timestamptz default now()
);

create index if not exists idx_features_game_id on features(game_id);
create index if not exists idx_features_asof_ts on features(asof_ts);

-- Model runs audit table
create table if not exists model_runs (
  id uuid primary key default gen_random_uuid(),
  league text,
  started_at timestamptz default now(),
  finished_at timestamptz,
  rows_written int,
  success boolean,
  error_text text,
  created_at timestamptz default now()
);

create index if not exists idx_runs_league on model_runs(league);
create index if not exists idx_runs_started_at on model_runs(started_at);

-- View: Latest odds + predictions per game for today
create or replace view games_today_enriched as
select
  g.id as game_id,
  g.league, 
  g.season, 
  g.game_time_utc,
  g.home_team, 
  g.away_team,
  o.line as book_spread,
  p.my_spread,
  (p.my_spread - o.line) as edge_pts,
  p.my_home_win_prob,
  p.model_version,
  p.asof_ts as prediction_ts,
  o.snapshot_ts as odds_ts
from games g
left join lateral (
  select line, snapshot_ts
  from odds_snapshots o
  where o.game_id = g.id and o.market = 'spread'
  order by snapshot_ts desc limit 1
) o on true
left join lateral (
  select my_spread, my_home_win_prob, model_version, asof_ts
  from model_predictions p
  where p.game_id = g.id
  order by asof_ts desc limit 1
) p on true
where g.game_time_utc::date = (now() at time zone 'America/Denver')::date
order by g.game_time_utc;

-- RLS Policies (public read-only)
alter table games enable row level security;
alter table odds_snapshots enable row level security;
alter table model_predictions enable row level security;
alter table features enable row level security;
alter table model_runs enable row level security;

-- Drop existing policies if they exist
drop policy if exists "public read games" on games;
drop policy if exists "public read odds" on odds_snapshots;
drop policy if exists "public read preds" on model_predictions;
drop policy if exists "public read features" on features;
drop policy if exists "public read runs" on model_runs;

-- Create read-only policies
create policy "public read games" on games for select using (true);
create policy "public read odds" on odds_snapshots for select using (true);
create policy "public read preds" on model_predictions for select using (true);
create policy "public read features" on features for select using (true);
create policy "public read runs" on model_runs for select using (true);




================================================
FILE: src/__init__.py
================================================
# Sports-Edge Package




================================================
FILE: src/data/__init__.py
================================================
# Data fetching modules




================================================
FILE: src/data/nba_fetcher.py
================================================
"""
NBA data fetcher using nba_api.
Fetches schedule, team game logs, and advanced statistics.
"""

import pandas as pd
from datetime import datetime
from typing import Optional, List, Dict
import os
from nba_api.live.nba.endpoints import scoreboard
from nba_api.stats.endpoints import teamgamelog, teamdashboardbygeneralsplits
from nba_api.stats.static import teams


def get_team_id(team_name: str) -> Optional[int]:
    """
    Convert team name/abbreviation to NBA team ID.
    
    Args:
        team_name: Team name or abbreviation (e.g., 'Lakers', 'LAL')
    
    Returns:
        Team ID or None if not found
    """
    nba_teams = teams.get_teams()
    
    # Try exact match first
    for team in nba_teams:
        if team_name.upper() in [team['abbreviation'], team['nickname'].upper(), team['full_name'].upper()]:
            return team['id']
    
    return None


def fetch_nba_schedule(season: int) -> pd.DataFrame:
    """
    Fetch NBA schedule for a given season.
    
    Args:
        season: NBA season year (e.g., 2024 for 2024-25 season)
    
    Returns:
        DataFrame with schedule information
    """
    # NBA API uses season format like '2024-25'
    season_str = f"{season}-{str(season + 1)[-2:]}"
    
    # Fetch scoreboard for current date (we'll need to iterate through dates)
    # For now, return empty - will be implemented with date-specific fetching
    return pd.DataFrame()


def fetch_nba_games_for_date(date: str) -> pd.DataFrame:
    """
    Fetch NBA games scheduled for a specific date.
    
    Args:
        date: Date string in YYYY-MM-DD format
    
    Returns:
        DataFrame with games for that date
    """
    try:
        scoreboard_data = scoreboard.ScoreBoard()
        games = scoreboard_data.get_data_frames()[0]
        
        # Filter by date
        games['game_date'] = pd.to_datetime(games['GAME_DATE_EST'])
        date_obj = datetime.strptime(date, '%Y-%m-%d')
        games = games[games['game_date'].dt.date == date_obj.date()]
        
        return games
    except Exception as e:
        print(f"Error fetching NBA games: {e}")
        return pd.DataFrame()


def fetch_nba_team_stats(team_id: int, season: str) -> pd.DataFrame:
    """
    Fetch NBA team statistics for a given team and season.
    
    Args:
        team_id: NBA team ID
        season: Season string (e.g., '2024-25')
    
    Returns:
        DataFrame with team statistics
    """
    try:
        team_log = teamgamelog.TeamGameLog(team_id=team_id, season=season)
        df = team_log.get_data_frames()[0]
        return df
    except Exception as e:
        print(f"Error fetching team stats for team {team_id}: {e}")
        return pd.DataFrame()


def fetch_nba_advanced_stats(team_id: int, season: str) -> pd.DataFrame:
    """
    Fetch advanced NBA team statistics (net rating, pace, etc.).
    
    Args:
        team_id: NBA team ID
        season: Season string (e.g., '2024-25')
    
    Returns:
        DataFrame with advanced statistics
    """
    try:
        dashboard = teamdashboardbygeneralsplits.TeamDashboardByGeneralSplits(
            team_id=team_id, season=season
        )
        df = dashboard.get_data_frames()[0]
        return df
    except Exception as e:
        print(f"Error fetching advanced stats for team {team_id}: {e}")
        return pd.DataFrame()


def cache_nba_data(data: pd.DataFrame, league: str, date: str, data_type: str):
    """
    Cache NBA data to disk for reproducibility.
    
    Args:
        data: DataFrame to cache
        league: 'nba'
        date: Date string YYYY-MM-DD
        data_type: 'schedule', 'stats', etc.
    """
    cache_dir = f"data/raw/{league}/{date}"
    os.makedirs(cache_dir, exist_ok=True)
    
    filepath = f"{cache_dir}/{data_type}.parquet"
    data.to_parquet(filepath, index=False)
    print(f"Cached {data_type} to {filepath}")




================================================
FILE: src/data/nfl_fetcher.py
================================================
"""
NFL data fetcher built on nfl_data_py.
Fetches schedule, play-by-play data, and team statistics.
"""

import pandas as pd
from datetime import datetime
from typing import Optional, List, Dict
import os

import nfl_data_py as nfl


def fetch_nfl_schedule(season: int, week: Optional[int] = None) -> pd.DataFrame:
    """
    Fetch NFL schedule for a given season and optionally a specific week.
    
    Args:
        season: NFL season year (e.g., 2024)
        week: Optional week number (1-18 for regular season)
    
    Returns:
        DataFrame with columns: game_id, season, week, game_date, home_team, away_team, etc.
    """
    schedule = nfl.import_schedules([season])
    
    if week is not None:
        schedule = schedule[schedule['week'] == week]
    
    return schedule


def fetch_nfl_team_stats(season: int, week: Optional[int] = None) -> pd.DataFrame:
    """
    Fetch NFL team-level statistics (EPA, success rate, etc.).
    
    Args:
        season: NFL season year
        week: Optional week number
    
    Returns:
        DataFrame with team statistics
    """
    pbp = nfl.import_pbp_data([season])
    if week is not None:
        pbp = pbp[pbp['week'] == week]
    
    team_stats = pbp.groupby(['posteam', 'week']).agg({
        'epa': 'mean',
        'success': 'mean',
        'yards_gained': 'sum'
    }).reset_index()
    
    return team_stats


def fetch_nfl_games_for_date(date: str) -> pd.DataFrame:
    """
    Fetch NFL games scheduled for a specific date.
    
    Args:
        date: Date string in YYYY-MM-DD format
    
    Returns:
        DataFrame with games for that date
    """
    date_obj = datetime.strptime(date, '%Y-%m-%d')
    season = date_obj.year if date_obj.month >= 9 else date_obj.year - 1
    
    schedule = fetch_nfl_schedule(season)
    schedule['game_date'] = pd.to_datetime(schedule['gameday'])
    
    games = schedule[schedule['game_date'].dt.date == date_obj.date()]
    return games


def cache_nfl_data(data: pd.DataFrame, league: str, date: str, data_type: str):
    """
    Cache NFL data to disk for reproducibility.
    
    Args:
        data: DataFrame to cache
        league: 'nfl'
        date: Date string YYYY-MM-DD
        data_type: 'schedule', 'stats', etc.
    """
    cache_dir = f"data/raw/{league}/{date}"
    os.makedirs(cache_dir, exist_ok=True)
    
    filepath = f"{cache_dir}/{data_type}.parquet"
    data.to_parquet(filepath, index=False)
    print(f"Cached {data_type} to {filepath}")



================================================
FILE: src/data/odds_fetcher.py
================================================
"""
Odds data fetcher using The Odds API.
Fetches spreads, totals, and moneylines from sportsbooks.
"""

import requests
import pandas as pd
from datetime import datetime
from typing import Optional, List, Dict
import os
from dotenv import load_dotenv

load_dotenv()


def fetch_odds(league: str, date: Optional[str] = None, regions: str = 'us', markets: str = 'spreads,totals,moneylines') -> pd.DataFrame:
    """
    Fetch odds from The Odds API.
    
    Args:
        league: 'nfl' or 'nba'
        date: Optional date string YYYY-MM-DD (default: today)
        regions: Comma-separated regions (default: 'us')
        markets: Comma-separated markets (default: 'spreads,totals,moneylines')
    
    Returns:
        DataFrame with odds data
    """
    api_key = os.getenv('ODDS_API_KEY')
    if not api_key:
        raise ValueError("ODDS_API_KEY not found in environment variables")
    
    # Convert league to API format
    sport = 'americanfootball_nfl' if league.upper() == 'NFL' else 'basketball_nba'
    
    # Build URL
    base_url = f"https://api.the-odds-api.com/v4/sports/{sport}/odds"
    
    params = {
        'apiKey': api_key,
        'regions': regions,
        'markets': markets,
    }
    
    if date:
        params['dateFormat'] = 'iso'
        # The Odds API expects dates in ISO format
        params['commenceTimeFrom'] = f"{date}T00:00:00Z"
        params['commenceTimeTo'] = f"{date}T23:59:59Z"
    
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        
        data = response.json()
        
        # Transform to DataFrame
        rows = []
        for game in data:
            game_id = game.get('id')
            commence_time = game.get('commence_time')
            home_team = game.get('home_team')
            away_team = game.get('away_team')
            
            for bookmaker in game.get('bookmakers', []):
                book_name = bookmaker.get('key')
                
                for market in bookmaker.get('markets', []):
                    market_key = market.get('key')
                    
                    for outcome in market.get('outcomes', []):
                        rows.append({
                            'game_id': game_id,
                            'commence_time': commence_time,
                            'home_team': home_team,
                            'away_team': away_team,
                            'book': book_name,
                            'market': market_key,
                            'outcome_name': outcome.get('name'),
                            'line': outcome.get('point'),
                            'price': outcome.get('price'),
                        })
        
        df = pd.DataFrame(rows)
        return df
        
    except requests.exceptions.RequestException as e:
        print(f"Error fetching odds: {e}")
        return pd.DataFrame()


def fetch_odds_for_date(league: str, date: str) -> pd.DataFrame:
    """
    Fetch odds for games on a specific date.
    
    Args:
        league: 'nfl' or 'nba'
        date: Date string YYYY-MM-DD
    
    Returns:
        DataFrame with odds for that date
    """
    return fetch_odds(league, date=date)


def cache_odds_data(data: pd.DataFrame, league: str, date: str):
    """
    Cache odds data to disk.
    
    Args:
        data: DataFrame to cache
        league: 'nfl' or 'nba'
        date: Date string YYYY-MM-DD
    """
    cache_dir = f"data/raw/{league}/{date}"
    os.makedirs(cache_dir, exist_ok=True)
    
    filepath = f"{cache_dir}/odds.parquet"
    data.to_parquet(filepath, index=False)
    print(f"Cached odds to {filepath}")




================================================
FILE: src/features/__init__.py
================================================
# Feature engineering modules




================================================
FILE: src/features/form_metrics.py
================================================
"""
Form metrics feature engineering.
Computes rolling averages for team performance (net rating for NBA, EPA for NFL).
"""

import pandas as pd
import numpy as np
from typing import Optional


def compute_rolling_net_rating(team: str, game_date: pd.Timestamp, 
                               game_logs: pd.DataFrame, window: int = 3) -> Optional[float]:
    """
    Compute rolling net rating (points per 100 possessions) for NBA team.
    
    Args:
        team: Team abbreviation
        game_date: Date of current game
        game_logs: DataFrame with game logs (must have 'game_date', 'team', 'off_rating', 'def_rating')
        window: Number of games to look back
    
    Returns:
        Rolling net rating (off_rating - def_rating) or None
    """
    team_games = game_logs[
        (game_logs['team'] == team) &
        (pd.to_datetime(game_logs['game_date']) < game_date)
    ].sort_values('game_date')
    
    if len(team_games) < window:
        return None
    
    recent_games = team_games.tail(window)
    
    if 'net_rating' in recent_games.columns:
        return recent_games['net_rating'].mean()
    elif 'off_rating' in recent_games.columns and 'def_rating' in recent_games.columns:
        net_rating = recent_games['off_rating'] - recent_games['def_rating']
        return net_rating.mean()
    
    return None


def compute_rolling_epa(team: str, game_date: pd.Timestamp, 
                        play_by_play: pd.DataFrame, window: int = 3,
                        side: str = 'offense') -> Optional[float]:
    """
    Compute rolling EPA (Expected Points Added) for NFL team.
    
    Args:
        team: Team abbreviation
        game_date: Date of current game
        play_by_play: DataFrame with play-by-play data (must have 'posteam', 'game_date', 'epa')
        window: Number of games to look back
        side: 'offense' or 'defense'
    
    Returns:
        Rolling EPA per play or None
    """
    if side == 'offense':
        team_games = play_by_play[
            (play_by_play['posteam'] == team) &
            (pd.to_datetime(play_by_play['game_date']) < game_date)
        ]
    else:  # defense
        team_games = play_by_play[
            (play_by_play['defteam'] == team) &
            (pd.to_datetime(play_by_play['game_date']) < game_date)
        ]
    
    if team_games.empty:
        return None
    
    # Group by game and compute average EPA, keeping game_date for sorting
    game_epa = team_games.groupby('game_id').agg({
        'epa': 'mean',
        'game_date': 'first'  # Get first game_date for each game_id
    }).reset_index()
    game_epa['game_date'] = pd.to_datetime(game_epa['game_date'])
    game_epa = game_epa.sort_values('game_date')
    
    if len(game_epa) < window:
        return None
    
    recent_epa = game_epa.tail(window)['epa'].mean()
    return recent_epa


def compute_rolling_success_rate(team: str, game_date: pd.Timestamp,
                                play_by_play: pd.DataFrame, window: int = 3) -> Optional[float]:
    """
    Compute rolling success rate for NFL team.
    
    Args:
        team: Team abbreviation
        game_date: Date of current game
        play_by_play: DataFrame with play-by-play data (must have 'posteam', 'success')
        window: Number of games to look back
    
    Returns:
        Rolling success rate (0-1) or None
    """
    team_games = play_by_play[
        (play_by_play['posteam'] == team) &
        (pd.to_datetime(play_by_play['game_date']) < game_date)
    ]
    
    if team_games.empty:
        return None
    
    # Group by game, keeping game_date for sorting
    game_success = team_games.groupby('game_id').agg({
        'success': 'mean',
        'game_date': 'first'  # Get first game_date for each game_id
    }).reset_index()
    game_success['game_date'] = pd.to_datetime(game_success['game_date'])
    game_success = game_success.sort_values('game_date')
    
    if len(game_success) < window:
        return None
    
    recent_success = game_success.tail(window)['success'].mean()
    return recent_success


def add_form_features_nba(games_df: pd.DataFrame, game_logs: pd.DataFrame, 
                         window: int = 3) -> pd.DataFrame:
    """
    Add form features for NBA games.
    
    Args:
        games_df: DataFrame with games
        game_logs: DataFrame with game logs
        window: Rolling window size
    
    Returns:
        DataFrame with added columns: form_home_net_rating_3, form_away_net_rating_3, etc.
    """
    df = games_df.copy()
    
    df[f'form_home_net_rating_{window}'] = df.apply(
        lambda row: compute_rolling_net_rating(
            row['home_team'], pd.to_datetime(row['game_date']), game_logs, window
        ),
        axis=1
    )
    
    df[f'form_away_net_rating_{window}'] = df.apply(
        lambda row: compute_rolling_net_rating(
            row['away_team'], pd.to_datetime(row['game_date']), game_logs, window
        ),
        axis=1
    )
    
    return df


def add_form_features_nfl(games_df: pd.DataFrame, play_by_play: pd.DataFrame,
                          window: int = 3) -> pd.DataFrame:
    """
    Add form features for NFL games.
    
    Args:
        games_df: DataFrame with games
        play_by_play: DataFrame with play-by-play data
        window: Rolling window size
    
    Returns:
        DataFrame with added columns: form_home_epa_off_3, form_away_epa_off_3, etc.
    """
    df = games_df.copy()
    
    df[f'form_home_epa_off_{window}'] = df.apply(
        lambda row: compute_rolling_epa(
            row['home_team'], pd.to_datetime(row['game_date']), play_by_play, window, 'offense'
        ),
        axis=1
    )
    
    df[f'form_away_epa_off_{window}'] = df.apply(
        lambda row: compute_rolling_epa(
            row['away_team'], pd.to_datetime(row['game_date']), play_by_play, window, 'offense'
        ),
        axis=1
    )
    
    df[f'form_home_epa_def_{window}'] = df.apply(
        lambda row: compute_rolling_epa(
            row['home_team'], pd.to_datetime(row['game_date']), play_by_play, window, 'defense'
        ),
        axis=1
    )
    
    df[f'form_away_epa_def_{window}'] = df.apply(
        lambda row: compute_rolling_epa(
            row['away_team'], pd.to_datetime(row['game_date']), play_by_play, window, 'defense'
        ),
        axis=1
    )
    
    return df




================================================
FILE: src/features/rest_schedule.py
================================================
"""
Rest and schedule feature engineering.
Computes rest days, back-to-back flags, and travel distance.
"""

import pandas as pd
from datetime import datetime, timedelta
from typing import Optional
import numpy as np


def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """
    Calculate the great circle distance between two points on Earth (in km).
    
    Args:
        lat1, lon1: Latitude and longitude of first point
        lat2, lon2: Latitude and longitude of second point
    
    Returns:
        Distance in kilometers
    """
    R = 6371  # Earth radius in km
    
    dlat = np.radians(lat2 - lat1)
    dlon = np.radians(lon2 - lon1)
    
    a = (np.sin(dlat / 2) ** 2 +
         np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) *
         np.sin(dlon / 2) ** 2)
    
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    distance = R * c
    
    return distance


# Team city coordinates (approximate)
TEAM_COORDINATES = {
    # NFL teams
    'ARI': (33.5275, -112.2625), 'ATL': (33.7550, -84.4010), 'BAL': (39.2780, -76.6227),
    'BUF': (42.7738, -78.7869), 'CAR': (35.2258, -80.8528), 'CHI': (41.8625, -87.6167),
    'CIN': (39.0950, -84.5160), 'CLE': (41.5061, -81.6996), 'DAL': (32.7473, -97.0945),
    'DEN': (39.7439, -105.0200), 'DET': (42.3400, -83.0456), 'GB': (44.5013, -88.0622),
    'HOU': (29.6847, -95.4107), 'IND': (39.7601, -86.1639), 'JAX': (30.3239, -81.6372),
    'KC': (39.0489, -94.4839), 'LV': (37.7519, -122.2009), 'LAC': (33.8643, -118.2611),
    'LAR': (34.0141, -118.2877), 'MIA': (25.9581, -80.2389), 'MIN': (44.9740, -93.2581),
    'NE': (42.0909, -71.2643), 'NO': (29.9511, -90.0815), 'NYG': (40.8135, -74.0745),
    'NYJ': (40.8135, -74.0745), 'PHI': (39.9008, -75.1673), 'PIT': (40.4468, -80.0158),
    'SF': (37.4033, -121.9694), 'SEA': (47.5952, -122.3316), 'TB': (27.9753, -82.5033),
    'TEN': (36.1665, -86.7713), 'WAS': (38.9077, -76.8644),
    # NBA teams
    'ATL': (33.7550, -84.4010), 'BOS': (42.3662, -71.0621), 'BKN': (40.6826, -73.9744),
    'CHA': (35.2258, -80.8528), 'CHI': (41.8625, -87.6167), 'CLE': (41.5061, -81.6996),
    'DAL': (32.7903, -96.8102), 'DEN': (39.7439, -105.0200), 'DET': (42.3400, -83.0456),
    'GSW': (37.7680, -122.3879), 'HOU': (29.7508, -95.3621), 'IND': (39.7639, -86.1554),
    'LAC': (34.0430, -118.2673), 'LAL': (34.0430, -118.2673), 'MEM': (35.1380, -90.0506),
    'MIA': (25.7814, -80.1866), 'MIL': (43.0436, -87.9169), 'MIN': (44.9794, -93.2771),
    'NOP': (29.9490, -90.0821), 'NYK': (40.7505, -73.9934), 'OKC': (35.4634, -97.5151),
    'ORL': (28.5392, -81.3839), 'PHI': (39.9012, -75.1720), 'PHX': (33.4453, -112.0712),
    'POR': (45.5316, -122.6668), 'SAC': (38.5802, -121.4998), 'SAS': (29.4269, -98.4375),
    'TOR': (43.6435, -79.3791), 'UTA': (40.7683, -111.9011), 'WAS': (38.8981, -77.0209),
}


def _filter_games_by_season(games_df: pd.DataFrame, season: Optional[int]) -> pd.DataFrame:
    """Return only games from the requested season."""
    if season is None or games_df.empty:
        return games_df
    if 'season' in games_df.columns:
        season_values = pd.to_numeric(games_df['season'], errors='coerce')
        return games_df[season_values == season]
    game_years = pd.to_datetime(games_df['game_date']).dt.year
    return games_df[game_years == season]


def compute_rest_days(team: str, game_date: datetime, previous_games: pd.DataFrame,
                      season: Optional[int] = None) -> Optional[int]:
    """
    Compute days of rest for a team before a game.
    
    Args:
        team: Team abbreviation
        game_date: Date of current game
        previous_games: DataFrame with previous games (must have 'game_date' and team columns)
    
    Returns:
        Number of days rest (or None if no previous game found)
    """
    # Find team's last game before this date
    season_games = _filter_games_by_season(previous_games, season)
    team_games = season_games[
        ((season_games['home_team'] == team) | (season_games['away_team'] == team)) &
        (pd.to_datetime(season_games['game_date']) < game_date)
    ]
    
    if team_games.empty:
        return None
    
    last_game_date = pd.to_datetime(team_games['game_date']).max()
    rest_days = (game_date - last_game_date).days
    
    return rest_days


def is_back_to_back(team: str, game_date: datetime, previous_games: pd.DataFrame,
                    season: Optional[int] = None) -> bool:
    """
    Check if team is playing back-to-back games.
    
    Args:
        team: Team abbreviation
        game_date: Date of current game
        previous_games: DataFrame with previous games
    
    Returns:
        True if playing back-to-back (1 day rest or less)
    """
    rest_days = compute_rest_days(team, game_date, previous_games, season=season)
    return rest_days is not None and rest_days <= 1


def compute_travel_distance(team: str, game_date: datetime, previous_games: pd.DataFrame,
                           is_home: bool, season: Optional[int] = None) -> Optional[float]:
    """
    Compute travel distance (km) from team's last game location to current game.
    
    Args:
        team: Team abbreviation
        game_date: Date of current game
        previous_games: DataFrame with previous games (must have 'home_team', 'away_team', 'game_date')
        is_home: Whether current game is at home
    
    Returns:
        Travel distance in km (or None if can't compute)
    """
    # Find team's last game
    season_games = _filter_games_by_season(previous_games, season)
    team_games = season_games[
        ((season_games['home_team'] == team) | (season_games['away_team'] == team)) &
        (pd.to_datetime(season_games['game_date']) < game_date)
    ]
    
    if team_games.empty:
        return None
    
    last_game = team_games.iloc[-1]
    was_home_last = last_game['home_team'] == team
    
    # Get coordinates
    if was_home_last:
        last_location = TEAM_COORDINATES.get(team)
    else:
        opponent = last_game['home_team'] if was_home_last else last_game['away_team']
        last_location = TEAM_COORDINATES.get(opponent)
    
    if is_home:
        current_location = TEAM_COORDINATES.get(team)
    else:
        # Need opponent - this would come from the game data
        # For now, return None if away (would need game context)
        return None
    
    if last_location is None or current_location is None:
        return None
    
    distance = haversine_distance(
        last_location[0], last_location[1],
        current_location[0], current_location[1]
    )
    
    return distance


def add_rest_features(games_df: pd.DataFrame, historical_games: pd.DataFrame) -> pd.DataFrame:
    """
    Add rest and schedule features to games DataFrame.
    
    Args:
        games_df: DataFrame with games to add features to
        historical_games: Historical games for computing rest
    
    Returns:
        DataFrame with added columns: rest_home, rest_away, b2b_home, b2b_away
    """
    df = games_df.copy()
    
    def _season_for_row(row):
        season_val = row.get('season')
        if pd.notna(season_val):
            try:
                return int(season_val)
            except (TypeError, ValueError):
                return pd.to_datetime(row['game_date']).year
        return pd.to_datetime(row['game_date']).year
    
    df['rest_home'] = df.apply(
        lambda row: compute_rest_days(
            row['home_team'],
            pd.to_datetime(row['game_date']),
            historical_games,
            season=_season_for_row(row)
        ),
        axis=1
    )
    
    df['rest_away'] = df.apply(
        lambda row: compute_rest_days(
            row['away_team'],
            pd.to_datetime(row['game_date']),
            historical_games,
            season=_season_for_row(row)
        ),
        axis=1
    )
    
    df['b2b_home'] = df.apply(
        lambda row: is_back_to_back(
            row['home_team'],
            pd.to_datetime(row['game_date']),
            historical_games,
            season=_season_for_row(row)
        ),
        axis=1
    )
    
    df['b2b_away'] = df.apply(
        lambda row: is_back_to_back(
            row['away_team'],
            pd.to_datetime(row['game_date']),
            historical_games,
            season=_season_for_row(row)
        ),
        axis=1
    )
    
    return df



================================================
FILE: src/features/strength.py
================================================
"""
Opponent strength feature engineering.
Computes season-long opponent strength metrics (SRS, Elo, etc.).
"""

import pandas as pd
import numpy as np
from typing import Optional


def compute_season_opponent_strength(team: str, game_date: pd.Timestamp,
                                    historical_games: pd.DataFrame,
                                    league: str = 'nfl',
                                    season: Optional[int] = None) -> Optional[float]:
    """
    Compute opponent strength for a team based on season performance.
    
    For NFL: Uses point differential and strength of schedule
    For NBA: Uses net rating and strength of schedule
    
    Args:
        team: Team abbreviation
        game_date: Date of current game
        historical_games: DataFrame with historical games
        league: 'nfl' or 'nba'
    
    Returns:
        Opponent strength metric (higher = stronger opponents faced)
    """
    # Determine season to use
    season_val = season or game_date.year
    
    if 'season' in historical_games.columns:
        season_mask = pd.to_numeric(historical_games['season'], errors='coerce') == season_val
    else:
        season_mask = pd.to_datetime(historical_games['game_date']).dt.year == season_val
    
    game_dates = pd.to_datetime(historical_games['game_date'])
    
    # Get all games before this date for the same season
    season_games = historical_games[
        season_mask & (game_dates < game_date)
    ]
    
    # Find games where team played
    team_games = season_games[
        (season_games['home_team'] == team) | (season_games['away_team'] == team)
    ]
    
    if team_games.empty:
        return None
    
    # Get opponents
    opponents = []
    for _, game in team_games.iterrows():
        if game['home_team'] == team:
            opponents.append(game['away_team'])
        else:
            opponents.append(game['home_team'])
    
    # Compute average opponent strength
    # Simple approach: average point differential of opponents
    opponent_strengths = []
    
    for opp in set(opponents):
        opp_games = season_games[
            ((season_games['home_team'] == opp) | (season_games['away_team'] == opp))
        ]
        
        if opp_games.empty:
            continue
        
        # Compute average point differential
        if 'home_score' in opp_games.columns and 'away_score' in opp_games.columns:
            opp_games = opp_games.copy()
            opp_games['point_diff'] = np.where(
                opp_games['home_team'] == opp,
                opp_games['home_score'] - opp_games['away_score'],
                opp_games['away_score'] - opp_games['home_score']
            )
            avg_diff = opp_games['point_diff'].mean()
            opponent_strengths.append(avg_diff)
    
    if not opponent_strengths:
        return None
    
    return np.mean(opponent_strengths)


def add_opponent_strength_features(games_df: pd.DataFrame, 
                                   historical_games: pd.DataFrame,
                                   league: str = 'nfl') -> pd.DataFrame:
    """
    Add opponent strength features to games DataFrame.
    
    Args:
        games_df: DataFrame with games
        historical_games: Historical games for computing strength
        league: 'nfl' or 'nba'
    
    Returns:
        DataFrame with added columns: opp_strength_home_season, opp_strength_away_season
    """
    df = games_df.copy()
    
    def _season_for_row(row):
        season_val = row.get('season')
        if pd.notna(season_val):
            try:
                return int(season_val)
            except (TypeError, ValueError):
                return pd.to_datetime(row['game_date']).year
        return pd.to_datetime(row['game_date']).year
    
    df['opp_strength_home_season'] = df.apply(
        lambda row: compute_season_opponent_strength(
            row['home_team'],
            pd.to_datetime(row['game_date']),
            historical_games,
            league,
            season=_season_for_row(row)
        ),
        axis=1
    )
    
    df['opp_strength_away_season'] = df.apply(
        lambda row: compute_season_opponent_strength(
            row['away_team'],
            pd.to_datetime(row['game_date']),
            historical_games,
            league,
            season=_season_for_row(row)
        ),
        axis=1
    )
    
    return df



================================================
FILE: src/models/__init__.py
================================================
# Model training and inference modules




================================================
FILE: src/models/link_function.py
================================================
"""
Link function to convert between spread and win probability.
Calibrated on historical game margins.
"""

import numpy as np
import pandas as pd
from scipy.optimize import curve_fit
from typing import Optional


def logistic_link(spread: float, a: float = 1.0, b: float = 0.0) -> float:
    """
    Convert point spread to win probability using logistic function.
    
    Args:
        spread: Point spread (home margin)
        a: Scaling parameter (default: 1.0, meaning ~1 point = ~2.5% win prob change)
        b: Offset parameter
    
    Returns:
        Win probability (0-1)
    """
    # Standard logistic: P = 1 / (1 + exp(-a * (spread - b)))
    # For spread of 0, win prob should be ~0.5 (home court advantage)
    # For spread of +3, win prob should be ~0.65-0.70
    return 1 / (1 + np.exp(-a * (spread - b)))


def fit_link_function(spread_like_signal: pd.Series, historical_wins: pd.Series,
                      max_abs_a: float = 0.3, min_abs_a: float = 0.05) -> tuple:
    """
    Fit link function parameters from historical data with slope clipping.
    
    Args:
        spread_like_signal: Series of spread predictions or market spreads
        historical_wins: Series of binary outcomes (1 = home wins, 0 = away wins)
        max_abs_a: Maximum absolute slope allowed for stability
        min_abs_a: Minimum absolute slope to avoid flat link
    
    Returns:
        Tuple of (a, b) parameters for logistic_link
    """
    def logistic_func(x, a, b):
        return 1 / (1 + np.exp(-a * (x - b)))
    
    x = np.asarray(spread_like_signal, dtype=float)
    y = np.asarray(historical_wins, dtype=float)
    
    if np.unique(y).size < 2:
        raise ValueError("Need both home and away wins to calibrate link function.")
    
    # Fit using curve_fit
    popt, _ = curve_fit(logistic_func, x, y, p0=[0.15, 2.5], maxfev=10000)
    
    a = float(np.clip(popt[0], -max_abs_a, max_abs_a))
    if abs(a) < min_abs_a:
        a = np.sign(a) * min_abs_a if a != 0 else min_abs_a
    b = float(popt[1])
    
    return a, b


def spread_to_win_prob(spread: float, a: float = 0.15, b: float = 2.5) -> float:
    """
    Convert point spread to win probability.
    
    Default parameters calibrated on NFL/NBA data:
    - Spread of 0 → ~55% win prob (home advantage)
    - Spread of +3 → ~65% win prob
    - Spread of +7 → ~75% win prob
    
    Args:
        spread: Point spread (home margin, positive = home favored)
        a: Scaling parameter (default: 0.15)
        b: Offset parameter (default: 2.5)
    
    Returns:
        Home win probability (0-1)
    """
    return logistic_link(spread, a, b)


def win_prob_to_spread(win_prob: float, a: float = 0.15, b: float = 2.5) -> float:
    """
    Convert win probability to point spread.
    
    Inverse of spread_to_win_prob.
    
    Args:
        win_prob: Home win probability (0-1)
        a: Scaling parameter (default: 0.15)
        b: Offset parameter (default: 2.5)
    
    Returns:
        Point spread (home margin)
    """
    # Inverse logistic: spread = b - (1/a) * ln((1/P) - 1)
    if win_prob <= 0:
        return -np.inf
    if win_prob >= 1:
        return np.inf
    
    spread = b - (1 / a) * np.log((1 / win_prob) - 1)
    return spread



================================================
FILE: src/models/predictor.py
================================================
"""
Production prediction module for NFL/NBA games.
Provides functions to predict win probabilities and spreads for specific games.
"""

import pandas as pd
import numpy as np
import pickle
import os
from typing import Dict, Optional, List
from datetime import datetime
import sys

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.features import rest_schedule, form_metrics, strength
from src.models.link_function import spread_to_win_prob, win_prob_to_spread

DEFAULT_LINK_PARAMS = (0.15, 2.5)


class GamePredictor:
    """
    Production predictor for game outcomes.
    Loads saved models and makes predictions for specific games.
    """
    
    def __init__(self, league: str, model_version: str = 'v1'):
        """
        Initialize predictor for a league.
        
        Args:
            league: 'NFL' or 'NBA'
            model_version: Model version string (e.g., 'v1')
        """
        self.league = league.upper()
        self.model_version = model_version
        self.models_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'models')
        
        # Load models
        self.win_prob_model = None
        self.spread_model = None
        self.feature_names = None
        self.win_feature_names = None
        self.spread_feature_names = None
        self.link_params = None
        self._median_cache = None
        
        self._load_models()
    
    def _load_models(self):
        """Load saved models from disk."""
        win_prob_path = os.path.join(self.models_dir, f'win_prob_model_{self.league.lower()}_{self.model_version}.pkl')
        spread_path = os.path.join(self.models_dir, f'spread_model_{self.league.lower()}_{self.model_version}.pkl')
        link_path = os.path.join(self.models_dir, f'link_function_{self.league.lower()}_{self.model_version}.pkl')
        
        if not os.path.exists(win_prob_path):
            raise FileNotFoundError(f"Win probability model not found: {win_prob_path}")
        if not os.path.exists(spread_path):
            raise FileNotFoundError(f"Spread model not found: {spread_path}")
        
        # Load win probability model
        with open(win_prob_path, 'rb') as f:
            win_prob_data = pickle.load(f)
            self.win_prob_model = win_prob_data['model']
            self.win_feature_names = win_prob_data.get('win_feature_names') or win_prob_data.get('feature_names')
        
        # Load spread model
        with open(spread_path, 'rb') as f:
            spread_data = pickle.load(f)
            self.spread_model = spread_data['model']
            self.spread_feature_names = spread_data.get('spread_feature_names') or spread_data.get('feature_names')
        
        if self.win_feature_names is None:
            self.win_feature_names = self.spread_feature_names
        if self.spread_feature_names is None:
            self.spread_feature_names = self.win_feature_names
        self.feature_names = sorted(set(self.win_feature_names or []) | set(self.spread_feature_names or []))
        
        # Load link function parameters
        if os.path.exists(link_path):
            with open(link_path, 'rb') as f:
                link_data = pickle.load(f)
                a = float(link_data.get('a', DEFAULT_LINK_PARAMS[0]))
                b = float(link_data.get('b', DEFAULT_LINK_PARAMS[1]))
                if (not np.isfinite(a)) or (not np.isfinite(b)) or abs(a) > 1:
                    print(f"Warning: Link parameters out of bounds (a={a}, b={b}). Using defaults.")
                    self.link_params = DEFAULT_LINK_PARAMS
                else:
                    self.link_params = (a, b)
        else:
            self.link_params = DEFAULT_LINK_PARAMS  # Default values

    def _load_feature_medians(self) -> Dict[str, float]:
        """Load cached feature medians from disk."""
        if self._median_cache is not None:
            return self._median_cache
        
        median_path = os.path.join(self.models_dir, f'feature_medians_{self.league.lower()}_{self.model_version}.pkl')
        if os.path.exists(median_path):
            with open(median_path, 'rb') as f:
                self._median_cache = pickle.load(f)
        else:
            self._median_cache = {}
        return self._median_cache
    
    @staticmethod
    def _prepare_feature_matrix(features_df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        """Project features_df onto the requested columns, defaulting missing ones to zero."""
        X = pd.DataFrame()
        for col in columns:
            if col in features_df.columns:
                X[col] = features_df[col]
            else:
                X[col] = 0.0
        return X

    @staticmethod
    def _fill_with_medians(df: pd.DataFrame, medians: Dict[str, float]) -> pd.DataFrame:
        """Fill NaNs with provided medians, defaulting to zero when missing."""
        for col in df.columns:
            if df[col].isna().any():
                df[col] = df[col].fillna(medians.get(col, 0.0))
        return df
    
    def build_features_for_game(self, game_row: pd.DataFrame, historical_games: pd.DataFrame,
                                play_by_play: Optional[pd.DataFrame] = None,
                                game_logs: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """
        Build features for a single game or games.
        
        Args:
            game_row: DataFrame with game(s) to predict (must have home_team, away_team, game_date)
            historical_games: Historical games DataFrame for computing rest/strength features
            play_by_play: Optional PBP data for NFL form features
            game_logs: Optional game logs for NBA form features
        
        Returns:
            DataFrame with features
        """
        df = game_row.copy()
        
        # Ensure game_date is datetime
        if 'game_date' not in df.columns:
            if 'gameday' in df.columns:
                df['game_date'] = pd.to_datetime(df['gameday'])
            else:
                raise ValueError("game_date or gameday column required")
        else:
            df['game_date'] = pd.to_datetime(df['game_date'])
        
        # Add rest features
        df = rest_schedule.add_rest_features(df, historical_games)
        
        # Add opponent strength features
        df = strength.add_opponent_strength_features(df, historical_games, league=self.league.lower())
        
        # Add team strength features (win %, point diff)
        df = self._add_team_strength_features(df, historical_games)
        
        # Add interaction features
        df = self._add_interaction_features(df)
        
        # Add form features if available
        if self.league == 'NFL' and play_by_play is not None:
            for window in [3, 5, 10]:
                df = form_metrics.add_form_features_nfl(df, play_by_play, window=window)
        elif self.league == 'NBA' and game_logs is not None:
            for window in [3, 5, 10]:
                df = form_metrics.add_form_features_nba(df, game_logs, window=window)
        
        # Add form interaction features if form features exist
        df = self._add_form_interactions(df)
        
        return df
    
    def _add_team_strength_features(self, games_df: pd.DataFrame, historical_games: pd.DataFrame) -> pd.DataFrame:
        """Add team strength features (win %, point diff).
        
        IMPORTANT: Only uses current season data. No fallback to previous seasons.
        Teams change year-to-year, so using old data would be inaccurate.
        """
        df = games_df.copy()
        df['home_team_win_pct'] = np.nan
        df['away_team_win_pct'] = np.nan
        df['home_team_point_diff'] = np.nan
        df['away_team_point_diff'] = np.nan
        
        for idx, row in df.iterrows():
            game_date = pd.to_datetime(row['game_date'])
            home_team = row['home_team']
            away_team = row['away_team']
            season = row.get('season', game_date.year)
            
            # Only use current season games (completed games only)
            season_games = historical_games[
                (pd.to_datetime(historical_games['game_date']) < game_date) &
                (historical_games.get('season', pd.to_datetime(historical_games['game_date']).dt.year) == season)
            ]
            
            # Filter to only completed games (have scores)
            if 'home_score' in season_games.columns and 'away_score' in season_games.columns:
                season_games = season_games[
                    season_games['home_score'].notna() & season_games['away_score'].notna()
                ]
            
            # If no current season completed games, leave as NaN
            # This signals that we don't have enough data for accurate prediction
            if len(season_games) == 0:
                continue  # Leave features as NaN
            
            # Home team stats
            home_games = season_games[
                (season_games['home_team'] == home_team) | (season_games['away_team'] == home_team)
            ]
            if len(home_games) > 0:
                home_wins = sum(1 for _, g in home_games.iterrows()
                              if (g['home_team'] == home_team and g.get('home_score', 0) > g.get('away_score', 0)) or
                                 (g['away_team'] == home_team and g.get('away_score', 0) > g.get('home_score', 0)))
                home_point_diff = [
                    g.get('home_score', 0) - g.get('away_score', 0) if g['home_team'] == home_team
                    else g.get('away_score', 0) - g.get('home_score', 0)
                    for _, g in home_games.iterrows()
                ]
                df.loc[idx, 'home_team_win_pct'] = home_wins / len(home_games)
                df.loc[idx, 'home_team_point_diff'] = np.mean(home_point_diff) if home_point_diff else 0
            
            # Away team stats
            away_games = season_games[
                (season_games['home_team'] == away_team) | (season_games['away_team'] == away_team)
            ]
            if len(away_games) > 0:
                away_wins = sum(1 for _, g in away_games.iterrows()
                              if (g['home_team'] == away_team and g.get('home_score', 0) > g.get('away_score', 0)) or
                                 (g['away_team'] == away_team and g.get('away_score', 0) > g.get('home_score', 0)))
                away_point_diff = [
                    g.get('home_score', 0) - g.get('away_score', 0) if g['home_team'] == away_team
                    else g.get('away_score', 0) - g.get('home_score', 0)
                    for _, g in away_games.iterrows()
                ]
                df.loc[idx, 'away_team_win_pct'] = away_wins / len(away_games)
                df.loc[idx, 'away_team_point_diff'] = np.mean(away_point_diff) if away_point_diff else 0
        
        return df
    
    def _add_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add interaction and derived features."""
        # Rest differentials
        if 'rest_home' in df.columns and 'rest_away' in df.columns:
            df['rest_differential'] = df['rest_home'] - df['rest_away']
            df['rest_advantage_home'] = (df['rest_home'] > df['rest_away']).astype(int)
        
        # Team strength differentials
        if 'home_team_win_pct' in df.columns and 'away_team_win_pct' in df.columns:
            df['win_pct_differential'] = df['home_team_win_pct'] - df['away_team_win_pct']
        if 'home_team_point_diff' in df.columns and 'away_team_point_diff' in df.columns:
            df['point_diff_differential'] = df['home_team_point_diff'] - df['away_team_point_diff']
            df['point_diff_gap'] = df['away_team_point_diff'] - df['home_team_point_diff']
            df['abs_point_diff_gap'] = df['point_diff_gap'].abs()
            df['point_diff_gap_flag'] = (df['point_diff_gap'] > 5).astype(int)
        
        # Opponent strength differential
        if 'opp_strength_home_season' in df.columns and 'opp_strength_away_season' in df.columns:
            df['opp_strength_differential'] = df['opp_strength_home_season'] - df['opp_strength_away_season']
        
        # Time features
        if 'game_date' in df.columns:
            df['week_number'] = pd.to_datetime(df['game_date']).dt.isocalendar().week
            df['month'] = pd.to_datetime(df['game_date']).dt.month
            if 'game_type' in df.columns:
                df['is_playoff'] = df['game_type'].str.contains('POST', case=False, na=False).astype(int)
            else:
                df['is_playoff'] = 0
        
        return df
    
    def _add_form_interactions(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add form feature interactions."""
        form_cols = [col for col in df.columns if col.startswith('form_')]
        
        for window in [3, 5, 10]:
            home_off = f'form_home_epa_off_{window}' if self.league == 'NFL' else f'form_home_net_rating_{window}'
            away_off = f'form_away_epa_off_{window}' if self.league == 'NFL' else f'form_away_net_rating_{window}'
            
            if home_off in df.columns and away_off in df.columns:
                df[f'form_off_diff_{window}'] = df[home_off] - df[away_off]
            
            if self.league == 'NFL':
                home_def = f'form_home_epa_def_{window}'
                away_def = f'form_away_epa_def_{window}'
                if home_def in df.columns and away_def in df.columns:
                    df[f'form_def_diff_{window}'] = df[home_def] - df[away_def]
        
        return df
    
    def predict(self, game_row: pd.DataFrame, historical_games: pd.DataFrame,
                play_by_play: Optional[pd.DataFrame] = None,
                game_logs: Optional[pd.DataFrame] = None,
                fill_missing_with_median: bool = True) -> Dict:
        """
        Predict outcome for a game.
        
        Args:
            game_row: DataFrame with game to predict
            historical_games: Historical games for feature computation
            play_by_play: Optional PBP data (NFL)
            game_logs: Optional game logs (NBA)
            fill_missing_with_median: Whether to fill missing features with median values
        
        Returns:
            Dictionary with predictions
        """
        # Build features
        features_df = self.build_features_for_game(game_row, historical_games, play_by_play, game_logs)
        
        medians = self._load_feature_medians() if fill_missing_with_median else None
        spread_cols = self.spread_feature_names or self.feature_names
        win_cols = self.win_feature_names or self.feature_names
        
        X_spread = self._prepare_feature_matrix(features_df, spread_cols)
        if fill_missing_with_median and medians is not None:
            X_spread = self._fill_with_medians(X_spread, medians)
        else:
            X_spread = X_spread.fillna(0)
        spread_pred = self.spread_model.predict(X_spread)
        
        X_win = self._prepare_feature_matrix(features_df, win_cols)
        if 'model_spread_feature' in X_win.columns:
            X_win['model_spread_feature'] = spread_pred
        if fill_missing_with_median and medians is not None:
            X_win = self._fill_with_medians(X_win, medians)
        else:
            X_win = X_win.fillna(0)
        
        # Predict
        win_prob_proba = self.win_prob_model.predict_proba(X_win)[:, 1]  # Probability of home win
        
        # Convert to win prob using link function (for consistency)
        link_a, link_b = self.link_params
        win_prob_from_spread = spread_to_win_prob(spread_pred, link_a, link_b)
        
        # Blend models smoothly; never fully discard either side so extreme disagreements stay informative
        disagreement = np.abs(win_prob_proba - win_prob_from_spread)
        max_disagreement = 0.30
        base_weight = 0.65
        min_weight = 0.25
        weight_drop = (disagreement / max_disagreement) * 0.4
        ensemble_weight = np.clip(base_weight - weight_drop, min_weight, base_weight)
        final_win_prob = (ensemble_weight * win_prob_proba +
                         (1 - ensemble_weight) * win_prob_from_spread)
        
        # Return predictions
        results = []
        for idx in range(len(game_row)):
            disagreement_val = disagreement[idx]
            model_disagreement = disagreement_val > 0.15
            
            result = {
                'home_team': game_row.iloc[idx]['home_team'],
                'away_team': game_row.iloc[idx]['away_team'],
                'game_date': pd.to_datetime(game_row.iloc[idx]['game_date']).strftime('%Y-%m-%d'),
                'predicted_spread': float(spread_pred[idx]),
                # Use ensemble probability as primary
                'home_win_probability': float(final_win_prob[idx]),
                'away_win_probability': float(1 - final_win_prob[idx]),
                # Keep individual model outputs for transparency
                'home_win_prob_from_model': float(win_prob_proba[idx]),
                'win_prob_from_spread': float(win_prob_from_spread[idx]),
                'model_disagreement': float(disagreement_val),
                'predicted_winner': game_row.iloc[idx]['home_team'] if final_win_prob[idx] > 0.5 else game_row.iloc[idx]['away_team'],
                'confidence': float(abs(final_win_prob[idx] - 0.5) * 2),
                'spread_interpretation': (
                    f"{game_row.iloc[idx]['home_team']} by {abs(spread_pred[idx]):.1f}"
                    if spread_pred[idx] > 0
                    else f"{game_row.iloc[idx]['away_team']} by {abs(spread_pred[idx]):.1f}"
                )
            }
            results.append(result)
        
        return results[0] if len(results) == 1 else results
    
    def predict_batch(self, games_df: pd.DataFrame, historical_games: pd.DataFrame,
                     play_by_play: Optional[pd.DataFrame] = None,
                     game_logs: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """
        Predict outcomes for multiple games.
        
        Returns:
            DataFrame with predictions
        """
        results = []
        for idx, game in games_df.iterrows():
            try:
                pred = self.predict(
                    pd.DataFrame([game]),
                    historical_games,
                    play_by_play,
                    game_logs
                )
                results.append(pred)
            except Exception as e:
                print(f"Error predicting {game.get('home_team')} vs {game.get('away_team')}: {e}")
                continue
        
        return pd.DataFrame(results)


def predict_single_game(home_team: str, away_team: str, game_date: str,
                       league: str, historical_games: pd.DataFrame,
                       play_by_play: Optional[pd.DataFrame] = None,
                       game_logs: Optional[pd.DataFrame] = None,
                       model_version: str = 'v1') -> Dict:
    """
    Convenience function to predict a single game.
    
    Args:
        home_team: Home team abbreviation
        away_team: Away team abbreviation
        game_date: Game date (YYYY-MM-DD)
        league: 'NFL' or 'NBA'
        historical_games: Historical games DataFrame
        play_by_play: Optional PBP data (NFL)
        game_logs: Optional game logs (NBA)
        model_version: Model version
    
    Returns:
        Prediction dictionary
    """
    predictor = GamePredictor(league, model_version)
    
    game_row = pd.DataFrame({
        'home_team': [home_team],
        'away_team': [away_team],
        'game_date': [game_date],
        'season': [pd.to_datetime(game_date).year]
    })
    
    return predictor.predict(game_row, historical_games, play_by_play, game_logs)



================================================
FILE: src/models/spread_model.py
================================================
"""
Spread prediction model (regression).
Predicts point spread (home margin).
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
import lightgbm as lgb
import pickle
import os
from typing import Optional, Dict


class SpreadModel:
    """
    Model to predict point spread (home team margin).
    Positive values mean home team is favored.
    """
    
    def __init__(self, model_type: str = 'ridge', **kwargs):
        """
        Initialize spread model.
        
        Args:
            model_type: 'ridge', 'rf', or 'lightgbm'
            **kwargs: Model-specific hyperparameters
        """
        self.model_type = model_type
        
        if model_type == 'ridge':
            self.model = Ridge(alpha=kwargs.get('alpha', 1.0))
        elif model_type == 'rf':
            self.model = RandomForestRegressor(
                n_estimators=kwargs.get('n_estimators', 100),
                max_depth=kwargs.get('max_depth', 10),
                random_state=42
            )
        elif model_type == 'lightgbm':
            self.model = lgb.LGBMRegressor(
                n_estimators=kwargs.get('n_estimators', 100),
                max_depth=kwargs.get('max_depth', 5),
                learning_rate=kwargs.get('learning_rate', 0.1),
                random_state=42
            )
        else:
            raise ValueError(f"Unknown model_type: {model_type}")
        
        self.feature_names = None
    
    def fit(self, X: pd.DataFrame, y: pd.Series):
        """
        Train the model.
        
        Args:
            X: Feature matrix
            y: Target (actual home margin)
        """
        self.feature_names = list(X.columns)
        self.model.fit(X.values, y.values)
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict spreads.
        
        Args:
            X: Feature matrix
        
        Returns:
            Predicted spreads
        """
        if self.feature_names is None:
            raise ValueError("Model not trained yet")
        
        # Ensure columns match
        X_aligned = X[[col for col in self.feature_names if col in X.columns]]
        return self.model.predict(X_aligned.values)
    
    def save(self, filepath: str, version: str = 'v0.1.0'):
        """
        Save model to disk.
        
        Args:
            filepath: Path to save model
            version: Model version string
        """
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        model_data = {
            'model': self.model,
            'model_type': self.model_type,
            'feature_names': self.feature_names,
            'version': version
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'SpreadModel':
        """
        Load model from disk.
        
        Args:
            filepath: Path to model file
        
        Returns:
            Loaded SpreadModel instance
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        instance = cls(model_type=model_data['model_type'])
        instance.model = model_data['model']
        instance.feature_names = model_data['feature_names']
        
        return instance




================================================
FILE: src/models/win_prob_model.py
================================================
"""
Win probability model (classification).
Predicts home team win probability.
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgb
import pickle
import os
from typing import Optional


class WinProbModel:
    """
    Model to predict home team win probability (0-1).
    """
    
    def __init__(self, model_type: str = 'logistic', **kwargs):
        """
        Initialize win probability model.
        
        Args:
            model_type: 'logistic', 'rf', or 'lightgbm'
            **kwargs: Model-specific hyperparameters
        """
        self.model_type = model_type
        
        if model_type == 'logistic':
            self.model = LogisticRegression(
                max_iter=kwargs.get('max_iter', 1000),
                random_state=42
            )
        elif model_type == 'rf':
            self.model = RandomForestClassifier(
                n_estimators=kwargs.get('n_estimators', 100),
                max_depth=kwargs.get('max_depth', 10),
                random_state=42
            )
        elif model_type == 'lightgbm':
            self.model = lgb.LGBMClassifier(
                n_estimators=kwargs.get('n_estimators', 100),
                max_depth=kwargs.get('max_depth', 5),
                learning_rate=kwargs.get('learning_rate', 0.1),
                random_state=42
            )
        else:
            raise ValueError(f"Unknown model_type: {model_type}")
        
        self.feature_names = None
    
    def fit(self, X: pd.DataFrame, y: pd.Series):
        """
        Train the model.
        
        Args:
            X: Feature matrix
            y: Target (1 if home wins, 0 if away wins)
        """
        self.feature_names = list(X.columns)
        self.model.fit(X.values, y.values)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict win probabilities.
        
        Args:
            X: Feature matrix
        
        Returns:
            Array of shape (n_samples, 2) with [P(away wins), P(home wins)]
        """
        if self.feature_names is None:
            raise ValueError("Model not trained yet")
        
        # Ensure columns match
        X_aligned = X[[col for col in self.feature_names if col in X.columns]]
        return self.model.predict_proba(X_aligned.values)
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Predict binary outcomes.
        
        Args:
            X: Feature matrix
        
        Returns:
            Binary predictions (1 = home wins, 0 = away wins)
        """
        proba = self.predict_proba(X)
        return (proba[:, 1] > 0.5).astype(int)
    
    def save(self, filepath: str, version: str = 'v0.1.0'):
        """
        Save model to disk.
        
        Args:
            filepath: Path to save model
            version: Model version string
        """
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        model_data = {
            'model': self.model,
            'model_type': self.model_type,
            'feature_names': self.feature_names,
            'version': version
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'WinProbModel':
        """
        Load model from disk.
        
        Args:
            filepath: Path to model file
        
        Returns:
            Loaded WinProbModel instance
        """
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        instance = cls(model_type=model_data['model_type'])
        instance.model = model_data['model']
        instance.feature_names = model_data['feature_names']
        
        return instance




================================================
FILE: src/pipeline/__init__.py
================================================
# Pipeline and CLI modules




================================================
FILE: src/pipeline/refresh.py
================================================
"""
CLI module for refreshing predictions.
Usage: python -m src.pipeline.refresh --league NFL --date 2025-11-06
"""

import argparse
import sys
import os
import pickle
from datetime import datetime
from typing import Optional
import pandas as pd
import numpy as np
import json
from dotenv import load_dotenv
from supabase import create_client, Client

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.data import nfl_fetcher, nba_fetcher, odds_fetcher
from src.features import rest_schedule, form_metrics, strength
from src.models.spread_model import SpreadModel
from src.models.win_prob_model import WinProbModel
from src.models.link_function import spread_to_win_prob
from src.models.predictor import GamePredictor

load_dotenv()


def get_supabase_client() -> Client:
    """Create Supabase client."""
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
    
    if not url or not key:
        raise ValueError("SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY must be set")
    
    return create_client(url, key)


def load_historical_data(league: str, current_season: int, seasons_back: int = 3) -> dict:
    """
    Load historical data for feature building.
    
    Args:
        league: 'NFL' or 'NBA'
        current_season: Current season year
        seasons_back: Number of seasons to load
    
    Returns:
        Dictionary with historical_games, play_by_play (NFL), or game_logs (NBA)
    """
    historical_data = {
        'historical_games': [],
        'play_by_play': None,
        'game_logs': None
    }
    
    seasons = list(range(current_season - seasons_back, current_season + 1))
    
    print(f"Loading historical data for {league} (seasons {seasons[0]}-{seasons[-1]})...")
    
    # Load schedules
    all_games = []
    for season in seasons:
        try:
            if league == 'NFL':
                schedule = nfl_fetcher.fetch_nfl_schedule(season)
            else:
                schedule = nba_fetcher.fetch_nba_schedule(season)
            
            if 'gameday' in schedule.columns:
                schedule['game_date'] = pd.to_datetime(schedule['gameday'])
            elif 'game_date' not in schedule.columns:
                schedule['game_date'] = pd.to_datetime(schedule.get('date', schedule.index))
            
            schedule['season'] = season
            all_games.append(schedule)
        except Exception as e:
            print(f"  Warning: Could not load {season}: {e}")
    
    if all_games:
        historical_data['historical_games'] = pd.concat(all_games, ignore_index=True)
        print(f"  Loaded {len(historical_data['historical_games'])} historical games")
    
    # Load play-by-play for NFL
    if league == 'NFL':
        try:
            import nfl_data_py as nfl
            pbp_data = []
            for season in seasons:
                try:
                    pbp = nfl.import_pbp_data([season])
                    if len(pbp) > 0:
                        pbp_data.append(pbp)
                except:
                    pass
            
            if pbp_data:
                pbp_df = pd.concat(pbp_data, ignore_index=True)
                if 'game_date' in pbp_df.columns:
                    pbp_df['game_date'] = pd.to_datetime(pbp_df['game_date'])
                elif 'gameday' in pbp_df.columns:
                    pbp_df['game_date'] = pd.to_datetime(pbp_df['gameday'])
                historical_data['play_by_play'] = pbp_df
                print(f"  Loaded {len(pbp_df)} play-by-play records")
        except ImportError:
            print("  Warning: nfl-data-py not available, skipping form features")
        except Exception as e:
            print(f"  Warning: Could not load PBP data: {e}")
    
    return historical_data


def build_features(games_df: pd.DataFrame, league: str, historical_data: dict) -> pd.DataFrame:
    """
    Build features for games using enhanced feature contract.
    Matches the feature engineering from the EDA notebook.
    
    Args:
        games_df: DataFrame with games
        league: 'NFL' or 'NBA'
        historical_data: Dict with historical_games, play_by_play (NFL), game_logs (NBA)
    
    Returns:
        DataFrame with features
    """
    df = games_df.copy()
    
    # Ensure game_date exists
    if 'game_date' not in df.columns:
        if 'gameday' in df.columns:
            df['game_date'] = pd.to_datetime(df['gameday'])
        else:
            raise ValueError("game_date or gameday column required")
    else:
        df['game_date'] = pd.to_datetime(df['game_date'])
    
    historical_games = historical_data.get('historical_games')
    if historical_games is None or len(historical_games) == 0:
        raise ValueError("historical_games required for feature building")
    
    # Add rest and schedule features
    df = rest_schedule.add_rest_features(df, historical_games)
    
    # Add opponent strength features
    df = strength.add_opponent_strength_features(df, historical_games, league.lower())
    
    # Add team strength features (win %, point diff)
    df = _add_team_strength_features(df, historical_games)
    
    # Add interaction features
    df = _add_interaction_features(df)
    
    # Add form features if available
    if league == 'NFL':
        play_by_play = historical_data.get('play_by_play')
        if play_by_play is not None and len(play_by_play) > 0:
            for window in [3, 5, 10]:
                df = form_metrics.add_form_features_nfl(df, play_by_play, window=window)
    else:  # NBA
        game_logs = historical_data.get('game_logs')
        if game_logs is not None and len(game_logs) > 0:
            for window in [3, 5, 10]:
                df = form_metrics.add_form_features_nba(df, game_logs, window=window)
    
    # Add form interaction features
    df = _add_form_interactions(df, league)
    
    return df


def _add_team_strength_features(games_df: pd.DataFrame, historical_games: pd.DataFrame) -> pd.DataFrame:
    """Add team strength features (win %, point diff)."""
    import numpy as np
    
    df = games_df.copy()
    df['home_team_win_pct'] = np.nan
    df['away_team_win_pct'] = np.nan
    df['home_team_point_diff'] = np.nan
    df['away_team_point_diff'] = np.nan
    
    for idx, row in df.iterrows():
        game_date = pd.to_datetime(row['game_date'])
        home_team = row['home_team']
        away_team = row['away_team']
        season = row.get('season', game_date.year)
        
        season_games = historical_games[
            (pd.to_datetime(historical_games['game_date']) < game_date) &
            (historical_games.get('season', pd.to_datetime(historical_games['game_date']).dt.year) == season)
        ]
        
        # Home team stats
        home_games = season_games[
            (season_games['home_team'] == home_team) | (season_games['away_team'] == home_team)
        ]
        if len(home_games) > 0:
            home_wins = sum(1 for _, g in home_games.iterrows()
                          if (g['home_team'] == home_team and g.get('home_score', 0) > g.get('away_score', 0)) or
                             (g['away_team'] == home_team and g.get('away_score', 0) > g.get('home_score', 0)))
            home_point_diff = [
                g.get('home_score', 0) - g.get('away_score', 0) if g['home_team'] == home_team
                else g.get('away_score', 0) - g.get('home_score', 0)
                for _, g in home_games.iterrows()
            ]
            df.loc[idx, 'home_team_win_pct'] = home_wins / len(home_games)
            df.loc[idx, 'home_team_point_diff'] = np.mean(home_point_diff) if home_point_diff else 0
        
        # Away team stats
        away_games = season_games[
            (season_games['home_team'] == away_team) | (season_games['away_team'] == away_team)
        ]
        if len(away_games) > 0:
            away_wins = sum(1 for _, g in away_games.iterrows()
                          if (g['home_team'] == away_team and g.get('home_score', 0) > g.get('away_score', 0)) or
                             (g['away_team'] == away_team and g.get('away_score', 0) > g.get('home_score', 0)))
            away_point_diff = [
                g.get('home_score', 0) - g.get('away_score', 0) if g['home_team'] == away_team
                else g.get('away_score', 0) - g.get('home_score', 0)
                for _, g in away_games.iterrows()
            ]
            df.loc[idx, 'away_team_win_pct'] = away_wins / len(away_games)
            df.loc[idx, 'away_team_point_diff'] = np.mean(away_point_diff) if away_point_diff else 0
    
    return df


def _add_interaction_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add interaction and derived features."""
    import numpy as np
    
    # Rest differentials
    if 'rest_home' in df.columns and 'rest_away' in df.columns:
        df['rest_differential'] = df['rest_home'] - df['rest_away']
        df['rest_advantage_home'] = (df['rest_home'] > df['rest_away']).astype(int)
    
    # Team strength differentials
    if 'home_team_win_pct' in df.columns and 'away_team_win_pct' in df.columns:
        df['win_pct_differential'] = df['home_team_win_pct'] - df['away_team_win_pct']
    if 'home_team_point_diff' in df.columns and 'away_team_point_diff' in df.columns:
        df['point_diff_differential'] = df['home_team_point_diff'] - df['away_team_point_diff']
        df['point_diff_gap'] = df['away_team_point_diff'] - df['home_team_point_diff']
        df['abs_point_diff_gap'] = df['point_diff_gap'].abs()
        df['point_diff_gap_flag'] = (df['point_diff_gap'] > 5).astype(int)
    
    # Opponent strength differential
    if 'opp_strength_home_season' in df.columns and 'opp_strength_away_season' in df.columns:
        df['opp_strength_differential'] = df['opp_strength_home_season'] - df['opp_strength_away_season']
    
    # Time features
    if 'game_date' in df.columns:
        df['week_number'] = pd.to_datetime(df['game_date']).dt.isocalendar().week
        df['month'] = pd.to_datetime(df['game_date']).dt.month
        if 'game_type' in df.columns:
            df['is_playoff'] = df['game_type'].str.contains('POST', case=False, na=False).astype(int)
        else:
            df['is_playoff'] = 0
    
    return df


def _add_form_interactions(df: pd.DataFrame, league: str) -> pd.DataFrame:
    """Add form feature interactions."""
    for window in [3, 5, 10]:
        if league == 'NFL':
            home_off = f'form_home_epa_off_{window}'
            away_off = f'form_away_epa_off_{window}'
            if home_off in df.columns and away_off in df.columns:
                df[f'form_epa_off_diff_{window}'] = df[home_off] - df[away_off]
            
            home_def = f'form_home_epa_def_{window}'
            away_def = f'form_away_epa_def_{window}'
            if home_def in df.columns and away_def in df.columns:
                df[f'form_epa_def_diff_{window}'] = df[home_def] - df[away_def]
        else:  # NBA
            home_net = f'form_home_net_rating_{window}'
            away_net = f'form_away_net_rating_{window}'
            if home_net in df.columns and away_net in df.columns:
                df[f'form_net_rating_diff_{window}'] = df[home_net] - df[away_net]
    
    return df


def upsert_games(supabase: Client, games_df: pd.DataFrame) -> dict:
    """
    Upsert games to Supabase.
    
    Returns:
        Dict mapping (league, season, home_team, away_team, game_time_utc) -> game_id
    """
    game_id_map = {}
    
    for _, game in games_df.iterrows():
        # Check if game exists
        result = supabase.table('games').select('id').eq('league', game['league'])\
            .eq('season', game['season'])\
            .eq('home_team', game['home_team'])\
            .eq('away_team', game['away_team'])\
            .eq('game_time_utc', game['game_time_utc'].isoformat() if hasattr(game['game_time_utc'], 'isoformat') else str(game['game_time_utc']))\
            .execute()
        
        game_data = {
            'league': game['league'],
            'season': game['season'],
            'game_time_utc': game['game_time_utc'].isoformat() if hasattr(game['game_time_utc'], 'isoformat') else str(game['game_time_utc']),
            'home_team': game['home_team'],
            'away_team': game['away_team']
        }
        
        if result.data:
            # Update existing
            game_id = result.data[0]['id']
            supabase.table('games').update(game_data).eq('id', game_id).execute()
        else:
            # Insert new
            result = supabase.table('games').insert(game_data).execute()
            game_id = result.data[0]['id']
        
        key = (game['league'], game['season'], game['home_team'], game['away_team'], str(game['game_time_utc']))
        game_id_map[key] = game_id
    
    return game_id_map


def upsert_odds(supabase: Client, odds_df: pd.DataFrame, game_id_map: dict):
    """Upsert odds snapshots to Supabase."""
    for _, row in odds_df.iterrows():
        # Find game_id
        game_id = None
        for key, gid in game_id_map.items():
            if row['home_team'] in key and row['away_team'] in key:
                game_id = gid
                break
        
        if not game_id:
            continue
        
        odds_data = {
            'game_id': game_id,
            'book': row['book'],
            'market': row['market'],
            'line': float(row['line']) if pd.notna(row['line']) else None,
            'price': float(row['price']) if pd.notna(row['price']) else None,
            'snapshot_ts': datetime.now().isoformat()
        }
        
        supabase.table('odds_snapshots').insert(odds_data).execute()


def upsert_predictions(supabase: Client, predictions_df: pd.DataFrame, game_id_map: dict, model_version: str):
    """Upsert model predictions to Supabase."""
    for _, row in predictions_df.iterrows():
        # Find game_id
        game_id = None
        for key, gid in game_id_map.items():
            if row['home_team'] in key and row['away_team'] in key:
                game_id = gid
                break
        
        if not game_id:
            continue
        
        pred_data = {
            'game_id': game_id,
            'model_name': 'sports_edge',
            'model_version': model_version,
            'my_spread': float(row['my_spread']) if pd.notna(row['my_spread']) else None,
            'my_home_win_prob': float(row['my_home_win_prob']) if pd.notna(row['my_home_win_prob']) else None,
            'asof_ts': datetime.now().isoformat()
        }
        
        supabase.table('model_predictions').insert(pred_data).execute()


def upsert_features(supabase: Client, features_df: pd.DataFrame, game_id_map: dict):
    """Upsert features to Supabase."""
    for _, row in features_df.iterrows():
        # Find game_id
        game_id = None
        for key, gid in game_id_map.items():
            if row['home_team'] in key and row['away_team'] in key:
                game_id = gid
                break
        
        if not game_id:
            continue
        
        # Convert features to JSON
        feature_dict = row.to_dict()
        # Remove non-feature columns
        for col in ['league', 'season', 'game_time_utc', 'home_team', 'away_team']:
            feature_dict.pop(col, None)
        
        feature_data = {
            'game_id': game_id,
            'feature_json': json.dumps(feature_dict),
            'asof_ts': datetime.now().isoformat()
        }
        
        supabase.table('features').insert(feature_data).execute()


def log_model_run(supabase: Client, league: str, success: bool, rows_written: int, error_text: Optional[str] = None):
    """Log model run to audit table."""
    run_data = {
        'league': league,
        'started_at': datetime.now().isoformat(),
        'finished_at': datetime.now().isoformat(),
        'rows_written': rows_written,
        'success': success,
        'error_text': error_text
    }
    
    supabase.table('model_runs').insert(run_data).execute()


def refresh(league: str, date: str, model_version: str = 'v0.1.0'):
    """
    Main refresh function.
    
    Args:
        league: 'NFL' or 'NBA'
        date: Date string YYYY-MM-DD
        model_version: Model version string
    """
    supabase = get_supabase_client()
    rows_written = 0
    error_text = None
    
    try:
        # 1. Fetch schedule
        print(f"Fetching {league} schedule for {date}...")
        if league == 'NFL':
            games_df = nfl_fetcher.fetch_nfl_games_for_date(date)
        else:
            games_df = nba_fetcher.fetch_nba_games_for_date(date)
        
        if games_df.empty:
            print(f"No games found for {date}")
            log_model_run(supabase, league, True, 0)
            return
        
        games_df['league'] = league
        games_df['season'] = datetime.strptime(date, '%Y-%m-%d').year
        
        # 2. Fetch odds
        print(f"Fetching odds for {league}...")
        odds_df = odds_fetcher.fetch_odds_for_date(league, date)
        
        # 3. Load historical data and build features
        print("Loading historical data...")
        current_season = datetime.strptime(date, '%Y-%m-%d').year
        historical_data = load_historical_data(league, current_season, seasons_back=3)
        
        print("Building features...")
        features_df = build_features(games_df, league, historical_data)
        
        # 4. Load models and predict using GamePredictor
        print("Loading models and predicting...")
        try:
            predictor = GamePredictor(league, model_version)
            
            # Predict using predictor (handles feature alignment automatically)
            predictions_list = []
            for idx, game in games_df.iterrows():
                game_row = pd.DataFrame([game])
                pred = predictor.predict(
                    game_row,
                    historical_data['historical_games'],
                    historical_data.get('play_by_play'),
                    historical_data.get('game_logs')
                )
                predictions_list.append(pred)
            
            predictions_df_temp = pd.DataFrame(predictions_list)
            features_df['my_spread'] = predictions_df_temp['predicted_spread'].values
            features_df['my_home_win_prob'] = predictions_df_temp['home_win_probability'].values
            
        except FileNotFoundError as e:
            print(f"Warning: {e}")
            print("Falling back to direct model loading...")
            
            # Fallback to direct model loading
            spread_model_path = f"models/spread_model_{league.lower()}_{model_version}.pkl"
            if os.path.exists(spread_model_path):
                spread_model = SpreadModel.load(spread_model_path)
                feature_cols = spread_model.feature_names
                
                # Prepare feature matrix
                X = pd.DataFrame()
                for col in feature_cols:
                    if col in features_df.columns:
                        X[col] = features_df[col]
                    else:
                        X[col] = 0
                X = X.fillna(0)
                
                predictions = spread_model.predict(X)
                features_df['my_spread'] = predictions
                
                # Convert spread to win prob
                link_path = f"models/link_function_{league.lower()}_{model_version}.pkl"
                if os.path.exists(link_path):
                    with open(link_path, 'rb') as f:
                        link_data = pickle.load(f)
                    link_a = link_data.get('a', 0.15)
                    link_b = link_data.get('b', 2.5)
                else:
                    link_a, link_b = 0.15, 2.5
                
                features_df['my_home_win_prob'] = features_df['my_spread'].apply(
                    lambda x: spread_to_win_prob(x, link_a, link_b) if pd.notna(x) else None
                )
            else:
                print(f"Warning: Model not found at {spread_model_path}, skipping predictions")
                features_df['my_spread'] = None
                features_df['my_home_win_prob'] = None
        
        # 5. Upsert to Supabase
        print("Upserting to Supabase...")
        game_id_map = upsert_games(supabase, games_df)
        rows_written += len(game_id_map)
        
        if not odds_df.empty:
            upsert_odds(supabase, odds_df, game_id_map)
            rows_written += len(odds_df)
        
        predictions_df = features_df[['home_team', 'away_team', 'my_spread', 'my_home_win_prob']]
        upsert_predictions(supabase, predictions_df, game_id_map, model_version)
        rows_written += len(predictions_df)
        
        upsert_features(supabase, features_df, game_id_map)
        rows_written += len(features_df)
        
        print(f"Successfully processed {len(games_df)} games")
        log_model_run(supabase, league, True, rows_written)
        
    except Exception as e:
        error_text = str(e)
        print(f"Error: {error_text}")
        log_model_run(supabase, league, False, rows_written, error_text)
        raise


def main():
    parser = argparse.ArgumentParser(description='Refresh sports-edge predictions')
    parser.add_argument('--league', type=str, required=True, choices=['NFL', 'NBA'], help='League (NFL or NBA)')
    parser.add_argument('--date', type=str, required=True, help='Date in YYYY-MM-DD format')
    parser.add_argument('--model-version', type=str, default='v0.1.0', help='Model version')
    
    args = parser.parse_args()
    
    refresh(args.league, args.date, args.model_version)


if __name__ == '__main__':
    main()



================================================
FILE: src/pipeline/train_models.py
================================================
"""
Model training and export script.
Trains models from notebook data and saves them in production format.
Can be run standalone or imported from notebook.
"""

import argparse
import pandas as pd
import numpy as np
import pickle
import os
from datetime import datetime
from typing import Optional, Dict, List
import sys

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, brier_score_loss
from sklearn.calibration import CalibratedClassifierCV
from lightgbm import LGBMClassifier, LGBMRegressor
from sklearn.base import clone
from pandas.api.types import is_numeric_dtype

from src.models.link_function import fit_link_function
from src.pipeline.refresh import build_features
from src.data import nfl_fetcher


def compute_season_sample_weights(df: pd.DataFrame, season_col: str = 'season',
                                  growth_per_year: float = 1.2) -> np.ndarray:
    """
    Create sample weights that emphasize more recent seasons.
    
    Args:
        df: DataFrame containing a season column
        season_col: Column with season/year values
        growth_per_year: Multiplicative boost applied per season step
    
    Returns:
        Numpy array of weights normalized to mean 1.
    """
    if season_col not in df.columns:
        return np.ones(len(df))
    
    seasons = pd.to_numeric(df[season_col], errors='coerce')
    if seasons.isna().all():
        return np.ones(len(df))
    
    min_season = seasons.min()
    weights = np.power(growth_per_year, seasons - min_season)
    weights = np.nan_to_num(weights, nan=1.0, posinf=1.0, neginf=1.0)
    return weights / np.mean(weights)


def _normalize_schedule(schedule: pd.DataFrame, season: int, league: str) -> pd.DataFrame:
    """Ensure schedule has game_date and standard columns."""
    df = schedule.copy()
    if 'game_date' in df.columns:
        df['game_date'] = pd.to_datetime(df['game_date'])
    elif 'gameday' in df.columns:
        df['game_date'] = pd.to_datetime(df['gameday'])
    else:
        raise ValueError("Schedule missing 'game_date'/'gameday' column.")
    
    df['season'] = season
    df['league'] = league.upper()
    return df


def load_completed_games(league: str, seasons: List[int]) -> pd.DataFrame:
    """
    Fetch and combine completed games for the requested seasons.
    """
    if not seasons:
        raise ValueError("At least one season must be provided.")
    
    frames = []
    league = league.upper()
    
    for season in seasons:
        if league == 'NFL':
            schedule = nfl_fetcher.fetch_nfl_schedule(season)
        else:
            raise NotImplementedError("Automated training currently supports NFL only.")
        
        schedule = _normalize_schedule(schedule, season, league)
        completed = schedule[
            schedule['home_score'].notna() & schedule['away_score'].notna()
        ].copy()
        
        if completed.empty:
            print(f"Warning: No completed games found for {league} {season}.")
            continue
        
        frames.append(completed)
    
    if not frames:
        raise ValueError("No completed games available for the provided seasons.")
    
    games = pd.concat(frames, ignore_index=True)
    games['game_date'] = pd.to_datetime(games['game_date'])
    games['season'] = pd.to_numeric(games['season'], errors='coerce')
    games = games.sort_values('game_date').reset_index(drop=True)
    return games


def load_play_by_play(seasons: List[int]) -> Optional[pd.DataFrame]:
    """
    Load play-by-play data for seasons (used for form features).
    """
    try:
        import nfl_data_py as nfl
        pbp = nfl.import_pbp_data(seasons)
    except Exception as exc:
        print(f"Warning: Failed to load play-by-play data ({exc}). Form features will be skipped.")
        return None
    
    keep_cols = [col for col in ['game_id', 'posteam', 'defteam', 'epa', 'success', 'game_date']
                 if col in pbp.columns]
    if not keep_cols:
        print("Warning: Play-by-play data missing required columns; skipping form features.")
        return None
    
    pbp = pbp[keep_cols].copy()
    if 'game_date' in pbp.columns:
        pbp['game_date'] = pd.to_datetime(pbp['game_date'])
    elif 'gameday' in pbp.columns:
        pbp['game_date'] = pd.to_datetime(pbp['gameday'])
    return pbp


def build_training_dataset(league: str, seasons: List[int], include_form: bool = True) -> pd.DataFrame:
    """
    Build training features by fetching historical games and engineering features.
    """
    games = load_completed_games(league, seasons)
    historical_data = {
        'historical_games': games.copy(),
        'play_by_play': None,
        'game_logs': None
    }
    
    if league.upper() == 'NFL' and include_form:
        pbp = load_play_by_play(seasons)
        if pbp is not None:
            historical_data['play_by_play'] = pbp
    
    features = build_features(games, league.upper(), historical_data)
    features = features[features['home_score'].notna() & features['away_score'].notna()].copy()
    features['home_win'] = (features['home_score'] > features['away_score']).astype(int)
    features['margin'] = features['home_score'] - features['away_score']
    return features


def _load_features_from_path(path: str) -> pd.DataFrame:
    """Load cached features from disk."""
    ext = os.path.splitext(path)[1].lower()
    if ext == '.parquet':
        return pd.read_parquet(path)
    if ext == '.csv':
        return pd.read_csv(path)
    raise ValueError(f"Unsupported features file type: {ext}")


def _save_features(df: pd.DataFrame, path: str) -> None:
    """Persist engineered feature set to disk."""
    ext = os.path.splitext(path)[1].lower()
    dir_name = os.path.dirname(path)
    if dir_name:
        os.makedirs(dir_name, exist_ok=True)
    if ext == '.parquet':
        df.to_parquet(path, index=False)
    elif ext == '.csv':
        df.to_csv(path, index=False)
    else:
        raise ValueError(f"Unsupported save format for {path}")


def train_and_save_models(features_df: pd.DataFrame, 
                          target_col: str = 'home_win',
                          margin_col: str = 'margin',
                          feature_cols: Optional[list] = None,
                          league: str = 'NFL',
                          model_version: str = 'v1',
                          use_lgbm: bool = True,
                          test_size: float = 0.2,
                          random_state: int = 42):
    """
    Train and save win probability and spread models.
    
    Args:
        features_df: DataFrame with features and targets
        target_col: Column name for binary win target
        margin_col: Column name for margin/spread target
        feature_cols: List of feature column names (if None, auto-detect)
        league: 'NFL' or 'NBA'
        model_version: Version string for saved models
        use_lgbm: Whether to use LightGBM (True) or Random Forest (False)
        test_size: Test set size fraction
        random_state: Random seed
    
    Returns:
        Dictionary with model metrics and paths
    """
    # Create models directory
    models_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'models')
    os.makedirs(models_dir, exist_ok=True)
    
    # Determine feature columns
    if feature_cols is None:
        # Auto-detect feature columns (exclude target, margin, and metadata columns)
        exclude_cols = [
            target_col, margin_col, 'game_id', 'game_date', 'gameday',
            'home_team', 'away_team', 'home_score', 'away_score',
            'league', 'season', 'game_type', 'week', 'weekday', 'gametime',
            'result', 'total', 'overtime', 'old_game_id', 'gsis',
            'nfl_detail_id', 'pfr', 'pff', 'espn', 'ftn',
            'away_qb_id', 'home_qb_id', 'stadium_id', 'referee',
            'temp', 'wind'
        ]
        feature_cols = [
            col for col in features_df.columns
            if col not in exclude_cols and is_numeric_dtype(features_df[col])
        ]
        
        # Explicitly drop sportsbook/odds inputs until we feed real-time prices
        odds_keywords = (
            'moneyline',
            'spread_line',
            'spread_odds',
            'total_line',
            'over_odds',
            'under_odds',
            'book'
        )
        feature_cols = [
            col for col in feature_cols
            if not any(keyword in col.lower() for keyword in odds_keywords)
        ]
    
    # Remove duplicates while preserving order
    seen = set()
    feature_cols = [col for col in feature_cols if not (col in seen or seen.add(col))]
    
    # Prepare data
    extra_cols = [target_col, margin_col]
    has_season_col = 'season' in features_df.columns
    if has_season_col:
        extra_cols.append('season')
    else:
        print("WARNING: 'season' column missing from features; sample weights will be uniform.")
    
    model_df = features_df[feature_cols + extra_cols].dropna(subset=[target_col, margin_col])
    
    if len(model_df) < 100:
        raise ValueError(f"Insufficient data: {len(model_df)} rows (need >= 100)")
    
    X = model_df[feature_cols]
    y_win = model_df[target_col]
    y_margin = model_df[margin_col]
    # Clip margin target to reduce blowout noise and emphasize realistic spreads
    y_margin_clipped = np.clip(y_margin, -21, 21)
    if has_season_col:
        sample_weights = compute_season_sample_weights(model_df, season_col='season')
    else:
        sample_weights = np.ones(len(model_df))
    
    # Boost weights for large mismatches so models learn blowout signals
    mismatch_boost = 1 + 0.5 * np.clip(np.abs(y_margin_clipped) / 14.0, 0, 1)
    sample_weights = sample_weights * mismatch_boost
    
    # Split data
    split_result = train_test_split(
        X, y_win, y_margin_clipped, sample_weights,
        test_size=test_size,
        random_state=random_state,
        stratify=y_win
    )
    (X_train, X_test,
     y_train_win, y_test_win,
     y_train_margin, y_test_margin,
     w_train, w_test) = split_result
    
    w_train = np.asarray(w_train)
    w_test = np.asarray(w_test)
    
    print(f"Training on {len(X_train)} samples, testing on {len(X_test)} samples")
    print(f"Features: {len(feature_cols)}")
    
    # Keep base feature matrices for each model
    spread_feature_cols = feature_cols.copy()
    X_train_base = X_train.copy()
    X_test_base = X_test.copy()
    X_full_base = X.copy()
    
    # Train spread model first
    print("\nTraining spread model...")
    if use_lgbm:
        spread_estimator = LGBMRegressor(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=7,
            num_leaves=31,
            random_state=random_state,
            verbose=-1
        )
    else:
        spread_estimator = RandomForestRegressor(
            n_estimators=200,
            max_depth=12,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=random_state,
            n_jobs=-1
        )
    
    n_splits = min(5, max(2, len(X_train_base) // 75))
    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    oof_spread = np.zeros(len(X_train_base))
    for train_idx, val_idx in kfold.split(X_train_base):
        reg = clone(spread_estimator)
        reg.fit(
            X_train_base.iloc[train_idx],
            y_train_margin.iloc[train_idx],
            sample_weight=w_train[train_idx]
        )
        oof_spread[val_idx] = reg.predict(X_train_base.iloc[val_idx])
    
    spread_model = clone(spread_estimator)
    spread_model.fit(X_train_base, y_train_margin, sample_weight=w_train)
    
    y_pred_spread = spread_model.predict(X_test_base)
    mae = mean_absolute_error(y_test_margin, y_pred_spread, sample_weight=w_test)
    rmse = np.sqrt(mean_squared_error(y_test_margin, y_pred_spread, sample_weight=w_test))
    
    print(f"Spread Model ({'LGBM' if use_lgbm else 'RF'}):")
    print(f"  MAE: {mae:.2f} points")
    print(f"  RMSE: {rmse:.2f} points")
    
    # Inject spread predictions as an anchor feature for the win model
    spread_feature_name = 'model_spread_feature'
    spread_pred_train = oof_spread
    spread_pred_full = spread_model.predict(X_full_base)
    
    X_train_win = X_train_base.copy()
    X_test_win = X_test_base.copy()
    X_full_win = X_full_base.copy()
    X_train_win[spread_feature_name] = spread_pred_train
    X_test_win[spread_feature_name] = y_pred_spread
    X_full_win[spread_feature_name] = spread_pred_full
    win_feature_cols = X_train_win.columns.tolist()
    
    # Train win probability model (now aligned with spread outputs)
    print("\nTraining win probability model...")
    if use_lgbm:
        try:
            base_model = LGBMClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=7,
                num_leaves=31,
                random_state=random_state,
                verbose=-1
            )
            win_prob_model = CalibratedClassifierCV(
                base_model,
                method='isotonic',
                cv=5,
                n_jobs=-1
            )
            win_prob_model.fit(X_train_win, y_train_win, sample_weight=w_train)
            model_type = 'lightgbm_calibrated'
        except Exception as e:
            print(f"LightGBM failed: {e}, falling back to Random Forest")
            use_lgbm = False
    
    if not use_lgbm:
        base_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=12,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=random_state,
            n_jobs=-1
        )
        win_prob_model = CalibratedClassifierCV(
            base_model,
            method='isotonic',
            cv=5,
            n_jobs=-1
        )
        win_prob_model.fit(X_train_win, y_train_win, sample_weight=w_train)
        model_type = 'rf_calibrated'
    
    y_pred_win = win_prob_model.predict(X_test_win)
    y_pred_proba = win_prob_model.predict_proba(X_test_win)[:, 1]
    accuracy = accuracy_score(y_test_win, y_pred_win, sample_weight=w_test)
    brier = brier_score_loss(y_test_win, y_pred_proba, sample_weight=w_test)
    
    print(f"Win Probability Model ({model_type.upper()}):")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Brier Score: {brier:.4f} (lower is better)")
    
    # Calibrate link function using model-predicted spreads
    print("\nCalibrating link function...")
    try:
        calibration_spreads = pd.Series(spread_pred_full, index=X.index)
        link_params = fit_link_function(calibration_spreads, y_win, max_abs_a=0.3)
    except Exception as e:
        print(f"  Warning: Link calibration failed ({e}); falling back to defaults.")
        link_params = (0.15, 2.5)
    link_a, link_b = link_params
    print(f"Link function parameters: a={link_a:.4f}, b={link_b:.4f}")
    
    # Check model consistency
    from src.models.link_function import spread_to_win_prob
    win_prob_from_spread = spread_to_win_prob(y_pred_spread, link_a, link_b)
    disagreement = np.abs(y_pred_proba - win_prob_from_spread)
    mean_disagreement = np.mean(disagreement)
    sign_agreement = ((y_pred_proba > 0.5) == (y_pred_spread > 0)).mean()
    
    print(f"\nModel Consistency Check:")
    print(f"  Mean disagreement: {mean_disagreement:.4f}")
    print(f"  Sign agreement: {sign_agreement:.1%}")
    
    if mean_disagreement > 0.15:
        print(f"  ⚠️  Warning: High disagreement between models. Consider ensemble approach.")
    
    # Calculate feature medians for missing value imputation
    feature_medians = X_train_win.median().to_dict()
    
    # Save models
    league_lower = league.lower()
    
    # Win probability model
    win_prob_path = os.path.join(models_dir, f'win_prob_model_{league_lower}_{model_version}.pkl')
    win_prob_data = {
        'model': win_prob_model,
        'model_type': model_type,
        'feature_names': win_feature_cols,
        'win_feature_names': win_feature_cols,
        'spread_feature_names': spread_feature_cols,
        'accuracy': accuracy,
        'brier_score': brier,
        'trained_date': datetime.now().isoformat(),
        'n_features': len(win_feature_cols),
        'n_samples': len(X_train_win),
        'league': league,
        'calibrated': True
    }
    with open(win_prob_path, 'wb') as f:
        pickle.dump(win_prob_data, f)
    print(f"\nSaved win probability model to {win_prob_path}")
    
    # Spread model
    spread_path = os.path.join(models_dir, f'spread_model_{league_lower}_{model_version}.pkl')
    spread_data = {
        'model': spread_model,
        'model_type': 'lightgbm' if use_lgbm else 'rf',
        'feature_names': spread_feature_cols,
        'spread_feature_names': spread_feature_cols,
        'mae': mae,
        'rmse': rmse,
        'trained_date': datetime.now().isoformat(),
        'n_features': len(spread_feature_cols),
        'n_samples': len(X_train_base),
        'league': league
    }
    with open(spread_path, 'wb') as f:
        pickle.dump(spread_data, f)
    print(f"Saved spread model to {spread_path}")
    
    # Link function
    link_path = os.path.join(models_dir, f'link_function_{league_lower}_{model_version}.pkl')
    link_data = {
        'a': link_a,
        'b': link_b,
        'calibrated_date': datetime.now().isoformat(),
        'league': league
    }
    with open(link_path, 'wb') as f:
        pickle.dump(link_data, f)
    print(f"Saved link function to {link_path}")
    
    # Feature medians
    medians_path = os.path.join(models_dir, f'feature_medians_{league_lower}_{model_version}.pkl')
    with open(medians_path, 'wb') as f:
        pickle.dump(feature_medians, f)
    print(f"Saved feature medians to {medians_path}")
    
    return {
        'win_prob_model_path': win_prob_path,
        'spread_model_path': spread_path,
        'link_function_path': link_path,
        'feature_medians_path': medians_path,
        'accuracy': accuracy,
        'brier_score': brier,
        'mae': mae,
        'rmse': rmse,
        'link_params': link_params,
        'mean_disagreement': mean_disagreement,
        'sign_agreement': sign_agreement,
        'win_feature_names': win_feature_cols,
        'spread_feature_names': spread_feature_cols,
        'n_samples': len(X_train_win)
    }


def parse_args() -> argparse.Namespace:
    """Parse CLI arguments."""
    parser = argparse.ArgumentParser(description="Train and export Sports-Edge models.")
    parser.add_argument('--league', default='NFL', choices=['NFL'], help="League to train.")
    parser.add_argument('--start-season', type=int, default=2021, help="First season to include.")
    parser.add_argument('--end-season', type=int, default=2024, help="Last season to include (inclusive).")
    parser.add_argument('--model-version', default='v1', help="Model version tag for saved artifacts.")
    parser.add_argument('--features-path', type=str, help="Optional path to cached features (csv/parquet).")
    parser.add_argument('--save-features-path', type=str, help="Optional path to save engineered training set.")
    parser.add_argument('--use-rf', action='store_true', help="Use RandomForest instead of LightGBM.")
    parser.add_argument('--no-form', action='store_true', help="Skip form metrics (no play-by-play fetch).")
    parser.add_argument('--test-size', type=float, default=0.2, help="Test size fraction.")
    parser.add_argument('--random-state', type=int, default=42, help="Random seed.")
    return parser.parse_args()


def main():
    args = parse_args()
    league = args.league.upper()
    
    if args.features_path:
        features_df = _load_features_from_path(args.features_path)
        print(f"Loaded cached features from {args.features_path} ({len(features_df)} rows)")
    else:
        if args.start_season > args.end_season:
            raise ValueError("start-season must be <= end-season.")
        seasons = list(range(args.start_season, args.end_season + 1))
        print(f"Building training set for {league} seasons {seasons[0]}-{seasons[-1]}...")
        features_df = build_training_dataset(league, seasons, include_form=not args.no_form)
        print(f"Engineered features for {len(features_df)} completed games.")
        if args.save_features_path:
            _save_features(features_df, args.save_features_path)
            print(f"Saved training features to {args.save_features_path}")
    
    metrics = train_and_save_models(
        features_df,
        league=league,
        model_version=args.model_version,
        use_lgbm=not args.use_rf,
        test_size=args.test_size,
        random_state=args.random_state
    )
    
    print("\nTraining complete:")
    for key in ['accuracy', 'brier_score', 'mae', 'rmse', 'mean_disagreement', 'sign_agreement']:
        if key in metrics:
            print(f"  {key}: {metrics[key]}")


if __name__ == "__main__":
    main()



================================================
FILE: .github/workflows/refresh.yml
================================================
name: Sports Edge Refresh

on:
  schedule:
    # Run every 15 minutes during game days (8 AM - 11 PM America/Denver)
    - cron: '*/15 8-23 * * *'
  workflow_dispatch:
    inputs:
      league:
        description: 'League to refresh'
        required: true
        type: choice
        options:
          - NFL
          - NBA
      date:
        description: 'Date (YYYY-MM-DD)'
        required: false
        default: ''

jobs:
  refresh:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Set date
        id: set_date
        run: |
          if [ -z "${{ github.event.inputs.date }}" ]; then
            DATE=$(TZ='America/Denver' date +%Y-%m-%d)
          else
            DATE="${{ github.event.inputs.date }}"
          fi
          echo "date=$DATE" >> $GITHUB_OUTPUT
      
      - name: Refresh NFL predictions
        if: github.event.inputs.league == 'NFL' || github.event_name == 'schedule'
        env:
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          python -m src.pipeline.refresh --league NFL --date ${{ steps.set_date.outputs.date }} --model-version v0.1.0
        continue-on-error: true
      
      - name: Refresh NBA predictions
        if: github.event.inputs.league == 'NBA' || github.event_name == 'schedule'
        env:
          ODDS_API_KEY: ${{ secrets.ODDS_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          python -m src.pipeline.refresh --league NBA --date ${{ steps.set_date.outputs.date }} --model-version v0.1.0
        continue-on-error: true



